---
title: "Lab 2"
author: "My Name"
output:
  pdf_document:
    fig_caption: yes
    number_sections: true
header-includes:
  - |
    ```{=latex}
    \usepackage{fvextra}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{
      breaksymbolleft={}, 
      showspaces = false,
      showtabs = false,
      breaklines,
      commandchars=\\\{\}
    }
    ```
---

```{r, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  tidy.opts=list(width.cutoff=70),
  tidy=TRUE
)
```

# Lab 2 (Set 1): The basics {-}

# A simple processing pipeline

Let's begin by creating an object consisting of a character string. In this case, the first sentence from *A Tale of Two Cities*.

```{r}
totc_txt <- "It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair."
```

And we'll load the tidyverse libraries.

```{r setup, message = FALSE, error=FALSE, warning=FALSE}
library(tidyverse)
```

We could then split the vector, say at each space.

```{r}
totc_tkns <- totc_txt %>% str_split(" ")
```

Then, we can create a table of counts.

```{r make_table}
totc_df <- table(totc_tkns) %>% # make a table of counts
  as_tibble() %>%
  rename(Token = totc_tkns, AF = n) %>% # rename columns
  arrange(-AF) # sort the data by frequency
```

```{r echo=FALSE}
knitr::kable(head(totc_df), caption = "Token counts of sample sentence.")
```

The process of splitting the string vector into constituent parts is called **tokenizing**. Think of this as telling the computer how to define a word (or a "token", which is a more precise, technical term). In this case, we've done it in an extremely simple way--by defining a token as any string that is bounded by spaces.

```{r echo=FALSE}
knitr::kable(filter(totc_df, str_detect(totc_df$Token, regex("^it$", ignore_case= T))), caption = "Case sensitive counts of the token *it*.")
```

Note that in doing so, we are counting capitalized and non-capitalized words as distinct tokens.

There may be specific instances when we want to do this. But normally, we'd want *it* and *It* to be the same token. To do that, we can add a step in the processing pipeline that converts our vector to lower case before tokenizing.

```{r}
totc_df <- tolower(totc_txt) %>%
  str_split(" ") %>%
  table() %>% # make a table of counts
  as_tibble() %>%
  rename(Token = ".", AF = n) %>% # rename columns
  arrange(-AF) # sort the data by frequency
```


```{r echo=FALSE}
knitr::kable(head(totc_df), caption = "Token counts of sample sentence.")
```

___
\begin{center}
STOP!\\
COMPLETE TASKS 1 \& 2
\end{center} 
___


