---
title: "Lab Set 1"
output:
  pdf_document:
    fig_caption: yes
    number_sections: true
author: "My Name"
---

```{r, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  tidy.opts=list(width.cutoff=70), # this last bit auto-wraps code and comments so the don't run off the page, but you need to have formatR installed
  tidy=TRUE
)
```

# Lab 01

## Questions related to the reading

### Task 1

Answer the following questions in response to the readings:

What is the distinction Schmidt makes between “algorithms” and “transformations”?

> Your response:

Is this a useful distinction to make, do you think?

> Your response:

### Task 2

Answer the the following question:

What interests you about the quantitative analysis of text?

> Your response:

## What questions does this example raise for you?

This process raises any number of potential questions: about sentiment analysis, about the choice of procedures, about their application to particular kinds of data, to the very choice of the data itself. In asking you to posit your questions, the goal is not to reach any summative conclusion as to whether the **syuzhet** package is "good" or "bad." Rather it is to have us think through all of the decisions that we make as analysts (or that get made for us inside R packages, software, etc.), and what we might want to know or test in order to advance and defend our conclusions.

### Task 3

We've walked through a complex procedure that involves a series of steps. Sketch out a research scenario or question where the patterns generated by the package might be useful. What information would be important to include to help potential readers understand the affordances and weaknesses of the patterns the package generates? 

> Your response:

# Lab 02

## What counts as a token?

These choices are important. To carry our any statistical analysis on texts, we radically reorganize texts into counts. Precisely how we choose to do that--the decisions we make in exactly **what** to count--affects everything else downstream.

So let's consider a chunk of text that is a little more complicated than the example from *A Tale of Two Cities*. In the code block below, you are going to write out some pseudocode that lists the steps you would take to get from the text example to the tokens you want to count. For example, you might consider if you want to include numbers? Or symbols? Punctuation? How would you handle hyphenated words? Contractions?

Pseudocode is simply a step-by-step description of an algorithmic process. While not written for any specific language, pseudocode can be "translated" into working code. We're not going to be too rigorous here about your pseudocode, but in general, you should:

* Capitalize the initial word.
* Have only one statement per line.
* Indent to show hierarchy, improve readability, and show nested constructs.
* Keep it simple, concise, and readable.

### Task 1

Write your own pseudocode for processing and tokenizing the following text:

> In spite of some problems, we saw a 35% uptick in our user-base in the U.S. But that's still a lot fewer than we had last year. We'd like to get that number closer to what we've experienced in the U.K.--something close to 3 million users.

```{r pseudo_code, eval=FALSE}
READ text
  NEXT step?
  DO another thing?
  AS MANY things as you need...
OUTPUT tokens
```

### Task 2

Briefly describe the tokens that you want to output

> Your response:

What are some kinds of tokens that you think are particularly challenging to deal with (e.g., hyphenated words, contractions, abbreviations, etc.)?

> Your response:

# Lab 03

## Task 1

Use the following text:

> "The more I dove in, though, the less I cared. I watched BTS perform their 2018 anthem \"Idol\" on The Tonight Show and wondered how their lungs didn't explode from exertion. I watched the sumptuous short film for their 2016 hit \"Blood, Sweat, and Tears\" and couldn't tell whether I was more impressed by the choreography or the high-concept storytelling. And I was entranced by the video for \"Spring Day,\" with its dreamlike cinematography and references to Ursula K. Le Guin and Bong Joon-ho's film Snowpiercer. When I learned that the video is often interpreted as a tribute to the school-age victims of 2014's Sewol ferry disaster, I replayed it and cried."

In the code chunk below, construct a pipeline that:

* tokenizes the text
* creates a corpus object
* creates a dfm
* generates a frequency count of tokens
* uses the **mutate()** function to add a **RF** column (for "relative frequency") to the data frame.

Hint: Relative frequency (or normalized frequency) just takes the frequency, divides it by total number of tokens/words, and multiplies by a normalizing factor (e.g., by 100 for percent of tokens).

```{r}

```

And report the results in a kable:

```{r}

```

## Task 2

Data from Lab 02:

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(quanteda)
```


```{r}
source("../R/helper_functions.R")
```


```{r}

totc_txt <- "It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair."

text_2 <- "Jane Austen was not credited as the author of 'Pride and Prejudice.' In 1813, the title page simply read \"by the author of Sense and Sensibility.\" It wasn't until after Austen's death that her identity was revealed. #MentalFlossBookClub with @HowLifeUnfolds #15Pages https://pbs.twimg.com/media/EBOUqbfWwAABEoj.jpg"

comb_corpus <-   data.frame(doc_id = c("text_1", "text_2"), text = c(totc_txt, text_2)) %>%
  mutate(text = preprocess_text(text)) %>%
  corpus()

docvars(comb_corpus) <- data.frame(text_type = c("Fiction", "Twitter"))

comb_tkns <- comb_corpus %>%
  tokens(what = "fastestword")
```


```{r}
# structure 1:
comb_dfm <- dfm(comb_tkns) %>% dfm_group(groups = text_type)
comb_freq <- dfm(comb_tkns) %>% quanteda.textstats::textstat_frequency(groups = text_type)

# structure 2:
comb_ntoken <- data.frame("Tokens" = ntoken(comb_tkns), docvars(comb_tkns))
```


Use one of these 2 data structures (**comb_freq** or **comb_ntoken**) to make a corpus composition table. It should have 2 columns (one for "Text Type" and the other for "Tokens") and 3 rows ("Fiction", "Twitter" and "Total").

```{r}

```

And report the results in a kable:

```{r}

```

