---
title: "Lab 3"
output:
  pdf_document:
    fig_caption: yes
    number_sections: true
author: "My Name"
---

```{r, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  tidy.opts=list(width.cutoff=70), # this last bit auto-wraps code and comments so the don't run off the page, but you need to have formatR installed
  tidy=TRUE
)
```

# Lab 3 (Set 1): Processing piplines {-}

# Tokenizing with quanteda

In the previous lab, we did some back-of-the-napkin text processing. In that lab, you were encouraged to think about what exactly is happening when you split a text into tokens and convert those into counts.

We'll build on that foundational work, but let an R package **quanteda** do some of the heavy lifting for us. So let's load our packages:


```{r setup, message = FALSE, error=FALSE, warning=FALSE}
library(tidyverse)
library(quanteda)
library(quanteda.textstats)
```

Load in some useful functions:

```{r}
source("../R/helper_functions.R")
```

And again, we'll start with the first sentence from *A Tale of Two Cities*.

```{r}
totc_txt <- "It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair."
```

## Create a corpus

The first step is to create a corpus object:

```{r create_corpus}
totc_corpus <- corpus(totc_txt)
```

And see what we have:

```{r echo=FALSE}
knitr::kable(summary(totc_corpus), caption = "Summary of corpus.")
```

Note that if we had more than 1 document, we would get a count of how many documents in which the token appear, and that we can assign documents to grouping variable. This will become useful later.

## Tokenize the corpus

```{r tokenize_corpus}
totc_tkns <- tokens(totc_corpus, what = "word", remove_punct = TRUE)
```

## Create a document-feature matrix (dfm)

```{r create_dfm}
totc_dfm <- dfm(totc_tkns)
```

A **dfm** is an important data structure to understand, as it often serves as the foundation for all kinds of downstream statistical processing. It is a table with rows for documents (or observations) and columns for tokens (or variables)

```{r echo=FALSE}
knitr::kable(convert(totc_dfm, to = "data.frame")[,1:12], caption = "Part of a document-feature matrix.", "simple")
```

## And count our tokens

```{r echo=FALSE}
knitr::kable(textstat_frequency(totc_dfm), caption = "Token counts of sample sentence.", "simple")
```

## Using pipes to expidite the process

This time, we will change **remove_punct** to **FALSE**.

```{r}
totc_freq <- totc_corpus %>%
  tokens(what = "word", remove_punct = FALSE) %>%
  dfm() %>%
  textstat_frequency()
```
 
```{r echo=FALSE}
knitr::kable(totc_freq, caption = "Token counts of sample sentence.", "simple")
```

# Tokenizing options

In the previous lab, you were asked to consider the questions: What counts as a token/word? And how do you tell the computer to count what you want?

As the above code block suggest, the **tokens()** function in **(quanteda)[http://quanteda.io/reference/tokens.html]** gives you some measure on control.

We'll read in a more complex string:

```{r}
text_2 <- "Jane Austen was not credited as the author of 'Pride and Prejudice.' In 1813, the title page simply read \"by the author of Sense and Sensibility.\" It wasn't until after Austen's death that her identity was revealed. #MentalFlossBookClub with @HowLifeUnfolds #15Pages https://pbs.twimg.com/media/EBOUqbfWwAABEoj.jpg"
```

And process it as we did earlier.

```{r}
text_2_freq <- text_2 %>%
  corpus() %>%
  tokens(what = "word", remove_punct = TRUE) %>%
  dfm() %>%
  textstat_frequency()
```
 
```{r echo=FALSE}
knitr::kable(text_2_freq, caption = "Token counts of sample Tweet.", "simple")
```

Note that in addition to various logical "remove" arguments (**remove_punct**, **remove_symbols**, etc.), the **tokens()** function has a **what** argument. The default, "word", is "smarter", but also slower. Another option is "fastestword", which splits at spaces.

```{r}
text_2_freq <- text_2 %>%
  corpus() %>%
  tokens(what = "fastestword", remove_punct = TRUE, remove_url = TRUE) %>%
  dfm() %>%
  textstat_frequency()  %>%
  as_tibble() %>%
  dplyr::select(feature, frequency)
```
 
```{r echo=FALSE}
knitr::kable(text_2_freq, caption = "Token counts of sample Tweet.", "simple")
```

This, of course, makes no difference with just a few tokens, but does if you're trying to process millions.

Also note that we've used the **select()** function to choose specific columns.

## Pre-processing

An alternative to making tokenizing decisions inside the tokenizing process, you can process the text before tokenizing using functions for manipulating strings in **stringr**, **stringi**, **textclean**, or base R (like **grep( )**). Some common and convenient transformations are wrapped in a **cmu.textstat** function called **preprocess_text( )**

```{r pre_process}
text_2_freq <- text_2 %>%
  preprocess_text() %>%
  corpus() %>%
  tokens(what = "fastestword") %>%
  dfm() %>%
  textstat_frequency() %>%
  as_tibble() %>%
  dplyr::select(feature, frequency) %>%
  rename(Token = feature, AF = frequency) %>% # for absolute frequency
  mutate(New = NA)
```
 
```{r echo=FALSE}
knitr::kable(text_2_freq, caption = "Token counts of sample Tweet.", "simple")
```

Note how the default arguments treat negation and possessive markers. As with the **tokens ()** function, many of these (options)[http://htmlpreview.github.io/?https://raw.githubusercontent.com/browndw/quanteda.extras/main/vignettes/preprocess_introduction.html] are logical.

Note, too, that we've renamed the columns and added a new one using **mutate()**. 

# Task 1

Use the following text:

> "The more I dove in, though, the less I cared. I watched BTS perform their 2018 anthem \"Idol\" on The Tonight Show and wondered how their lungs didn't explode from exertion. I watched the sumptuous short film for their 2016 hit \"Blood, Sweat, and Tears\" and couldn't tell whether I was more impressed by the choreography or the high-concept storytelling. And I was entranced by the video for \"Spring Day,\" with its dreamlike cinematography and references to Ursula K. Le Guin and Bong Joon-ho's film Snowpiercer. When I learned that the video is often interpreted as a tribute to the school-age victims of 2014's Sewol ferry disaster, I replayed it and cried."

In the code chunk below, construct a pipeline that:

* tokenizes the text
* creates a corpus object
* creates a dfm
* generates a frequency count of tokens
* uses the **mutate()** function to add a **RF** column (for "relative frequency") to the data frame.

Hint: Relative frequency (or normalized frequency) just takes the frequency, divides it by total number of tokens/words, and multiplies by a normalizing factor (e.g., by 100 for percent of tokens).

```{r}

```

And report the results in a kable:

```{r}

```


# Creating a corpus composition table

Whenever you report the results of a corpus-based analysis, it is best practice to include a table that summarizes the composition of your corpus (or corpora) and any relevant variables. Most often this would include token counts aggregated by relevant categorical variables and a row of totals.

## Adding a grouping variable

We have 2 short texts (one from fiction and one from Twitter). Let's first combine them into a single corpus. First, a data frame is created that has 2 columns (**doc_id** and **text**). Then, the **text** column is passed to the **preprocess_text()** function before creating the corpus.

```{r}
comb_corpus <-   data.frame(doc_id = c("text_1", "text_2"), text = c(totc_txt, text_2)) %>%
  mutate(text = preprocess_text(text)) %>%
  corpus()
```

Next well assign a grouping variable using **docvars()**. In later labs, we'll use a similar process to assign variables from tables of metadata.

```{r}
docvars(comb_corpus) <- data.frame(text_type = c("Fiction", "Twitter"))
```

Now we can tokenize.

```{r}
comb_tkns <- comb_corpus %>%
  tokens(what = "fastestword")
```

Once we have done this, we can use that grouping variable to manipulate the data in a vraiety of ways. We could use **dfm_group()** to aggregate by group instead of individual text. (Though because we have only 2 texts here, it amounts to the same thing.) 

```{r}
comb_dfm <- dfm(comb_tkns) %>% dfm_group(groups = text_type)
```

 
```{r echo=FALSE}
knitr::kable(comb_dfm[,1:10], caption = "A grouped dfm.", "simple")
```

Or we can return a frequency table with a **group** column.

```{r}
comb_freq <- dfm(comb_tkns) %>% textstat_frequency(groups = text_type)
```


```{r echo=FALSE}
knitr::kable(comb_freq %>% group_by(group) %>% slice_max(frequency, n = 3), caption = "Subset of a grouped frequency table", "simple")
```

An additional option would be to use the **ntoken()** function to collect the total counts by text and combine those in a data frame with the **docvars()**:

```{r}
comb_ntoken <- data.frame("Tokens" = ntoken(comb_tkns), docvars(comb_tkns))
```


```{r echo=FALSE}
knitr::kable(comb_ntoken, caption = "A data frame with the counts by text.", "simple")
```


# Task 2

Use one of these 2 data structures to make a corpus composition table. It should have 2 columns (one for "Text Type" and the other for "Tokens") and 3 rows ("Fiction", "Twitter" and "Total").

```{r}

```

And report the results in a kable:

```{r}

```

