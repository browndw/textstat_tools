This study focuses on soil-borne nonvolatile, immobile, and/or hydrophobic contaminants. For chemicals in soil to pose a potential human or ecological health risk, chemicals must be present in a specific medium, and a mechanism, or exposure pathway, must exist for chemical migration from the medium to a receptor organism. These exposure pathways are illustrated in Figure 1. One exposure pathway is inhalation of dust that may contain an organic or inorganic chemical. Organic chemicals volatilized from the soil also have the potential to be inhaled independently of dust. Soil ingestion is another exposure pathway. Inadvertently, all people ingest some small amount of dirt or dust. Children ingest roughly 50-200 mg soil/day (2,3). Soil ingestion is particularly problematic for children with pica syndrome for soil, a mental condition characterized by habitual soil consumption (4). When a person's skin is caked with mud or dirt, there is a potential for chemicals to detach from the soil matrix and be absorbed through the skin. This process is named the dermal absorption exposure pathway. Lastly, uptake of contaminated groundwater is problematic in sites where water-soluble contaminants leech into drinking water supplies. Soil ingestion is the risk exposure pathway that frequently drives site risk assessment with nonvolatile, immobile, and/or hydrophobic contaminants chemicals.
Harmful chemical influences on the human body are typically divided into carcinogenic and noncarcinogenic effects. Potential carcinogenic effects are typically expressed in terms of a hypothetical cancer risk. This value is determined for a chemical concentration that may cause one additional person to develop cancer out of one hundred thousand or one million people (1 x 10-5 or 1 x 10-6) over a person's lifetime (70 years is the typically assumed length). On the other hand, noncarcinogenic chemicals cause a deleterious effect on a system or organ, and the receptor at greatest risk is typically young children. There is a formidable level of difficulty and controversy in determining whether a compound is unacceptably dangerous at a certain concentration. Typically, epidemiological data are analyzed or studies are performed on rodents or other organisms whose digestive system most closely resemble the human gastrointestinal (GI) system. To keep the experimental populations small, incredibly high doses are given to the organisms to increase the effect size. In this testing, animals are typically fed the contaminant through a highly soluble form such as HgCl2 (5).
In the bioavailability literature, the terms bioavailability and bioacessibility are used to describe similar, although different, concepts. Bioaccessibility is defined as the contaminant concentration that becomes available for absorption into the central blood or lymph system. On the other hand, bioavailability is defined as the chemical concentration that actually becomes absorbed into the central blood or lymph system.
Fundamentally, the physicochemical processes that occur in the human gastrointestinal system dictate the contaminant concentration levels that become absorbed into systemic circulation through contaminated soil or sediment ingestion. An overview of this system highlights the relevant mechanisms related to contaminated soil mobilization and absorption into the blood (or lymph) stream. Soil enters the body through the oral cavity where saliva and teeth have the potential to chemically or mechanically alter the soil. Then, the soil is transported via the esophagus through the thoracic cavity, past the diaphragm, and into the stomach. In the stomach, contractions, which induce mixing and peristalsis, move the ingested soil toward the pyloric sphincter, the exit from the stomach to the small intestine. The small intestine is subdivided into three components: the duodenum, the jejunum, and the ileum. In the small intestine, the low pH solution from the stomach is neutralized through carbonate buffering. The small intestine is covered with villi, fingerlike projects that greatly increase the surface area. These villi serve an invaluable role through enhancing digestion and absorption of nutrients (or contaminants). The exocrine portion of the pancreas secretes pancreatic juice, which contains numerous digestive enzymes. Additionally, the liver produces and secretes bile, which the gallbladder then concentrates. When fat is detected in the duodenum, the intestinal mucosa secretes the hormone cholecystolkinin, which stimulates the gallbladder to contract thereby releasing concentrated bile. The bile is especially important in terms of fat absorption as a result of fat droplet emulsification. The ileocecal vale, located at the end of the ileum, connects the small intestine to the large intestine. Digestion is essentially complete upon entry into the large intestine, and the large intestine serves as the passageway to the rectum where waste is excreted from the body.
State risk assessment calculations often assume that the Environmental Protection Agency's (EPA) requisite extraction techniques provide a reasonable estimate of contaminant bioavailability. These tests were designed to extract all of the contaminants present in the soil matrix. For example, EPA Method 3050 B, the standard EPA procedure for heavy metals, is an aggressive heated extraction procedure that utilizes 6 N nitric and hydrochloric acid (6). On the other hand, Oomen et al. (7) suggests a stomach pH range between one and two for the fasted state or between two and five for the fed conditions. While these tests do provide a conservative estimate of contaminant bioavailability, there are significant concerns that these tests are overly conservative and that they are unable to accurately predict differences in contaminant bioavailability among different sites. Numerous authors have highlighted the potential bioavailability overestimation when using EPA methods (5, 8-9 10 1112).
Substantially different human health impacts have been observed in different areas with the same measured soil chemical concentration. Steele et al. (13) found that blood levels for children in urban areas with lead concentrations of 1000 mg/kg have blood lead levels ranging from 1.1 to 7.6 (μg Pb/dL blood), while children living in mining areas with the same measured lead concentrations had blood lead levels ranging from 0 to 4.8 (μg Pb/dL blood). Dugan and Inskip (14) also often found increased blood levels in children who live in urban areas with lead levels ranging from 500-1000 mg/kg. Other studies have documented lowered lead bioavailability in mining areas as opposed to urban areas (11, 13, 15, 16).
Bioavailability tests have been developed to address this incongruity between the total amount of contaminant present at a site and the amount that would become available in the human gastrointestinal system,. These tests stem from earlier studies in the early 1980's that assayed iron availability in foods (e.g. 17). Due to differences in the desorption processes for organic chemicals, inorganic chemicals, and radionuclides, different but related bioavailability tests have been developed for each contaminant classification. Within these categories, different research groups have proposed in vitro tests, which are subsequently "validated" through in vivo animal experiments. The value of the in vitro test corresponds to its ability to reproduce the in vivo results. One limitation of this approach is that the correlation between in vivo results for certain animals and in vivo results from humans is unknown. It is possible then that an in vitro test could provide very accurate results in terms of the actual human bioavailability while not accurately reproducing results from a certain animal. This challenge makes determining the best bioavailability test among different physiologically reasonably tests difficult if not impossible. However, the National Research Council (18) recently recommended that the EPA allow human testing under strict scientific and ethical conditions. No tests have been conducted to date comparing human and animal or human and human bioavailability tests for soils, although human bioavailability studies have been performed using low concentrations of lead (19) and chromium (20) in soils. One possibility would be to spike soils with benign chemicals such as calcium or iron and to conduct a human in vivo study. However, the sorption and desorption mechanisms vary widely among chemicals. It may be inappropriate to extrapolate results from one chemical to other chemicals. Currently, correlating in vitro human bioavailability tests with in vivo animal experiments is the best method for evaluating in vitro tests. However, there is controversy regarding which animals, weanling pigs or weanling rats, are the most appropriate surrogate for children (21). It remains unclear which, if either, of these two surrogate animals best represents the child or adult GI system and whether either of these two animals are sufficiently accurate surrogates.
Ruby et al. (22) developed the most commonly used bioavailability test. This test was initially used to determine the bioavailability of arsenic and lead in mines wastes. Since this study, bioavailability students have been extended to numerous additional chemicals (organic, inorganic and radionuclides) (See Table 1). This table only includes studies of soil-borne chemicals using a human bioavailability test.
The vast majority of bioavailability studies has focused on inorganic chemicals, and specifically lead, cadmium, and arsenic. This focus is reasonable because lead, cadmium, and arsenic are common, problematic contaminants. Only one study focused specifically on radionuclides (35), and relatively few studies have examined organic contaminants (28, 38-40, 51-53)
Table 2 summarizes the current inorganic bioavailability tests. This table was modified from Oomen et al. (7) to include the physiologically based extraction test (PBET) (22), the in vitro gastrointestinal (IVG) method (23), and the artificial gastric fluid extraction (26). While other bioavailability tests have been published (i.e. 33), the tests summarized in Table 2 are the most frequently cited.
There are numerous differences among the various bioavailability tests. These differences are based predominately on the level of detail, or conversely assumptions, for a test. For example, most of the tests neglect the oral cavity and its impact on the bioavailability processes. In addition, many tests assume that absorption occurs predominately in the small intestine and that the contaminant concentration measured in the small intestine most accurately represents the bioavailable concentration. As long as the unique procedures accurately match the corresponding human physiological conditions, more detailed tests should better correlate with in vivo measurements, although the test may become more difficult to perform. Another difference among the tests is the pH of the stomach, which varies from 1.1 (RIVM Method) to 4.0 (SHIME Method) among the different tests (7). Using a significantly more acidic stomach environment would increase the bioavailability (22). Another difference among these tests is the bile type used. Oomen et al. (28) showed that the chicken bile significantly increases the measured bioaccessibility concentrations for lead and cadmium compared to ox and pig bile.
Three major processes dictate soil ingestion bioavailability (7). First, a certain fraction of the contaminants in soil become bioaccessible through the gastrointestinal tract (FB). Second, a certain fraction of the bioaccessible contaminants are transported across the small intestinal epithelium (FA). Third, a certain fraction of these contaminants are metabolized or otherwise detoxified by the liver (FH). The following equation describes the steps in contaminant bioavailability where F indicates the bioavailable fraction (7).
Many inorganic bioavailability assays ignore the third step because the enzymes in the liver are not expected to substantially detoxify or metabolize inorganic chemicals. Bioavailability tests typically assume that contaminant desorption from soils is the rate limiting step for contaminant absorption, although tests which model absorption (23) have been proposed. In other words, it is assumed that the bioavailable fraction is approximately equal to the bioaccessible fraction (i.e. FA and FH are approximately equal to one). On the other hand, enzymatic detoxification could be important for organic chemicals.
Organic bioavailability tests differ from inorganic bioavailability tests due to different desorption processes in the GI system. Ingested lipids are believed to be fundamental to the solubilization, transport, and absorption of many organic contaminants through the production of micelles (38, 23,) while the ingestion of lipids may be ignored in inorganic contaminant tests (22). Insights into the bioavailability of organic contaminants can be culled from pharmaceutical research. An excellent review of the effects of lipids, pH, and food digestion on drug absorption is Charman et al. (54). Lipid absorption and the concomitant release of bile salts have profound compound specific results, and the bile salts may increase or decrease bioavailability, which agrees with Oomen et al. (28). Stomach and/or small intestine pH is important when assessing the bioavailability of ions, weak acids, or weak bases. Norris et al. (24) review the oral absorption of microparticulates literature. They show evidence of microparticles absorption across the small intestine's cellular mucosa or through the lymphoid tissue, such as the Peyer's patches. Microparticulates are on the size range of a micrometer.
Organic contaminant bioavailability tests are summarized in Table 3. There are clear differences among the physiological parameter estimates among the tests (i.e. the small-intestinal times used) and among the level of detail used. There are additional test-specific manipulations not captured on this table. For example, Holman et al. (53) centrifuged and filtered their mixture prior to testing. This filtration was a unique aspect of this test and was designed to capture only the micelles, which are small enough to pass between the microvilli.
Bioavailability research on inorganic contaminants in soils or sediments has predominately focused on correlations with in vivo studies, mineralogical analyses of contaminant species in relationship to bioavailability, and assessing the influence of soil amendments on inorganic contaminant bioavailability. A thorough review of the early research focused on arsenic and lead bioavailability is available (25).
A primary focus of inorganic bioavailability research has been to elucidate the relationship between inorganic contaminant mineralogy and bioavailability. Several studies have assessed differences in lead mineralogy between lead contaminated soil from mine wastes and lead contaminated urban soils and how these mineralogical differences relate to bioavailability (11, 16, 22). The lead in Butte mine waste was found to be principally composed of lead phosphates, galena (PbS), and anglesite (PbSO4), while the lead from Bartlesville, Ok was primarily composed of manganese-lead oxide, iron-lead oxide, and lead phosphate. In addition, the Pb in Butte samples was frequently encapsulated in sulfide or silicate minerals, while the Pb in Bartlesville samples was generally present in separate particles. Accordingly, the lead in the Butte samples was found to be less bioavailable.
This increased understanding of bioavailability processes has fostered novel remediation techniques that capitalize on immobilizing lead through changing its specie to a less bioavailable form such as pryomorphite. The most commonly added amendment to expedite this transformation was phosphorous (21, 31, 32, 34 43, 44, 49, 50), and it was added in several different forms such as rock phosphate (31) or phosphoric acid (44). Other amendments include Portland cement (49), iron oxyhydroxides (49), biosolids (31, 32, 34, 49, 50), composted leaves (49), wood ash(34), sulfate (34), and manganese oxides (43). Field experiments using phosphorous treated soil combined with in vivo swine blood lead tests have confirmed the efficacy of phosphorous amendments to lead contaminated soils (21).
Ruby et al. (38) conducted the only known comparison between in vivo and in vitro results for organic chemicals. This study compared in vivo results for 2,3,7,8-tetracholorodibenzo-p-dioxin (TCDD) from various animals with in vitro results from a PBET modified for organic contaminants. This study discovered that the average bioavailability for TCDD was 25% and suggested that an inverse relationship exists between bioavailability and total organic carbon (TOC).
Oomen et al. (51) assessed the mobilization of polychlorinated biphenyls (PCBs) and lindane in a spiked surrogate soil. They determined that 35% of PCBs and 57% of lindane were bioavailable. In addition, roughly 25% of the total PCBs and 23% of the lindane were associated with bile salts at the conclusion of the bioavailability test. This finding corroborates previous discussions of the importance of bile salts in the desorption process for hydrophobic contaminants (54).
Numerous studies have assayed the impact of different amendments (i.e. food stuffs) or conditions (i.e. gallbladder emptying phase) on organic contaminant bioavailability. Hack and Selenka (52) assessed differences in PAH and PCB bioavailability based upon food amendments under both gastro-intestinal and gastric models. The addition of mucine (8 g/L) or whole milk powder (56 g/L) both substantially increased the bioavailable fraction of PAHs or PCBs in the gastro-intestinal model and the gastric model, although the whole milk powder had a greater impact. Wittsiepe et al. (39) performed a similar analysis using red slag Kieselrot, a material used as a surface layer in numerous sports fields in Germany that is contaminated with polychlorinated dibenzo-p-dioxins and dibenzofurans (PCDD/F). They also determined that higher bile content and whole powder milk resulted in significantly higher bioavailability (60%) compared to low bile content and the absence of food (5%). Similarly, Holman et al. (53) discovered that the total petroleum hydrocarbon and the crude oil bioavailability increased during fat digestion as compared to the gallbladder empty phase of fasting, and total petroleum hydrocarbon were consistently more bioavailable crude oil.
Past bioavailability research has yielded insights into soil-borne contaminant desorption processes in the GI system. Bioavailability tests have accurately predicted the contaminant bioavailability concentrations measured through in vivo studies. Current bioavailability work has also elucidated fundamental differences in bioavailability between organic and inorganic contaminants. Research has also stimulated novel remediation procedures with exciting potentials for in situ usage.
Nevertheless, many of the fundamental challenges, which initially faced the broad application of bioavailability to risk assessment, remain. As previously described, uncertainty in the correlation between in vivo studies using either weanling swine or Dawley-Sprague rats and the actual human GI system is problematic (21). In addition, correlations between bioavailability tests and in vivo studies exist for relatively few organic chemicals and radionuclides. Until more chemicals have been tested, reasonable doubts will remain concerning the applicability of a specific bioavailability assay for all chemicals and the ability of any specific test to capture the complex GI processes. Further, only a limited number of studies have compared the predicted bioavailability for specific contaminated soils among different bioavailability tests and/or in vivo experiments (7, 23). Additional critical comparisons among bioavailability tests are crucial in order to select a single (or several) bioavailability tests for risk assessment.
Another profound problem facing bioavailability tests is the notion that accounting for bioavailability is just a means to increase the acceptable contamination level (1). It should be realized that bioavailability is a parameter, although not always explicitly stated, in all risk assessment calculations. If not explicitly defined, the bioavailable fraction is assumed to be one hundred percent. It should also be realized that bioavailability tests have the potential to decrease, not change, or increase the acceptable soil contaminant levels at a specific site (26). Nevertheless, including bioavailability often does increase the soil cleanup level because only a fraction of the chemicals measured through aggressive extraction techniques (e.g. EPA 3050B) are typically bioavailable. Including bioavailability also improves the accuracy of the risk assessment calculations. While significant attention has been paid to bioavailability, corollary factors that would decrease the soil cleanup levels have been explored less rigorously. The potential for contaminant exposure at multiple sites (i.e. at home and at work) has the potential to yield unacceptably high intake levels even though the soil concentrations at both sites may be deemed acceptable. Some states (i.e. Massachusetts) have incorporated this possibility into their risk assessment models (27).
While the current bioavailability literature has yielded many exciting developments and enhanced scientific knowledge, numerous promising research directions exist. First, there is a glaring imbalance in the knowledge concerning organic contaminant bioavailability and even more so for radionuclides as compared to inorganic bioavailability. It is realized that much of the early bioavailability work focused on lead and arsenic due to their prevalence and their well-known human health impacts. Still, limited work has been conducted on organic contaminants and in vivo tests have not been conducted with many of the widespread environmental contaminants (TCE, PCE, etc.). Second, all of the past bioavailability research has been on single soil measurements. There have been attempts to tease correlations between contaminant bioavailability and soil properties, but there has not been a systematic investigation of the effects of a soil parameter (such as diagenetic age) with contaminant bioavailability. Recent research has developed a distributed reactivity model for soil sorption that has yielded insights into differences in sorption characteristics between soft and hard carbons (e.g. 28). It is expected that differences in the relative proportions of soft and hard carbons would profoundly impact organic contaminant bioavailability. Third, none of the currently developed bioavailability assays have accounted for absorption of microparticulates across the intestinal wall or through Peyer's Patches. Assessing the absorption potential of small soil particles may yield new bioavailability pathways. The finding that soil-borne chemical concentration typically increases with decreasing soil particle size augments the importance of these findings. Fourth, testing common assumptions of many bioavailability tests may lead to additional insights. Oomen et al. (7) proposes three fundamental steps for contaminants to enter blood circulation. Further exploring these steps for different classes of contaminants may help narrow the gap between the current in vivo results and the in vitro bioavailability assays. A better understanding of these steps will also reveal the accuracy of assumptions in other bioavailability tests. Some work has already been conducted regarding these steps for lead bioavailability (41). Fifth, bioavailability estimates of chemicals in soils could be assessed using human subjects for the most common, and these results could be compared to in vivo animal studies and human bioavailability tests. While the chemical concentrations used in such a study would have to be significantly below potentially harmful levels, extrapolating these results is more reasonable than extrapolations from animal studies where incredibly high dosages were used. It is also possible that rough correlations could be obtained through epidemiological analyses.
