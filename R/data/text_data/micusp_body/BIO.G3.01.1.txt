As Platt (1964) argued, hypothesis-testing is essential to rapid scientific progress, with dispositive studies guiding it efficiently along a "logical tree" akin to a chemist's flow chart or "conditional computer program." Moreover, this "strong inference" approach attains its greatest efficiency when multiple alternative hypotheses are tested simultaneously, enabling researchers to quickly eliminate fruitless branches of the logical tree (Platt 1964).
Now that many ecologists rely upon computer programming to generate models and run simulations, the question arises of whether such experience in constructing parsimonious sequences of conditional propositions has promoted the use by these scientists of a hypothesis-testing approach in their empirical ecological investigations. Furthermore, because scientific investigation is strongly influenced by the culture in which it is embedded (Kuhn 1970), perhaps even ecologists who do not themselves engage in programming-aided theoretical studies would nevertheless display heightened hypothesis-testing if they are affiliated with institutions that emphasize computer reliant theoretical research.
Based upon these considerations, I propose the Programming Promotes Hypothesis-Testing Hypothesis (PPHTH), which states that ecologists occurring in habitats in which computer-aided theoretical modeling is emphasized should show increased frequency, rigor, and efficiency of hypothesis testing when compared with ecologists from other milieus. In the present paper, as a test of this hypothesis, I compare empirical ecological research produced at an institution that places an extremely high emphasis upon computer-assisted, theoretical studies with empirical ecological studies emanating from a more broad-based institution.
For the computer-rich environment, I chose the University of Chicago's Department of Ecology and Evolution (Chicago), home to a self-styled "naturalist" publication in which the most frequently studied species are named "x" and "y", and in which a computer monitor passes for an observation blind. Moreover, this university served as Platt's intellectual home, as he was a professor there (Platt 1964) and, as a whole, it has historically been steeped in theory, from Fermi in physics to Friedman in finance. I compared research of ecologists who are Chicago professors with research produced by ecologists who are professors of the Department of Ecology and Evolutionary Biology of the University of Michigan at Ann Arbor ( Michigan).
The latter institution, while certainly strong in ecological theory, does not, unlike the former, place such a strong emphasis on computer programming that its graduate school application largely comprises an assessment of such skills (Rosenthal, pers. obs.). In other respects, however, the institutions are similar, especially in that they both have highly prominent ecological research programs.
For Chicago, I visited the Department of Ecology and Evolution web page listing faculty research areas (http://pondside.uchicago.edu/ecol-evol/research/), identifying as ecologists all those professors listed under the Ecology heading. For Michigan, my search similarly started at the Department of Ecology and Evolutionary Biology website listing faculty research interests (http://www.eeb.lsa.umich.edu/eebfaculty.asp). However, because this site did not have a separate list for ecology or ecologists, I identified as ecologists those professors for whom the word ecology appeared in the description of their field of interest. For both sites, sampling was conducted in February, 2002.
After the ecologists were identified, ecological publications were selected in the following manner: I visited each ecologist's individual web page, and went through his or her posted list of publications in reverse chronological order (with publication titles within the same year examined in alphabetical order), up to and including 1997, if necessary, until one suitable publication title was encountered. Publications were excluded if they were books rather than journal articles; or if they were not available either online or at the Shapiro Science Library; or if their titles reflected material that was either not ecological in nature (e.g., single species ethological studies); or clearly not explanatory in nature (e.g., an account of fish recruitment rates) or purely theoretical, without an empirical component. For some of the ecologists, no suitable publication was found, and therefore these individuals were not represented in the study.
Because of the frequent changes in institutional affiliations prevalent in academia, it is quite likely that some of the publications selected in the above manner were written, submitted or published prior to the ecologist's affiliation with their current institution. However, I decided that this should not preclude the inclusion of such publications in the current study, because: 1) their current institutions presumably selected faculty who reflected their own institutional cultures; and 2) the faculty members presumably chose institutions that encouraged their own approaches. Therefore, the different degrees of emphasis on theoretical approaches, and the predicted consequences for hypothesis testing should be reflected even in such prior publications.
Co-authorship of papers was not a bar to their inclusion in the study, as elimination of studies having multiple authors would have reduced the number of reviewed articles to an unworkably small number (quite close to zero). Moreover, as co-authored studies were included from both institutions, this approach should not affect the outcome of a comparison between the two.
For the purposes of this study, the hypothesis testing process was defined as containing the following four essential elements: 1) proposal of an a priori hypothesis (a posteriori hypotheses being excluded here because, by definition, they are not tested within the published studies within which they appear); 2) formulation of at least one factual prediction predicated upon this hypothesis, such that results contrary to the prediction would necessitate rejection of the hypothesis; 3) performance of a test to determine whether the prediction is borne out; and 4) comparison of the test's results to those predicted by the hypothesis, yielding a conclusion that the hypothesis has accordingly been rejected or supported.
In operationalizing the four elements outlined above, I emphasized substance, rather than form, such that:
The hypothesis-testing activity in the two populations of publications was assessed with respect to frequency, rigor, and efficiency as follows:
For each publication, the presence or absence of each of the four functional elements was recorded. Only those publications having all four present were classified as hypothesis-testing ("HT"). I then computed the proportion of the total that were HT. If PPHTH were to obtain, the HT proportion should be higher in the Chicago than the Michigan sample.
Each publication was examined to determine if it purported to evaluate hypotheses (i.e., by explicitly making this assertion). For each institution, the proportion of such publications that also qualified as HT was then computed. This approach was undertaken in order to evaluate the relative success of the two institutions in achieving hypothesis testing when explicitly attempted. If PPHTH were to obtain, the proportion of "rigorous" studies should be higher in the Chicago than the Michigan sample.
Each HT publication was examined to determine if it tested multiple alternative hypotheses ("MAHs") was computed. For a publication to be designated as testing MAHs, it would need to have two or more non-null a priori hypotheses (as defined above) with each of them tested pursuant to the criteria listed above. I then computed the proportion of the HT publications that tested MAHs. If PPHTH were to obtain, the MAH proportion should be higher for the Chicago than the Michigan studies.
The individual evaluations of the selected studies are presented in Table 1.
The group comparisons for hypothesis-testing assays are displayed in Table 2. As Table 2 shows, whereas the values for hypothesis-testing frequency are the same for the two samples, those for both rigor and efficiency are higher in the Chicago sample.
The results of this study provide limited support, at most, for PPHTH. Although there was a higher efficiency value for the Chicago publications, this was simply due to the fact that its only hypothesis-testing article evaluated multiple alternative hypotheses. It would be an over-generalization-based solely upon this single episode of MAH evaluation -- to characterize the Chicago ecologists, as a whole, as more efficient in their hypothesis-testing.
On the other hand, the difference in the values for the rigor index is more likely to reflect an actual disparity between the ways in which ecological research is framed at the two institutions. Although at both departments only one out of the five reviewed studies met all the hypothesis-testing criteria, at Michigan a majority (3 of 5) of publications asserted that they were evaluating hypotheses. At Chicago, in contrast, the only study to mention hypotheses was the one that actually tested them. There, 3 of the remaining 4 studies (Dwyer et al. 1998; Pfister 1998; Wootton 2001) focused instead upon the parameterization of mathematical models. Thus, the institutional difference in rigor values might be due to the Michigan ecologists more frequently attempting to fit their research into a hypothesis-testing framework (whether justified or not) -- whereas the bulk of Chicago research addresses simulations and other mathematical manipulations.
The one prediction of PPHTH that clearly was not borne out was that of a higher frequency of hypothesis-testing in the Chicago publications. Instead, the frequency was identically low for both institutions, due perhaps to Chicago ecologists largely eschewing hypothesis-testing in favor of more purely theoretical investigations, while Michigan ecologists tackled topics that are of such scale that they pose practical obstacles to adequate testing (e.g., Pascual et al. 2000 ; Vandermeer et al. 2000).
In conclusion, the results of the present study suggest that although a programming-oriented environment does not increase the frequency of hypothesis-testing, it does reduce the frequency with which it is claimed, thereby increasing its apparent rigor. Additionally, this environment might increase the efficiency of hypothesis-testing, but this effect is far from clear.
