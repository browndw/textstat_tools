In Convergence of Simulation-Based Policy Iteration (SBPI), Cooper, Henderson and Lewis are able to derive conditions under which a novel algorithm for computing optimal policies for Markov decision processes (MDPs). They point out that SBPI, originally suggested by Bertsekas and further developed by Cao, works similarly to the modified policy iteration algorithm (MPIA) and to the actor critic algorithm. It is therefore expected to converge faster than by the value iteration algorithm, yet the exact conditions under which SBPI converges had not been developed. That places the results of this paper in a position of great importance, and its applicability is shown in the description of methods to converge to estimators via simulation. These estimators can then be used to ensure the "almost-sure" convergence of SBPI.
Let us begin by comparing MPIA and SBPI. It is important to point out that the model used in this paper is the average-reward model, not the discounted reward model as studied thus far in IOE 512. This distinction required that I understand new measures:
for a policy
Since the AEE (average evaluation equations) are satisfied by a unique vector h, if we can find a solution (g, h) that satisfies
The two algorithms are similar in that they produce estimates (
For simplicity's sake, I appreciated the Ratio Estimator given in section 5.3. As the authors describe it, it "is perhaps the least complex to implement, as it can be applied based on a single simulated sample path." The estimator of
What I like about the paper, and the Ratio Estimator in particular, is that it shows how a computationally inhibitive MDP can be solved iteratively, in a finite (although perhaps very large) number of steps. It is possible to code such a problem, and the assurance of convergence outlined herein makes SBPI an attractive alternative. What I would like to know is how the convergence rate of SBPI compares with those of the actor-critic and MPI algorithms mentioned by the authors, as well as what constitutes a legitimate stopping rule in the computation of the
