<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE TEI.2 SYSTEM "tei_bawe.dtd">
<TEI.2 id="_6175c" n="version 1.0">
<teiHeader>
<fileDesc>
<titleStmt>
<title>Prediction of the onset of Parkinsonian tremor from the sub-thalamic nucleus Local Field Potentials Using Artificial Neural Networks</title>
</titleStmt>
<extent/>
<publicationStmt>
<distributor>British Academic Written English (BAWE) corpus</distributor>
<availability>
<p>The British Academic Written English (BAWE) corpus was developed at the Universities of Warwick, Reading and Oxford Brookes, under the directorship of Hilary Nesi and Sheena Gardner (formerly of the Centre for Applied Linguistics [previously called CELTE], Warwick), Paul Thompson (Department of Applied Linguistics, Reading) and Paul Wickens (Westminster Institute of Education, Oxford Brookes), with funding from the ESRC. Subject to the rights of the these institutions in the BAWE corpus, and pursuant to the ESRC agreement, the BAWE corpus is available to researchers for research purposes PROVIDED THAT the following conditions are met:</p>
<p>1. The corpus files are not distributed in either their original form or in modified form.</p>
<p>2. The texts are used for research purposes only; they should not be reproduced in teaching materials.</p>
<p>3. The texts are not reproduced in full for a wider audience/readership, although researchers are free to quote short passages of text (up to 200 running words from any given text).</p>
<p>4. The BAWE corpus developers (contact: BAWE@warwick.ac.uk) are informed of all projects, dissertations, theses, presentations or publications arising from analysis of the corpus.</p>
<p>5. Researchers acknowledge their use of the corpus using the following form of words: "The data in this study come from the British Academic Written English (BAWE) corpus, which was developed at the Universities of Warwick, Reading and Oxford Brookes under the directorship of Hilary Nesi and Sheena Gardner (formerly of the Centre for Applied Linguistics [previously called CELTE], Warwick), Paul Thompson (Department of Applied Linguistics, Reading) and Paul Wickens (Westminster Institute of Education, Oxford Brookes), with funding from the ESRC (RES-000-23-0800)."</p>
</availability>
</publicationStmt>
<notesStmt>
<note/>
</notesStmt>
<sourceDesc>
<p n="level">4</p>
<p n="date">2007-02</p>
<p n="module title">Neural Networks</p>
<p n="module code">CYMN2</p>
<p n="genre family">Research report</p>
<p n="discipline">Cybernetics &amp; Electronic Engineering</p>
<p n="disciplinary group">PS</p>
<p n="grade">D</p>
<p n="number of authors">1</p>
<p n="number of words">2521</p>
<p n="number of s-units">119</p>
<p n="number of p">21</p>
<p n="number of tables">2</p>
<p n="number of figures">5</p>
<p n="number of block quotes">0</p>
<p n="number of formulae">1</p>
<p n="number of lists">0</p>
<p n="number of paragraphs formatted like lists">0</p>
<p n="abstract present">abstract present</p>
<p n="average words per s-unit">21.2</p>
<p n="average s-units per p">5.7</p>
<p n="macrotype of assignment">simple assignment</p>
</sourceDesc>
</fileDesc>
<encodingDesc>
<p>TEI P4 (documented in: BAWE.documentation.pdf)</p>
</encodingDesc>
<profileDesc>
<particDesc>
<person>
<p n="gender">m</p>
<p n="year of birth">1973</p>
<p n="first language">Thai</p>
<p n="education">OSa</p>
<p n="course">MSc. in Cybernetics</p>
<p n="student ID">6175</p>
</person>
</particDesc>
</profileDesc>
</teiHeader>
<text>
<front>
<titlePage>
<docTitle>
<titlePart rend="bold">PREDICTION OF THE ONSET OF PARKINSONIAN TREMOR FROM THE SUB-THALAMIC NUCLEUS LOCAL FIELD POTENTIALS USING ARTIFICIAL NEURAL NETWORKS </titlePart>
</docTitle>
<titlePart>
<name type="student name"/> MSc. Cybernetics Email: <name type="other"/>
</titlePart>
</titlePage>
</front>
<body>
<div1 type="abstract">
<head rend="bold">ABSTRACT</head>
<p n="p1.21">
<s n="s1.4;p1.21">This assignment tried to predict the occurrence of the tremor in Parkinson's disease from the local field potentials (LFPs) recorded from sub-thalamus by using artificial neural networks. </s>
<s n="s2.4;p1.21">This may contribute to design a demand-driven intelligent stimulator. </s>
<s n="s3.4;p1.21">The data recorded from sub-thalamus was processed by using some statistic method before being trained by neural network. </s>
<s n="s4.4;p1.21">The experimental results show that it is possible to use neural networks to predict Parkinsonian tremor by using the LFPs recorded from sub-thalamic nucleus. </s>
</p>
</div1>
<div1 type="section">
<head rend="bold">1. INTRODUCTION</head>
<p n="p2.21">
<s n="s1.12;p2.21">Parkinson's disease (PD) is a neurodegenerative disorder characterized by many forms of change in motor function known by neurological signs like tremor, muscle rigidity, bradykinesia, akinesia and loss of postural reflexes [1]. </s>
<s n="s2.12;p2.21">Such symptoms tend to be associated with basal ganglia, thalamus and motor cortex [1]. </s>
<s n="s3.12;p2.21">Parkinson's tremor is the most common form of resting tremor caused by idiopathic Parkinson's disease [2]. </s>
<s n="s4.12;p2.21">This tremor of Parkinson's disease is one of the involuntary movement disorders affecting many people, in particular, elderly. </s>
<s n="s5.12;p2.21">There are several possible therapies for Parkinson's disease. </s>
<s n="s6.12;p2.21">These approaches include L-Dopa, antioxidants, drugs which stimulate dopamine receptors, drugs which block glutamate, drugs which decrease apoptosis, neurotrophins, high-frequency electrical stimulation of the globus pallidus and transplants of neurons from a fetus, but such treatments have some limitations [3]. </s>
<s n="s7.12;p2.21">Patients usually have internal globus pallidus Deep Brain Stimulation (DBS) for surgical treatment [2]. </s>
<s n="s8.12;p2.21">Some patients are implanted a deep brain electrode into thalamus or sub-thalamus. </s>
<s n="s9.12;p2.21">Such treatments are less dangerous than lesioning, particularly subthalamic nucleus stimulation [4]. </s>
<s n="s10.12;p2.21">The cost of Deep Brain Stimulation (DBS), however, limits the number of patients using this to stimulate deep brain structure because of the frequent replacement of batteries [4]. </s>
<s n="s11.12;p2.21">Demand-driven Deep Brain Stimulation may be a solution to this problem. </s>
<s n="s12.12;p2.21">To achieve this, the occurrence of tremor must be predicted. </s>
</p>
<p n="p3.21">
<s n="s1.7;p3.21">Artificial neural networks (ANNs), the field of artificial intelligence, emerge from the development of the first neural model by McCulloch and Pitts in the 1940s. </s>
<s n="s2.7;p3.21">Since then many researchers have become increasingly interested in artificial neural networks, commonly referred to as "neural networks" in [5]. </s>
<s n="s3.7;p3.21">There are many advantages of neural networks such as machine learning, generalization, adaptation and fault tolerance. </s>
<s n="s4.7;p3.21">Artificial neural network can be applied to medicine or biomedical engineering domain. </s>
<s n="s5.7;p3.21">The medical applications fall into four basic fields: modeling, bioelectric signal processing, classification for diagnosis and prediction for prognostics [6]. </s>
<s n="s6.7;p3.21">A lot of research shows that neural networks have been used successfully in many areas in prediction such as the prediction of ovarian cancer and also several types of cancer [7]. </s>
<s n="s7.7;p3.21">It is likely that neural networks might be utilised in the prediction of the occurrences of tremor in Parkinson's disease from sub-thalamic nucleus (STN) LFPs signal. </s>
</p>
</div1>
<div1 type="section">
<head rend="bold">2. IMPLEMENTATION</head>
<div2>
<head rend="bold">2.1 Pre-processing</head>
<p n="p4.21">
<s n="s1.6;p4.21">Figure 1 shows the local field potentials recorded from sub-thalamus with a sampling rate of 800 Hz. </s>
<s n="s2.6;p4.21">This data will be used for an input vector. </s>
<s n="s3.6;p4.21">However, a large number of the input of a neural network may increase the size of a neural network. </s>
<s n="s4.6;p4.21">The reduction in input dimensionality is achieved by pre-processing. </s>
<s n="s5.6;p4.21">To do this, the data should be analysed. </s>
<s n="s6.6;p4.21">Figure 2 presents the envelop signal of the electromyogram (EMG) with the same sampling rate. </s>
</p>
<figure id="BAWE_6175c-fig.001">
<head>
<hi rend="bold">Figure 1.</hi> STN LFPs over the onset and cessation of the tremor</head>
</figure>
<figure id="BAWE_6175c-fig.002">
<head>
<hi rend="bold">Figure 2.</hi> EMG signal over the onset and cessation of the tremor</head>
</figure>
<p n="p5.21">
<s n="s1.10;p5.21">As can be seen from figure 1 and 2, the pattern between the amplitude of STN LFPs and that of EMGs is an alternating pattern. </s>
<s n="s2.10;p5.21">In other words, the amplitude of EMGs increases sharply when the tremor occurs while figure 1 show that the variance of STN LFPs decreases dramatically at the occurrences of tremor. </s>
<s n="s3.10;p5.21">Consequently, it is necessary to perform pre-processing procedures. </s>
<s n="s4.10;p5.21">First of all, the input data will be scaled. </s>
<s n="s5.10;p5.21">The approach for this is to normalize the mean and standard deviation of this data and the new data will then have zero mean and unity standard deviation. </s>
<s n="s6.10;p5.21">The next procedure is to find the variance (<hi rend="sup">2</hi>) of the normalized raw input data every 800 sample; as a result, the new data will be 100 input data in time domain (see figure 3). </s>
<s n="s7.10;p5.21">Not only was pre-processing applied for an input vector, but also target vector. </s>
<s n="s8.10;p5.21">The target vector based on EMG signal was set into two states after finding the mean value of EMGs data at each second. </s>
<s n="s9.10;p5.21">The two states are defined as normal or abnormal conditions. </s>
<s n="s10.10;p5.21">The onset of tremor is set to 1, whereas the cessation of tremor is set to 0. </s>
</p>
<figure id="BAWE_6175c-fig.003">
<head>
<hi rend="bold">Figure 3.</hi> The variance (<hi rend="sup">2</hi> ) of the normalized raw input data</head>
</figure>
</div2>
<div2>
<head rend="bold">2.2 The types of neural networks</head>
<p n="p6.21">
<s n="s1.7;p6.21">There are many way to implement neural networks. </s>
<s n="s2.7;p6.21">The two supervised learning neural networks like a Multi-layer perceptrons (MLPs) network and a Radial Basis Function (RBF) network are most commonly used in a lot of research, particularly feed-forward MLP networks [8]. </s>
<s n="s3.7;p6.21">The reason for this is that a wide variety of problems can be solved by MLP networks due to the capacity of input-output mapping [8]. </s>
<s n="s4.7;p6.21">In addition to this, MLP networks are more likely to outperform RBF networks and also requires a smaller number of parameters than RBF networks in case of nonlinear input-output mapping for the same degree of accuracy [5], [9]. </s>
<s n="s5.7;p6.21">As a consequence, MLP networks tend to be more accurate than RBF networks, although RBF networks train faster than MLP networks [10]. </s>
<s n="s6.7;p6.21">However, the relative importance of selection of the type of neural networks depends on the application. </s>
<s n="s7.7;p6.21">This experiment, therefore, uses an MLP network for prediction of the occurrences of tremor in Parkinson's disease from STN LFP signals. </s>
</p>
</div2>
<div2>
<head rend="bold">2.3 The neural networks architecture </head>
<p n="p7.21">
<s n="s1.7;p7.21">In designing an MLP network, some variables must be determined. </s>
<s n="s2.7;p7.21">These variables are the number of input node, the number of hidden layer, the number of hidden neurons and the number of output neurons. </s>
<s n="s3.7;p7.21">The selection of these parameters depends on the problem. </s>
<s n="s4.7;p7.21">However, MLP networks architecture is not completely constrained by the problem. </s>
<s n="s5.7;p7.21">This is because the number of inputs nodes is constrained by the problem and the number of neurons in the output layer is constrained by the number of outputs which is required by the problem, whereas the number of hidden layers and the number of neurons in the hidden layers depend on designer. </s>
<s n="s6.7;p7.21">In this experiment, there is one input parameter which is STN LFPs; hence, there will be one input node in the input layer. </s>
<s n="s7.7;p7.21">For output layer, this should be one output neuron corresponding to a target vector. </s>
</p>
<p n="p8.21">
<s n="s1.6;p8.21">In determining the number of hidden layers and the number of hidden neurons, there are no formulae for this. </s>
<s n="s2.6;p8.21">The most common forms of the architecture of neural networks are one or two hidden layers and it does not need more than two hidden layer because the accuracy depends on the number of neurons per layer [8],[10]. </s>
<s n="s3.6;p8.21">Furthermore, one hidden layer with sigmoid transfer function can train for any mapping between inputs and outputs with arbitrary accuracy [9], [10]. </s>
<s n="s4.6;p8.21">In addition, the number of hidden neurons was determined by trial-and-error, which is the common way to select the number of hidden neurons [10]. </s>
<s n="s5.6;p8.21">This experiment is firstly to compare the accuracy of the number of hidden neurons which are 50, 100 and 150 and the performance between one and two hidden layers was then compared by using two topologies with the same input data and the number of neurons such as 1-100-1 and 1-50-50-1 network. </s>
<s n="s6.6;p8.21">The former topology means the one hidden layer network with 1 input node-100 hidden neurons-1 output neuron and the latter topology is the two hidden layer network with 1 input node-50 hidden neurons-50 hidden neurons-1 output neuron. </s>
</p>
</div2>
<div2>
<head rend="bold">2.4 The selection of activation function</head>
<p n="p9.21">
<s n="s1.3;p9.21">There is some consensus on which activation function should be used for hidden neurons. </s>
<s n="s2.3;p9.21">In general, a large number of researchers mainly use standard sigmoid or logistic activation function for hidden neurons [8]. </s>
<s n="s3.3;p9.21">The equation for this function displays as below </s>
</p>
<p n="p10.21">
<s n="s1.1;p10.21">
<formula notation="" id="BAWE_6175c-form.001"/> </s>
</p>
<p n="p11.21">
<s n="s1.5;p11.21">Moreover, the target output values range from 0 to 1. </s>
<s n="s2.5;p11.21">This exactly matches a logistic activation function which has a range of [0, 1]. </s>
<s n="s3.5;p11.21">This seems well suited for any neurons in this experiment. </s>
<s n="s4.5;p11.21">Such transfer function may contribute substantially to the accuracy and speed of training. </s>
<s n="s5.5;p11.21">As a consequence of this, a logistic activation function has been selected for both hidden neurons and an output neuron. </s>
</p>
</div2>
<div2>
<head>
<hi rend="bold">2.5 The selection of</hi> <hi rend="bold">training algorithm</hi>
</head>
<p n="p12.21">
<s n="s1.7;p12.21">The standard backpropagation algorithm based on the gradient descent algorithm often has a poor convergence rate and dependence on some parameters which is set by the user such as learning rate and momentum. </s>
<s n="s2.7;p12.21">Such parameters are sensitive to the performance of algorithm. </s>
<s n="s3.7;p12.21">Several variations on backpropagation have been developed with faster convergence such as conjugate gradient algorithm and Levenberg-Marquardt algorithm [11]. </s>
<s n="s4.7;p12.21">A modified conjugate gradient algorithm known as the scaled conjugate gradient algorithm based upon the conjugate gradient method was designed to avoid the time-consuming line search by using the Levenberg-Marquardt approach called the model-trust region approach in order to scale an appropriate step size [12],[13]. </s>
<s n="s5.7;p12.21">This makes the algorithm more computationally efficient. </s>
<s n="s6.7;p12.21">Such an algorithm is a fast algorithm and seems to perform well over many problems, in particular for large networks; therefore, this is a very good general purpose training algorithm and it also work well with using early stopping method [12],[13]. </s>
<s n="s7.7;p12.21">This is the reasons why a scaled conjugate gradient algorithm was used in this experiment. </s>
</p>
</div2>
<div2>
<head rend="bold">2.6 Data partitioning</head>
<p n="p13.21">
<s n="s1.5;p13.21">As can be seen from figure 2, the tremor occur around 18 <hi rend="sup">th</hi> and 50 <hi rend="sup">th</hi> second. </s>
<s n="s2.5;p13.21">In this experiment, the onset of the tremor will be predicted around this time. </s>
<s n="s3.5;p13.21">To do this, two groups of data will be set by the division of each group into training, validation and test sets. </s>
<s n="s4.5;p13.21">The first group for prediction of tremor around 50 <hi rend="sup">th</hi> second was divided by the ratios 4:1:1 for training, validation and test sample, while the second group for prediction of tremor around 18 <hi rend="sup">th</hi> second was divided by the ratios 1:1:1 for training, validation and test sample. </s>
<s n="s5.5;p13.21">The reason for this is that these ranges are the same as the target values which have both states of tremor (0 and 1). </s>
</p>
</div2>
<div2>
<head rend="bold">2.7 Training and testing</head>
<p n="p14.21">
<s n="s1.5;p14.21">In this experiment, both group of input data was trained and tested by neural network toolbox in MathWorks Inc. </s>
<s n="s2.5;p14.21">Matlab version 7.0. </s>
<s n="s3.5;p14.21">In addition, the mean squared error (MSE) was used for the measure of performance. </s>
<s n="s4.5;p14.21">However, some problems that might occur when the neural network is being trained are over-fitting and also poor generalization performance to unseen data. </s>
<s n="s5.5;p14.21">To avoid the over-fitting problem, the stopping criterion or 'early stopping' approach which works well with a scaled conjugate gradient backpropagation was used. </s>
</p>
</div2>
</div1>
<div1 type="section">
<head rend="bold">3. RESULTS</head>
<p n="p15.21">
<s n="s1.3;p15.21">Table 1 illustrates the accuracy of each topology. </s>
<s n="s2.3;p15.21">As can be seen from this table, the topologies with 100 hidden neurons have the minimum values of MSE for both one and two hidden layers, which are 0.0118 and 0.0132 respectively. </s>
<s n="s3.3;p15.21">It is clear from Table 1 that the best topology seems to be 1-100-1 network. </s>
</p>
<table id="BAWE_6175c-tab.001">
<head>
<hi rend="bold">Table 1.</hi> Experimental result1</head>
<row>
<cell/>
</row>
</table>
<p n="p16.21">
<s n="s1.2;p16.21">Table 2 demonstrates the performance of both one and two hidden layers by the comparison between the 1-100-1 network and the 1-50-50-1 network. </s>
<s n="s2.2;p16.21">The MSE of 1-100-1 network is slightly different from 1-50-50-1 network. </s>
</p>
<table id="BAWE_6175c-tab.002">
<head>
<hi rend="bold">Table 2.</hi> Experimental result2</head>
<row>
<cell/>
</row>
</table>
<p n="p17.21">
<s n="s1.4;p17.21">Figure 4 and 5 illustrate the results of 1-50-50-1 network and 1-100-1 network respectively, compared with the target values. </s>
<s n="s2.4;p17.21">Generally, the results of both networks are almost the same although 1-100-1 network is more slightly accurate than 1-50-50-1 network. </s>
<s n="s3.4;p17.21">The 1-50-50-1 network almost reaches the target at the 15 <hi rend="sup">th</hi> second, while the 1-100-1 network can achieve the target at this time. </s>
<s n="s4.4;p17.21">It seems that both networks predict the onset of Parkinsonian tremor at the 15 <hi rend="sup">th</hi> second and again at the 44 <hi rend="sup">th</hi> second. </s>
</p>
<figure id="BAWE_6175c-fig.004">
<head>
<hi rend="bold">Figure 4.</hi> The predicted values of the states of tremor in Parkinson's disease by the 1-50-50-1 network</head>
</figure>
<figure id="BAWE_6175c-fig.005">
<head>
<hi rend="bold">Figure 5.</hi> The predicted values of the states of tremor in Parkinson's disease by the 1-100-1 network</head>
</figure>
</div1>
<div1 type="section">
<head rend="bold">4. DISCUSSION</head>
<p n="p18.21">
<s n="s1.4;p18.21">Overall, it is more likely that the onset of the tremor in Parkinson's disease can be predicted by using artificial neural network and STN LFPs. </s>
<s n="s2.4;p18.21">It is evident that selecting the appropriate number of hidden neurons has a significant impact on the performance of training and generalisation. </s>
<s n="s3.4;p18.21">Moreover, it appears that an increase in the number of hidden layers can lead to a slight decrease in the accuracy of training and generalisation. </s>
<s n="s4.4;p18.21">As a result of this, the two hidden layers cannot improve significantly performance, compared to one hidden layer with the same number of hidden neurons. </s>
</p>
<p n="p19.21">
<s n="s1.12;p19.21">Nevertheless, there may be some errors in experimental results for both networks. </s>
<s n="s2.12;p19.21">Poor training performance might be a major problem in this experiment. </s>
<s n="s3.12;p19.21">There is some possible reason for this problem. </s>
<s n="s4.12;p19.21">Initialisation of weights may cause such a problem if the magnitudes of weights are too large. </s>
<s n="s5.12;p19.21">Another problem probably occurs is poor generalisation performance. </s>
<s n="s6.12;p19.21">There are several possible reasons for this. </s>
<s n="s7.12;p19.21">Over-training may be the reason why generalisation performance on the test set is poor. </s>
<s n="s8.12;p19.21">This is because the noise on the training patterns might be learnt. </s>
<s n="s9.12;p19.21">Another cause of poor generalisation performance may be over-fitting. </s>
<s n="s10.12;p19.21">The possibly reason for this is that the neural network learns the detail of the training patterns and then fit the noise causing a decrease in the generalisation performance on the test data. </s>
<s n="s11.12;p19.21">A local minimum on the error surface might also cause poor generalisation performance. </s>
<s n="s12.12;p19.21">It is worth noting, however, that the other considered approaches can achieve a comparable, or even better, performance of training and generalisation after fine-tuning the values of the parameters including the number of hidden neurons, the number of hidden layer initial weights and some parameters in training algorithm. </s>
</p>
</div1>
<div1 type="section">
<head rend="bold">5. CONCLUSIONS </head>
<p n="p20.21">
<s n="s1.5;p20.21">This assignment presents the use of artificial neural network to predict the onset of the tremor in Parkinson's disease from the local field potentials (LFPs) recorded from sub-thalamus. </s>
<s n="s2.5;p20.21">MLP neural network was used for the prediction task. </s>
<s n="s3.5;p20.21">Based on these results, it seems generally clear that the MLP neural network using one hundred hidden neurons is more likely to predict the onset of Parkinsonian tremor, even though it is not more effective enough because of the existence of errors. </s>
<s n="s4.5;p20.21">The experimental results obtained suggest that the impact of the number of hidden neurons is significant for the performance of training and generalisation. </s>
<s n="s5.5;p20.21">Overall, selecting one hidden layer with approximately 100 hidden neurons including logistic activation function and being trained by a scaled conjugate gradient algorithm appear to be an appropriate choice for MLP neural network in this experiment. </s>
</p>
</div1>
<div1 type="section">
<head rend="bold">6. FUTURE WORK</head>
<p n="p21.21">
<s n="s1.4;p21.21">In a future correspondence, alternative topologies need to be investigated, as well as the potential benefits from employing genetic algorithms [14], [15] for training neural networks along with an implementation of pruning the networks. </s>
<s n="s2.4;p21.21">Genetic algorithms have a better chance of finding a global optimum; hence, it can avoid local minima in the error surface. </s>
<s n="s3.4;p21.21">Network pruning techniques can eliminate unnecessary synaptic weights in a neural network. </s>
<s n="s4.4;p21.21">Such a technique is known as the Optimal Brain Surgeon (OBS) algorithm [16]. </s>
</p>
</div1>
</body>
<back>
<div1 type="bibliography">
<head rend="bold">7. REFERENCES</head>
<p>[1] R. Edwards, A. Beuter and L. Glass. (1999). Parkinsonian Tremor and Simplification in Network Dynamics. <hi rend="italic">Bulletin of Mathematical Biology.</hi> [Online]. 51, pp. 157-177. Available: <seg type="URL" n="http://www"/>. idealibrary.com</p>
<p>[2] R. Bhidayasiri. (2005). Differential diagnosis of common tremor syndromes. <hi rend="italic">Postgrad. Med. J.</hi> [Online]. 81, pp. 756-762. Available: <seg type="URL" n="http://pmj.bmj.com/cgi/"/> content/full/81</p>
<p>/962/756</p>
<p>[3] J.W. Kalat, <hi rend="italic">Biological psychology</hi>, Australia: Thomson-Wadsworth, 2004, pp. 252-253.</p>
<p>[4] M.N. Gasson, S.Y. Wang, T.Z. Aziz, J.F. Stein, K Warwick, 'Towards a Demand Driven Deep-Brain Stimulator for the Treatment of Movement Disorders', MASP2005, 3rd IEE International Seminar on Medical Applications of Signal Processing, London, UK, pp.83-86, Nov. 2005.</p>
<p>[5] S.S. Haykin, <hi rend="italic">'Neural networks: a comprehensive foundation</hi>', Upper</p>
<p> Saddle River, N.J.: Prentice Hall, 1998. </p>
<p>[6] K. Papik, B. Molnar, R. Schaefer, Z. Dombovari, Z. Tulassay and J. Feher, 'Application of neural networks in medicine-a review', <hi rend="italic">Med. Sci. Monit.</hi>, 4(3), pp. 538-546, 1998. </p>
<p>[7] R. Nayak, L.C. Jain and B.K.H. Ting, 'Artificial neural networks in biomedical engineering: A review', In Proc. Asia-Pacific Conference on Advance Computation, 2001.</p>
<p>[8] G. Zhang, B.H. Patuwo and M.Y. Hu, 'Forecasting with artificial neural networks: The state of the art', <hi rend="italic">International Journal of Forecasting</hi>, 14, pp. 35-62, 1998.</p>
<p>[9] L. Tarassenko, '<hi rend="italic">A guide to neural computing applications',</hi> London: Arnold, 1998</p>
<p>[10] P. Picton, '<hi rend="italic">Neural networks</hi>', Basingstoke: Palgrave, 2000.</p>
<p>[11] M.T. Hagan, H.B. Demuth and M.H. Beale, '<hi rend="italic">Neural network design',</hi> Boston: PWS Publishing, 1996.</p>
<p>[12] M.F. Moller, 'A scaled conjugate gradient algorithm for fast supervised learning', <hi rend="italic">Neural networks,</hi> vol. 6, pp. 525-523, 1993.</p>
<p>[13] H. Demuth and M. Beale, '<hi rend="italic">Neural network toolbox: for use with MATLAB'</hi>, Natick, Mass.: The MathWorks, Inc., 2000.</p>
<p>[14] Z. Michalewicz, '<hi rend="italic">Genetic algorithms + data structures = evolution programs</hi>', Berlin: Springer, 1996.</p>
<p>[15] Z. Michalewicz and D.B. Fogel, '<hi rend="italic">How to solve it : modern heuristics</hi>', Berlin: Springer, 2004.</p>
<p>[16] B. Hassibi, D.G. Stock and G.J. Wolff, 'Optimal brain surgeon and general network pruning', <hi rend="italic">IEEE International Conference on Neural Networks</hi>, San Francisco, 1992, vol. 1, pp. 293-299.</p>
</div1>
</back>
</text>
</TEI.2>
