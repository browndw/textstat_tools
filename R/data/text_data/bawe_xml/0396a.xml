<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE TEI.2 SYSTEM "tei_bawe.dtd">
<TEI.2 id="_0396a" n="version 1.0">
<teiHeader>
<fileDesc>
<titleStmt>
<title>ST323 Multivariate Statistics Assignment One</title>
</titleStmt>
<extent/>
<publicationStmt>
<distributor>British Academic Written English (BAWE) corpus</distributor>
<availability>
<p>The British Academic Written English (BAWE) corpus was developed at the Universities of Warwick, Reading and Oxford Brookes, under the directorship of Hilary Nesi and Sheena Gardner (formerly of the Centre for Applied Linguistics [previously called CELTE], Warwick), Paul Thompson (Department of Applied Linguistics, Reading) and Paul Wickens (Westminster Institute of Education, Oxford Brookes), with funding from the ESRC. Subject to the rights of the these institutions in the BAWE corpus, and pursuant to the ESRC agreement, the BAWE corpus is available to researchers for research purposes PROVIDED THAT the following conditions are met:</p>
<p>1. The corpus files are not distributed in either their original form or in modified form.</p>
<p>2. The texts are used for research purposes only; they should not be reproduced in teaching materials.</p>
<p>3. The texts are not reproduced in full for a wider audience/readership, although researchers are free to quote short passages of text (up to 200 running words from any given text).</p>
<p>4. The BAWE corpus developers (contact: BAWE@warwick.ac.uk) are informed of all projects, dissertations, theses, presentations or publications arising from analysis of the corpus.</p>
<p>5. Researchers acknowledge their use of the corpus using the following form of words: "The data in this study come from the British Academic Written English (BAWE) corpus, which was developed at the Universities of Warwick, Reading and Oxford Brookes under the directorship of Hilary Nesi and Sheena Gardner (formerly of the Centre for Applied Linguistics [previously called CELTE], Warwick), Paul Thompson (Department of Applied Linguistics, Reading) and Paul Wickens (Westminster Institute of Education, Oxford Brookes), with funding from the ESRC (RES-000-23-0800)."</p>
</availability>
</publicationStmt>
<notesStmt>
<note resp="British Academic Written English (BAWE) corpus project">Language used in quote: <foreign id="English">English</foreign>
</note>
</notesStmt>
<sourceDesc>
<p n="level">3</p>
<p n="date">2007-01</p>
<p n="module title">Multivariate Statistics</p>
<p n="module code">ST323</p>
<p n="genre family">Exercise</p>
<p n="discipline">Mathematics</p>
<p n="disciplinary group">PS</p>
<p n="grade">D</p>
<p n="number of authors">1</p>
<p n="number of words">1479</p>
<p n="number of s-units">47</p>
<p n="number of p">22</p>
<p n="number of tables">6</p>
<p n="number of figures">9</p>
<p n="number of block quotes">10</p>
<p n="number of formulae">0</p>
<p n="number of lists">1</p>
<p n="number of paragraphs formatted like lists">3</p>
<p n="abstract present">no abstract</p>
<p n="average words per s-unit">31.5</p>
<p n="average s-units per p">2.1</p>
<p n="macrotype of assignment">simple assignment</p>
</sourceDesc>
</fileDesc>
<encodingDesc>
<p>TEI P4 (documented in: BAWE.documentation.pdf)</p>
</encodingDesc>
<profileDesc>
<particDesc>
<person>
<p n="gender">m</p>
<p n="year of birth">1986</p>
<p n="first language">Chinese unspecified</p>
<p n="education">UK2</p>
<p n="course">MORSE</p>
<p n="student ID">0396</p>
</person>
</particDesc>
</profileDesc>
</teiHeader>
<text>
<front>
<titlePage>
<docTitle>
<titlePart rend="bold">ST323 Multivariate Statistics Assignment One</titlePart>
</docTitle>
<titlePart rend="italic">By <name type="student name"/> (MORSE) <name type="student ID"/>
</titlePart>
</titlePage>
</front>
<body>
<div1 type="section">
<head rend="bold">Part 1: Principal Component Analysis</head>
<table id="BAWE_0396a-tab.001">
<row>
<cell/>
</row>
</table>
<p n="p1.22">
<s n="s1.2;p1.22">Note in red that the variance of V3 (liver weight) seems to be significantly larger, as well as its covariance with other organ weights. </s>
<s n="s2.2;p1.22">This is evidence that the scale of variation in the data might be different, largely explained by the age factor of hamsters. </s>
</p>
<table id="BAWE_0396a-tab.002">
<row>
<cell/>
</row>
</table>
<p n="p2.22">
<s n="s1.5;p2.22">Now both the sample variance and standard deviation are of a similar scale (&lt;1). </s>
<s n="s2.5;p2.22">The log-scale, particularly of base 2 in this case, helps a lot mainly because the growth of hamsters is achieved primarily by every cell dividing into two, i.e. an exponential growth of base 2. </s>
<s n="s3.5;p2.22">So the log-scale can reduce a great deal of variation due to the age factor and hopefully bring the data to a comparable scale. </s>
<s n="s4.5;p2.22">But there can still be other factors that are left unaccounted for. </s>
<s n="s5.5;p2.22">In addition, scaling can be quite subjective and had a different scaling been adopted, the result would have been different. </s>
</p>
<table id="BAWE_0396a-tab.003">
<row>
<cell/>
</row>
</table>
<p n="p3.22">
<s n="s1.1;p3.22">So the first two principal components explain about 84.9% of the total variation in the data. </s>
</p>
<table id="BAWE_0396a-tab.004">
<row>
<cell/>
</row>
</table>
<p n="p4.22">
<s n="s1.5;p4.22">The first component has all positive entries, so it can be regarded as a size vector. </s>
<s n="s2.5;p4.22">Large values along this direction in the graph would mean a general large size of all organs. </s>
<s n="s3.5;p4.22">The second component displays the contrast between V2 (heart) and V6 (testes). </s>
<s n="s4.5;p4.22">The actual direction of axes could be different from what SPLUS command princomp() produces. </s>
<s n="s5.5;p4.22">But in this case, a positive value along the direction of the second component means relatively heavy testes compared to heart and vice versa. </s>
</p>
<p n="p5.22">
<s n="s1.1;p5.22">It is interesting to observe the slight variation between what SPLUS princomp() produces and what the real eigenvalues and eigenvectors are. </s>
</p>
<table id="BAWE_0396a-tab.005">
<row>
<cell/>
</row>
</table>
<p n="p6.22">
<s n="s1.1;p6.22">Three things that seem to be observed about the SPLUS command princomp() are: </s>
</p>
<p rend="ordered" n="p7.22">
<s n="s1.1;p7.22">It centres the data automatically </s>
</p>
<p rend="ordered" n="p8.22">
<s n="s1.1;p8.22">The directions of the new axes are automatically decided although they could be set in other directions. </s>
</p>
<p rend="ordered" n="p9.22">
<s n="s1.1;p9.22">Loadings are selected to reflect their relative importance, i.e. some small and non- influential values are set as zero. </s>
</p>
<quote lang="English">> plot(Y$scores[, 1], Y$scores[, 2])</quote>
<p n="p10.22">
<s n="s1.1;p10.22">The plot is the 'best' 2-D summary of the data in the sense that the 2-D plane captures 84.9% of the variation in the data, i.e. it displays a 2-D slice of the 6-D data with the largest area covered. </s>
</p>
<figure id="BAWE_0396a-fig.001"/>
</div1>
<div1 type="section">
<head rend="bold">Part 2a: Data Recovery from dij - Euclidean Distances</head>
<quote lang="English">> HX &lt;- X > for(k in 1:6) HX[, k] &lt;- HX[, k] - mean(HX[, k]) > Q &lt;- HX %*% t(HX)</quote>
<p n="p11.22">
<s n="s1.2;p11.22">The Q matrix is just the sample variance matrix and hence symmetrical. </s>
<s n="s2.2;p11.22">So one way of recovering the centred data matrix HX is to use metric scaling - orientating the recovered HX in the same way as PCA does. </s>
</p>
<quote lang="English">P &lt;- eigen(Q)$vectors D.half &lt;- diag(sqrt(eigen(Q)$values)) HX1 &lt;- P %*% D.half</quote>
<p n="p12.22">
<s n="s1.3;p12.22">In theory, Q here should be positive semi-definite with all its eigenvalues non-negative. </s>
<s n="s2.3;p12.22">However in this case, some eigenvalues are computed by SPLUS to be negative but very close to 0. </s>
<s n="s3.3;p12.22">So this causes a little problem when producing HX1 and its covariance matrix - there are many NA values. </s>
</p>
<quote lang="English">> a &lt;- rep(0, 73) > for(i in 1:73) a[i] &lt;- var(HX1[, i], na.method = "include") > (a[1] + a[2])/sum(a, na.rm = T) [1] 0.8488463</quote>
<p n="p13.22">
<s n="s1.1;p13.22">By treating the NA values as just 0's, I can still find the proportion of total variation in the data explained by the first two columns of HX1 to be 84.9% - same as PCA result. </s>
</p>
<figure id="BAWE_0396a-fig.002">
<head>plot(HX1[,1], HX1[,2])</head>
</figure>
<p n="p14.22">
<s n="s1.3;p14.22">As we can see, the two graphs are identical, but with a 180-degree rotation. </s>
<s n="s2.3;p14.22">This is only because of a different definition for directions of the eigenvectors (axes). </s>
<s n="s3.3;p14.22">Again, it confirms the fact that PCA and metric scaling are related processes. </s>
</p>
</div1>
<div1 type="section">
<head rend="bold">Part 2b: Data Recovery from dij - another proximity matrix</head>
<quote lang="English">> d1 &lt;- matrix(0, 73, 73) > for(i in 1:73) for(j in 1:73) for(k in 1:6) d1[i, j] &lt;- d1[i, j] + abs(HX[i, k] - HX[j, k])/(abs(HX[i, k]) + abs(HX[j, k])) > d1.sq &lt;- d1^2 > m &lt;- mean(d1.sq) > Q1 &lt;- matrix(0, 73, 73) > for(i in 1:73) for(j in 1:73) Q1[i, j] &lt;- -0.5 * (d1.sq[i, j] - mean(d1.sq[, j]) - mean(d1.sq[i, ]) + m)</quote>
<table id="BAWE_0396a-tab.006">
<row>
<cell/>
</row>
</table>
<p n="p15.22">
<s n="s1.1;p15.22">Clearly there are some (far-from-zero) negative eigenvalues which prohibit Q1 from being positive semi-definite by definition. </s>
</p>
</div1>
<div1 type="section">
<head rend="bold">Part 2c: Data Recovery from dij - a better proximity matrix</head>
<quote lang="English">> d2.sq &lt;- matrix(0, 73, 73) > for(i in 1:73) for(j in 1:73) for(k in 1:6) d2.sq[i,j] &lt;- d2.sq[i,j] + (abs(HX[i,k] - HX[j,k])/(abs(HX[i,k]) + abs(HX[j,k])))^2 > m &lt;- mean(d2.sq) > Q2 &lt;- matrix(0, 73, 73) > for(i in 1:73) for(j in 1:73) Q2[i, j] &lt;- -0.5 * (d2.sq[i, j] - mean(d2.sq[, j]) - mean(d2.sq[i, ]) + m)</quote>
<p n="p16.22">
<s n="s1.2;p16.22">Now the matrix Q2 is positive semi-definite by checking that all its eigenvalues are non- negative. </s>
<s n="s2.2;p16.22">So we can apply metric scaling to obtain a newly recovered HX2. </s>
</p>
<quote lang="English">> P &lt;- eigen(Q2)$vectors > D.half &lt;- diag(sqrt(eigen(Q2)$values)) > HX2 &lt;- P %*% D.half</quote>
<figure id="BAWE_0396a-fig.003">
<head>> plot(HX2[, 1], HX2[, 2])</head>
</figure>
<figure id="BAWE_0396a-fig.004">
<head>> plot(density(HX2[, 1])$x, density(HX2[, 1])$y, type = "l")</head>
</figure>
<figure id="BAWE_0396a-fig.005">
<head>> plot(density(HX2[, 2])$x, density(HX2[, 2])$y, type = "l")</head>
</figure>
<p n="p17.22">
<s n="s1.3;p17.22">The new 2-D summary of the data quite clearly shows four clusters of points. </s>
<s n="s2.3;p17.22">And the density plots along both axes just double confirms this special pattern. </s>
<s n="s3.3;p17.22">Clearly some hidden features in the Euclidean distance have been picked up by the new proximity matrix. </s>
</p>
</div1>
<div1 type="section">
<head rend="bold">Part 2d: Data Recovery from dij - Gaussian sample</head>
<quote lang="English">> for(k in 1:6) HX[, k] &lt;- rnorm(73) * sqrt(k) > for(k in 1:6) HX[, k] &lt;- HX[, k] - mean(HX[, k]) > Q &lt;- HX %*% t(HX) > P &lt;- eigen(Q)$vectors > D.half &lt;- diag(sqrt(eigen(Q)$values)) > HX1 &lt;- P %*% D.half</quote>
<quote lang="English">> a &lt;- rep(0, 73) > for(i in 1:73) a[i] &lt;- var(HX1[, i], na.method = "include") > (a[1] + a[2])/sum(a, na.rm = T) [1] 0.5917205</quote>
<p n="p18.22">
<s n="s1.1;p18.22">This time the first two columns of HX1 only explain 59.2% of variation in the new data sampled from a normal distribution. </s>
</p>
<figure id="BAWE_0396a-fig.006">
<head>> plot(HX1[, 1], HX1[, 2])</head>
</figure>
<p n="p19.22">
<s n="s1.3;p19.22">This plot seems to be more evenly spread out than the one obtained from the original data matrix, also with a larger scale on both axes (this time the matrix has not been log-scaled before centering). </s>
<s n="s2.3;p19.22">But the fact that it explains much less variation in the data implies that the original data matrix might be quite far from a truly normal sample. </s>
<s n="s3.3;p19.22">This means that apart from the age factor that has been reduced by the log-scaling, there might still be some linear age factor or other different factors left such that the scaled data is still not quite normally distributed. </s>
</p>
<quote lang="English">> d2.sq &lt;- matrix(0, 73, 73) > for(i in 1:73) for(j in 1:73) for(k in 1:6) d2.sq[i,j] &lt;- d2.sq[i,j] + (abs(HX[i,k] - HX[j,k])/(abs(HX[i,k]) + abs(HX[j,k])))^2 > m &lt;- mean(d2.sq) > Q2 &lt;- matrix(0, 73, 73) > for(i in 1:73) for(j in 1:73) Q2[i, j] &lt;- -0.5 * (d2.sq[i, j] - mean(d2.sq[, j]) - mean(d2.sq[i, ]) + m) > P &lt;- eigen(Q2)$vectors > D.half &lt;- diag(sqrt(eigen(Q2)$values)) > HX2 &lt;- P %*% D.half</quote>
<figure id="BAWE_0396a-fig.007">
<head>> plot(HX2[, 1], HX2[, 2])</head>
</figure>
<figure id="BAWE_0396a-fig.008">
<head>> plot(density(HX2[, 1])$x, density(HX2[, 1])$y, type = "l")</head>
</figure>
<figure id="BAWE_0396a-fig.009">
<head>> plot(density(HX2[, 2])$x, density(HX2[, 2])$y, type = "l")</head>
</figure>
<p n="p20.22">
<s n="s1.4;p20.22">Compared with the previous results, there is much less clustering in the 2-D summary with a normal sample. </s>
<s n="s2.4;p20.22">The two peaks displayed in the density plot of HX2[, 2] (above right) is only a special case. </s>
<s n="s3.4;p20.22">Once the random sampling is repeated, I found that it does not normally show this kind of peaks. </s>
<s n="s4.4;p20.22">Overall, the plots are suggesting that a truly normally sampled data matrix shouldn't display unusual clustering that our data shows. </s>
</p>
<p n="p21.22">
<s n="s1.3;p21.22">It is interesting to see that using the Euclidean distances doesn't extract this kind of pattern as clearly as the 'better-defined' proximity matrix does. </s>
<s n="s2.3;p21.22">So the metric scaling with respect to (*) in the question sheet somehow enlarges the hidden factors that are left out after log-scaling and are causing the sample to be non-normal. </s>
<s n="s3.3;p21.22">My understanding is that d <hi rend="sub">ij</hi> is: </s>
</p>
<list type="ordered">
<item>very close to zero if hamster i and j are both grown-up hamsters or both baby hamsters</item>
<item>very close to 1 if i is a grown-up and j is a baby hamster or the other way round.</item>
</list>
<p n="p22.22">
<s n="s1.2;p22.22">This essentially causes the grouping / clustering of grown-up and baby hamsters in the 2-D summary plot. </s>
<s n="s2.2;p22.22">And therefore this 'better' proximity matrix is able to pick up the hidden age factor that is still left in the data after log-scaling but is not detectable using the normal Euclidean metric scaling. </s>
</p>
</div1>
</body>
<back/>
</text>
</TEI.2>
