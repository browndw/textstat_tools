In 1986, Rumelhart and McClelland demonstrated that a computer could learn regular and irregular English past tenses, apparently extracting rules and regularities from the input to make predictions and guesses about unfamiliar verbs that appeared to mimic closely the way that children learn and produce English past tenses. It threw into question the very basis of the psycholinguistic orthodoxy that had prevailed for two decades: that language is derived from internally represented rules. Chomsky built his Universal Grammar theory as the faute de mieux explanation of how children, from limited and imperfect input learn, not mimicry, but the abstract rules of their native tongue. Rumelhart and McClelland 's model opened up the possibility that psychologists and linguists had grossly underestimated the mind 's ability to learn grammaticality, and overestimated the complexity of the mechanism required to display apparently complex behaviour: input to an associationist system could be all that is required to explain language acquisition.
Since 1986, connectionist theories have been extended and refined to model a wide range of language phenomena, they have increasingly withstood criticism and found a place in or alongside other theories. In this assignment I will assess the contribution made by connectionism to our understanding of language acquisition, and I will consider how this new understanding could affect approaches to language teaching.
The principle behind the connectionist theories of psychology holds that the mind consists of units or nodes of information which exist in a network of multiple simultaneous interconnections. Input can activate a node or set of nodes which then trigger the activation of neighbouring associated nodes. The connections between the nodes can be stronger or weaker, the strength of connection being determined by the degree of stimulation of the connection. If asked to produce the past tense of bake, we momentarily hesitate before saying baked, due to the strong activation of the - ake irregularity in make and take, whereas we can more easily produce loved from love, because there are fewer competing connections (Seidenberg and Bruck, 199;, cited in N. Ellis, 2002). Knowledge is seen as the sum of the various strengths of connections, and learning "consists of discovering the right connection strengths from the input" (R. Ellis 1994: 405). Importantly for language acquisition, and in contrast to behaviourist accounts, generalisations can be generated, so that previous learning can inform new experience, and thus rule-like behaviours emerge.
The first connectionist computer simulations were in the area of visual perception before Rumelhart and McClelland (1986; cited in Redington and Chater, 1998) applied a connectionist model to language acquisition and 'taught' a computer to learn English past tenses. It succeeded with surprisingly (although not entirely) human-like accuracy, replicating the U-shaped curve of learning irregular past forms. Generalisation from input without the pre-requirement for grammatical architecture or a symbolic system, for so long discredited as behaviourist, was now a tenable proposition.
MacWhinney, Leinbach, Taraban, and McDonald (1989; cited in Plunkett, 1998) modelled the assignment of gender to German nouns, which the model succeeded in doing using hitherto unknown phonological information in the stem, showing how subtle cues in the input could be put to use in acquisition.
Second language learning was modelled by Blackwell and Broeder (1992, cited in Cook, 2001) who compared pronoun acquisition order in a computer simulation, first learning Arabic from input based on naturally occurring frequencies, then learning Turkish as a  'second language', and vice versa with Arabic as the  'second language'. The acquisition order in the second language models correlated with those found in human learners.
Connectionist models have been employed to reflect spelling to sound mapping in English children learning to read (Zorzi, Houghton and Butterworth, 1998), including the handling of irregular spellings and novel words, an example of how connectionism can approximate real-life learner output.
Other studies have suggested how children segment sound using prosodic cues, stress patterns and phoneme distribution probabilities (Redington and Chater, 1998). Finch and Chater (1992) modelled how children can develop knowledge of word classes from pattern distribution.
Ellis and Schmidt (1998) studied the acquisition of regular and irregular morphology (in this case plurals) using an artificial language to compare human and artificially modelled acquisition from a 'zero' state. The machine results largely corresponded with the human ones, and was able to generalise to a default regular for new items, without recourse to an affixation rule.
It has been argued (Prasada and Pinker, 1993, cited in Redington and Chater, 1998) that connectionist models are able to map the past tense morphology of English because of the peculiarities of distribution in English: - ed is both the default and the most frequent inflection. A connectionist model would surely not learn infrequent default values, such as those found in the German plural - s and the Arabic plural system. Forrester and Plunkett (1994, cited in Redington and Chater, 1998) demonstrated that a minority regular default could emerge in a system (such as Arabic) where the irregular items were grouped around subregularities. Further to this, Nakisa and Hahn (1996, cited in Redington and Chater, 1998) modelled a high accuracy rate in novel noun prediction of both regular and irregular plurals in German, demonstrating again how connectionist modelling can make use of patterns and associations in input in ways not immediately apparent in simple surface frequencies. Nakisa and Hahn compared this with a dual-system model whereby a default rule would add - s when the associative memory failed. The default-rule model performed worse than the purely connectionist model: a challenge to the memory-plus-rules position of many psycholinguists such as Pinker (1999).
The argument that connectionism could not account for grammaticality (Chomsky 's colourless green ideas) was tested by Allen and Seidenberg (1999) whose connectionist network was trained on (what could be described as the impoverished quantity of 100) correct sentences, each word endowed with semantic information (e.g. HOUSE lodging structure construction artefact object physical-object entity-house). Based on statistical regularities of both lexical and semantic features, the network was able to judge the grammaticality of novel sentences to a high degree of accuracy, even in nonsensical, but grammatically correct, sentences. Interestingly, the sentences which the model performed worst on were of a type that aphasics have most difficulty judging.
Impressive and surprising as some of the computer modelling experiments have been, it is important to approach with caution and ask the question: what has really been demonstrated? Firstly, computers are very good at succeeding in a particular, narrowly defined task, such as computing past tenses, but do not necessarily demonstrate an insight into human learning as a whole. The success of the output depends on the operational principles that it has been programmed with, and the nature of the input. Connectionism is not one unified model of network connections: there are numerous architectures which differ widely in terms of layers of nodes, the relationships between them and the direction of stimulation. Ellis and Schmidt (1998) used four different network architectures to achieve their optimal results. Rumelhart and McClelland (1986) had to modify their network several times before their published model. When the experimenter is at liberty to change the experiment, he can get the results he wants. There is the danger of an argument that is circular and unfalsifiable: the settings merely need to be tweaked until the 'correct' output is obtained. The counter-argument is that the principal purpose of the network experiments was to demonstrate that a grand symbolic generative grammar model is not an essential requirement of a theory of language and to generate some solid data to counter the a priori arguments of generative cognitivism, and to show a way in which mechanisms, previously judged to be impossible, could be possible.
Even if one accepts the validity of computer modelling, there are clear limitations to the correspondences one can infer with human learning. Carroll (1995) reminds us that computer 'learners' have the benefit of 100% feedback on their efforts, something that the average human never gets. Christiansen and Chater (2001) urge caution in assuming that the success of modelling of small-scale localised linguistic features implies similar success in large (i.e. human) scale language systems: can connectionism cope with the complexities of a large interacting system? Where do well-attested characteristics of the learning process fit in, such as working memory, explicit and implicit learning, and consciousness?.
Another question arises when we consider situations where the human input and output do not match. A learner is exposed to a model of correctness which he will eventually equal (in the case of L1) or approach (in the case of L2). How does this explain pragmatic learner language (e.g. "I bus home", "me good eat" or the habit of a Down  's syndrome friend of mine to use "me", almost like the French reflexive, in "I me not hungry") when structures not present in the input are incorporated into the output? And how does it explain languages like Kaluli (Papua New Guinea) where the female care givers frequently use an imperative form that is quickly taken up by girls, but not by boys ( Schieffelin, 1990; cited in Mitchell and Myles, 2004)? A credible theory of connectionism will need to incorporate pragmatic language and sociolinguistic variation alongside statistical computation of frequencies drawn from the input. The increasing attention given to semantic/pragmatic-driven syntax (Bod, 1998; Allen & Seidenberg, 1999) and the possibilities of sociolinguistic information being incorporated into a network (N. Ellis, 2002) may point the way.
One criticism of connectionist theory put forward as crucial evidence of a rule-influenced system (Gregg, 2001; Pinker 1999) is that it does not explain negative knowledge. Supporters of connectionism (such as Ellis, 2002; Redington & Chater, 1998) play down the significance of this objection, pointing out the fuzzy and variable nature of grammaticality, and the fact that much of our knowledge of the world also lacks such negative evidence.
This ability of computer models to display rule-like behaviour without the need for any in-built symbolism seriously undermines Chomsky 's a priori argument that language is based on deep, fixed, inherited, grammatical knowledge residing in a mysterious Language Acquisition Device. Chomsky seems to have pushed the complexity problem of language acquisition away from the individual child 's immediate environment and tucked it safely away in the realms of evolutionary psychology. The burden of explanation remains - no longer in the stimulus, but in the inheritance. Cognitive linguistics, then, was born, relieved of (at least some of) its duty to model language acquisition as a process played out between mind and environment.
Computer models have successfully demonstrated that the input could actually be enough to account for the acquisition of complex language features in a general neural architecture: the complexity of language structure can be product of the complexity of the environment, without the need to invoke a putative Language Acquisition Device.
Some recent developments in linguistics lie comfortably with a connectionist framework. Another product of the computer age, corpus studies, has moved the study of language away from introspection to produce objective analyses of real language. These studies, such as the Cobuild project (Sinclair, 1991), have challenged the view of a core grammar acting on a separate lexis in favour of memorised collocations, chunks and idioms: each word and phrase has a range of preferred neighbours and interdependencies (even extending beyond clause boundaries) in a way that blurs or dissolves the distinction between words and rules. Good grammar does not equate with good language : how late is it ? is not an acceptable alternative to what time is it? (Bod, 1998).
R. Ellis (1999; cited in N. Ellis 2002) points out that what we consider to be grammar is highly variable, even within an individual speaker, while Barsalou (1998; ibid.) and Luka (1999; ibid.) demonstrated how grammaticality judgements can be primed by recent exposure to grammatically 'borderline' structures, suggesting that grammar is less a fixed absolute, more a collection of memories and probability judgments.
The Competition Model (MacWhinney, 2001), meanwhile, models the relationship between form and meaning as multiple cues in the input competing to determine comprehension, and characterises learning as the modification of the weighting between these cues in the light of experience, and as such provides a complementary framework of the functional application of connectionist theories.
To sum up, the advantages of a connectionist model of language are that it suggests how neuroscience can relate to linguistics; it describes language representation, processing, variability and learning in one system; it is tangible and testable, unlike many other theories, generating solid data and predictions that can be compared to human data; computer models have produced some surprising evidence that have challenged linguists' assumptions about the emergence of linguistic competence; finally, it concords with the instincts of many teachers, learners and the linguistically na  ve : "outside linguistics it is widely held that cognitive representations are highly affected by experience" (Bybee and Hopper, 2001: 1).
As a very young theory, it is not clear whether connectionism will replace, absorb or merely accompany other theories. Strong judgements must be withheld, and in many aspects of language acquisition it still has a lot to prove, but there is clearly considerable potential for fruitful future exploration. These reasons, combined with the fact that it is a radical new theory with relatively unexplored pedagogical implications that may confirm or challenge the classroom practice informed by earlier theories, form the rationale of this assignment.
As explained above, connectionism represents a considerable departure from previously held theories of second language acquisition. How, then, can a teacher apply these principles to the classroom? I will focus on what I consider the three most important aspects of teaching that connectionism draws attention to.
Firstly, connectionism puts input frequency along with practice firmly and unambiguously at the heart of any learning process: "It is the piecemeal learning of many thousands of constructions and the frequency-biased abstraction of regularities within them" (N. Ellis, 2002: 143). This may come as no surprise to most teachers and students, but after behaviourism and the audiolingual method were sidelined, many cognitive theories of learning held frequency and practice as having only indirect influence upon the acquisition of language. In the case of strong Universal Grammar theory, for example, input only serves as a trigger to set the parameters of innate language knowledge. Connectionist learning, on the other hand, relies on large quantities of natural input combined with practice.
Secondly, the associative, rather than rule-governed, mechanisms of connectionism, evidenced in the corpus analysis of natural language, suggest a move away from a grammar-focussed syllabus towards a greater emphasis on the wider patterns of language: chunking, collocation and formulae.
Thirdly, the view of connectionism that learning consists of adjusting the weights between nodes and, expressed in the Competition Model, meaning is understood and expressed through the interplay of competing cue weight, leads us to consider how the teacher can best facilitate form to meaning mapping.
Although the pedagogical issues explored here are general and applicable to most second language situations, the teaching situation which I have most experience of, and which I have held in mind during this assignment is as follows: adult learners of English as a foreign or second language, studying and usually working in the UK, in multilingual classes of up to 18 students. The typical learner has come to the UK to study full-time (15 contact hours per week) for two to three years, and generally comes from (mainly Eastern) Europe, Asia, South America, or possibly Africa. A minority of students have settled in the UK and may be described as second language learners.
The teaching approach can be said to be broadly communicative, reflecting the approach of the Trinity TEFL certificate course; teaching is based on course books, typically the Cutting Edge or English File, or a Cambridge exam-orientated course book.
The connectionist view of practice is that the more often an item is encountered in the input, the stronger the connections between the nodes, and learning is the gathering of the statistical information contained in the input. However, it would be misleading to characterise frequency as the only factor; people can learn from a single instance, but fail to learn from repeated instances, as can be seen in naturalistic learners of English who rarely master auxiliary verbs and the third person - s. Learning cannot be reduced, therefore, to the simple tallying of instances: token frequency.
Bybee and Hopper (2001) claim that type frequency is important for the emergence of phonological, morphological and syntactic patterns; for example, every verb you hear with an - ed ending strengthens all the other regular verbs, so you do n't need to hear every - ed ending to be able to extrapolate the likely rule. Conversely, an item encountered frequently only as a token will be learnt as a token and not as a representative of a wider pattern, as is the case with irregular forms and idioms. In a teaching situation, this is an important reminder that gereralisable patterns must not become learnt as fixed lexical items. In order for patterns to be acquired, examples need to be given with a wide, but representative, variety of lexical items, in differering contexts, so that a general category is formed.
Repetition is central to a connectionist view of learning. Not the mindless, de-contextualised drilling of audiolinguism, but the reformulation of the same items in different, stimulating contexts and media, grounded in real-life communication.
Revision and recycling are usually underplayed in most textbooks, and even then the focus is generally on the 'core' grammar rather than other aspects of language. Hulstijn (2001: 280) summarises research into vocabulary practice frequency as follows:
Computer technology can have a part to play here in supplementing the classroom by providing the necessary stimulating recycling that would be impractical in the classroom as well as providing corrective feedback that is instant and non-threatening.
Another distributional phenomenon associated with connectionism is the consistency effect, whereby activation of an item will call up other items with associated features. In speech recognition, low-frequency words take longer to recognise if there are high frequency words which share the same beginning (Lively, Pisoni & Goldinger, 1994; cited in N. Ellis, 2002). In reading aloud, consistent words - regular words (e.g. bed) with 'neighbours' that share the same phonological and orthographic features - are learnt quicker than inconsistent, regular words (e.g. mint) with deviant 'neighbours' (e.g. pint) (Liveley, Pisoni & Goldinger, 1994; ibid). This suggests that teachers should focus attention not only to irregular forms, but also to forms that share features with irregular forms, which may cause interference in the development of schema-formation.
The role of consciousness and implicit versus explicit learning is difficult to define and demonstrate, but it seems that, at least for certain low-saliency features (e.g. prepositional idioms), noticing (Schmidt, 1990) or consciousness-raising is necessary to initially register the qualities or importance of a feature, and then, proposes Ellis (2002: 175), subsequent fine-tuning of the input "tally" can be implicit. Connectionism does not necessarily imply a process of implicit learning by statistical frequency that does not have an important place for consciousness and meta-linguistic processes. Attention can be another factor in building and prioritising positive associative strength. Untutored learners generally acquire lexical communication over grammatical accuracy, indicating that, unlike first language learners, second language learners cannot rely purely on natural input to reset their weightings: formal instruction can speed the acquisition of accurate language, and special emphasis needs to be given to non-salient features (e.g. third-person - s), marked features (e.g. phrasal verbs), or problems of transfer. Research, such as Robinson (1996; cited in DeKeyser, 2003) and Williams (1999; ibid.) suggest that explicit learning is beneficial for acquiring simple rules and form-function mappings.
The question of focus-on-form and focus-on-forms is also relevant here. Connectionism suggests that language is best learnt in a realistic exemplar-based context so that the distributional and functional networks can be accurately tuned. Excessive focus on forms, beyond that which is necessary for noticing and comprehension, risks creating discrete declarative networks: knowledge about the language can over-ride the implementation of the language.
The view that language involves slotting lexical items into a grammatical framework has the advantage of considerable teachability. The contrasting perspective implied by connectionism - that fluent language consists of collocational streams - presents some daunting challenges to the teacher. How do you design a structured, coherent syllabus around thousands of scraps of language? One attempt at a course book modelled on a lexical syllabus is Innovations (Dellar and Hocking, 2000). I would argue that the more frequent and generalisable patterns in language (e.g MODAL + LEMMA) can usefully be learnt as rules. At lower levels in particular, learning rules (supported by examples) can quickly increase productivity, while at more advanced levels, rules tend to give way to idiom, collocation, and context-restricted words (e.g. staunch + supporter). What is important, at all levels, is not to teach words in isolation, but illustrate them in a variety of contexts that reflect the item 's semantic range and combinatorial potential. The resources of the teacher will be enriched by harnessing the knowledge gleaned from corpus studies - either by direct teacher access to corpora, or through corpus-inspired dictionaries and course materials. With the learning of formulations, however, the limit of the teacher 's role is firmly in sight, and the best course is, again, to provide opportunities for processing large amounts of natural language.
In a connectionist model meaning is deduced and constructed from all the possible cues in a language, whether phonological, lexical, morphological, syntactic, pragmatic or social:
The competent language user can weigh the competing cues to form the appropriate understanding, or, working in the opposite direction, an appropriate utterance. For example, in The car kissed the King, despite the strong cue of animacy (kings are better kissers than cars), the strength of word order is such a strong predictor of the agent in English that most people would interpret this as an absurd or surreal phrase, rather than nominate the King as the agent.
The second language learner has the additional task of readjusting these mappings away from the L1:
The focus on meaning is a central part of a language classroom, and teachers will do well to guide the learner through the many cues that lead to meaning (e.g. in speech the lengthening of the a in ca n't is a much stronger cue to negation than the barely audible t). Focus on contrastive forms (the students who passed the exam... versus the students, who passed the exam,... ) and explicit 'weighing the evidence' in the interpretation of comprehension tasks can be valuable activities.
A syllabus with the emphasis on meaning and naturalistic contexts, incorporating excursions into focus-on-forms, provide direct form-meaning relationships. These can be strengthened with the use of drama, games, video, music and out-of-class lessons, which bring students into direct and vivid contact with a wider range of situations, experiences, emotions and social settings than are usually available in the cosy self-contained conventions of a language class.
Connectionist theories offer some exciting fresh perspectives on the nature of human learning which clarify and strengthen some of the core aspects of second language teaching, notably with regard to input, practice and meaning. The prospect, however, of a comprehensive and durable theory of learning and classroom practice is still remote. In the meantime a pluralistic approach, informed by careful classroom research and moderated by a large dose of humility seems to me to offer the best chances of a successful learning outcome.
