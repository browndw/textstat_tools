In order to understand and closely predict abnormal spontaneous electrical activity in brains of patients suffering from movement disorders (PD in our case) this report shows how Artificial Neural Networks are employed to achieve this goal. In this assignment I have used MatLab 's Neural Network Toolbox to extract relevant portions of data provided to us and work upon it by testing it on two different Artificial Neural Networks as required.
PD is a degenerative disorder of the central nervous system that often impairs the sufferer 's motor skills and speech. This takes place as a result of degeneration of neurons in a region of the brain that controls movement, causing shortage of brain signaling chemical dopamine . Although formally recognized and documented in the early nineteenth century by British Physician James Parkinson, The oldest description of it is found in the ancient Indian medical system of Ayurveda under the name Kampavata around 2500 BC. The first and most common symptom of PD is tremor (trembling or shaking) of a limb, especially when the body is at rest. Tremor often begins on one side of the body, frequently in one hand. Other symptoms include slow movement (Bradykinesia), an inability to move (Akinesia), rigid limbs, a shuffling gait and a stooped posture. People with PD often show reduced facial expressions and speak in a soft voice. The disease also causes depression, personality changes, dementia, sleep disturbances, speech impairments and sexual difficulties. Severity of PD related symptoms worsen over time.
Dopamine is a Naturally produced Chemical in the body and functions as a neurotransmitter activating receptors helping us "command" our body.
An Essay on the Shaking Palsy - JP, Royal College of Surgeons 1817.
There are many theories about the causes of PD ranging from Environmental factors to Inherited susceptibility. There is no test that can clearly identify the disease Although Tests such as brain scans, can help doctors decide if a patient has true Parkinson 's Disease or some other disorder that resembles it .
Some experts also believe that PD is something of an "iceberg; phenomenon" lurking in as many as 20 people for each known to have PD. There is no cure for PD and hence can only be controlled (partially).
In this assignment we have been provided with a data file (from hereon referred to as dat) containing actual statistics (split into two columns for convenience)from a patient with the dat is recorded over 100 seconds (as it has a frequency of 800 Hz and has a total of 80000 observations) with both variable having a diverse range of amplitudes . In other words the way we load any data into a Neural Network would be dependent on how we would want to Interpret it as . This will not only help us in getting a good response but will also aid our understanding of the whole procedure .
Our Purpose here is to devise a neural network such that when trained with a certain occurrence(s) of input LFP with target EMG ,it is able to correctly predict EMG if provided with just the LFP when running. In other words we need to model an autonomous system in which only the input LFP is enough to predict when an EMG change will occur so that a countering action is generated to reduce the amplitude of EMG so that a large tremor does n't take place.
There are several ways by which we can bring the participating data up-to a consistent standard that is easy to manipulate , recognizable and is comparatively smaller. The best way that would suit this purpose initially is to create a mean value for each participant for every certain number of occurrences. The main factors needs to be taken into consideration are.
That it acknowledges that LFP has both positive and negative value and hence there is a strong possibility that means at some interval would be 0 leading to inconsistencies . (That is a reason we would have to use a variance (  2) based approach for LFP . This ensures that there is a "visible accentuated difference" between the two participant .).
EMG (green line in Fig 2) is something which would suit us best if it is 'fuzzified' . In other words, rather that having an intensity based EMG model , It is far more simpler to have and implement it based on an 'on-off' approach.
There is a tiny 'blip' in our observation at the beginning where our EMG slightly increases at the 15 second mark (around the 13000th observation).This means that it would have to be appropriately thresholded .
We then plot the simplest way in which the data can be plotted with a total of 80,000/800 = 100 observations . Visually we are able to observe in this case that as LFP goes down EMG activates. Our main focus now is to have an appropriate approach to realize a solution .
Artificial Neural Networks can be summarized as connectionist paradigm based programming on interconnecting neurons (or information processing units) in order to arrive at an overall response . An ANN is usually designed to change its structure based on external or internal information flowing through it to get a 'desirable' output . Understanding ways in which ANNs work is beyond the scope of this assignment (as the report assumes reader to have prior knowledge) and the report focusses on how accurately is a tremor predicted . Normalised dat is split into a ratio of 1:1:1 (Fig 3)with each being for training , validation and testing . There are several ways in which the data can be split for this purpose but the reason I employed this approach was because the initial blip is capable of training the network well so that it is adjusted according to its error also the validation bit is used to measure network generalisation , and to halt training when generalisation stops improving . Testing has no effect on training and so provide an independent measure of network performance during and after training . The intention is to have final plots with the prediction for all the three portions (even if they have been trained with).
Back propagation is a somewhat ad hoc process. The initial step is to check that the output is correct. If it is not correct, an algorithm goes back and modifies the weights of the inputs to the output layer, such that a correct response would have been received. These inputs are, in turn, outputs of a middle layer, and the weighted inputs of the middle layer must now also change to reflect the new output. This process continues successively until input weights in each of the layers have changed. There are essentially two passes through a neural network. This process continues until the input weights no longer change. Back propagation is a common technique; however this is not a method that biological systems utilize . The rough figure shows a simple BPN Feed-forward (with one hidden layer trained with gradient descent Backpropagation) used for this purpose . Learning mode used was Gradient descent with momentum weight and bias learning function.
Fig 4 Shows that the Prediction(shown in blue) and the original value (red dash dot lines) are close . The arrows marked on the Fig show that : i) Although not visible (due to printing constraints) the Prediction is very close to the desired target value . ii) The twin peaks are a result of momentum and are a common problem when using simple BPNs . iii) Due to no limits being present the momentum causes the plot to go down into a negative area . iv) & v) are due to overfitting .
A Multilayer Perceptron can be considered as a advanced Feedforward Backpropagation model but this is where the similarity ends as a MLP has high capacity of input-output mapping . In addition to this MLPs can far outperform other neural networks (such as RBF) and also requires smaller number of parameters in case of nonlinear input-output mapping for the same degree of accuracy. As a consequence MLPs tend to be far more accurate than other complex ANNs (although there is a tradeoff as far as training time concerned as MLPs take the longest to train) . Architecture of MLPs are not completely constrained by the problem in focus.This is because the number of inputs nodes is constrained by the problem and the number of neurons in the output layer is constrained by the number of outputs which is required by the problem, whereas the number of hidden layers and the number of neurons in the hidden layers depend on designer . As we know that there will be one input node in the input layer one output neuron corresponding to a target vector . There is n't a set procedure to arrive at a suitable number of hidden layers but after experimenting this was set at two . The activation function selected for this purpose is a logistic activation function . A modified conjugate gradient algorithm based on the Levenberg-Marquart approach is utilized as the preferred learning algorithm . Fig 5 shows the result observed in this case . In this two interesting observations are made as (i) is very close to the predicted error but (ii) makes for an erratic viewing this is a trend we observed in the previous ANN. The peaks are generated due to overfitting .
When comparing the errors in the networks (Fig 6.) we get an expected result . An MLP implementation of the problem is roughly 22 times more accurate than a conventional BPN one .
The conclusion can be summarized in the following points :
A lot actually depends on the way we manipulate and work with the given data . This means what we decide to partition and where has a major impact on the overall decision (in this case at least).
The solution discussed in this report is highly context specific and cannot be generalized whatsoever unless we have a training data from either several patients or different plots from the same patient.
ANNs at this stage are not very reliable , In addition to that any response needs to be thresholded properly .
Although we got a MLP to work relatively well , Its use in a real situation depends upon several factors such as criticality of the system or whether an ANN is used as the sole method used for prediction or an aid .
There is a strong possibility that such a system can be put into use in the future as it is fairly reliable at the moment .
