The Izhikevich Neuron model is currently the forefront spiking neural net model. It can emulate all the key modes of firing, and does so with only 13 floating point operations per second of emulation time 1. The Hodgkin 's and Huxley model - the only other model capable of displaying so many firing modes - requires 1200. In an experiment done by Izhikevich, where he ran a network of these neurons with 10 10 synapses on a Beowulf supercomputer, it took 50 days to run what could be considered one second of brain activity. The integrate-and-fire model, the fastest spiking neuron model, which has drastically lower functionality, still requires 5 FLOPS and thus would have still taken around 20 days to run the simulation. This highlights a problem with such neurons - running them on conventional computers - even supercomputers, is not efficient. The only way to run them, at any kind of reasonable speed is on massively parallel hardware dedicated to running them. This report charts the development of a simulator for a theoretical board which could run Izhikevich type neurons, in a parallel fashion with easily expandable processing capabilities.
The task in its simplest terms is to develop a simulation of some type, or part, of a computer system. Having selected what we are going to simulate we are to examine other systems, and develop a design for our own version, with highlight on certain differences. For this particular project, 'computer system' will be taken to be a 'Spiking Neural Network Board' - as permitted by the project coordinator. Due to its relative complexity, it will be developed using an iterative methodology, initially only being a simulation in an abstract sense, which will then be refined to a greater depth.
The development of Neural Networks has gone through three main generations 2. The first was focused around the McCulloch-Pitts neuron. A conceptually simple model, consisting of neurons which multiplied their inputs by a weighting, summed them and fired a binary high signal if its input went above a set threshold. These neurons are quite powerful, have many uses, and do not use much computational power however they do have limitations, such as only being about to handle digital values. The second generation solved this problem, and begun using continuous activation functions and used rate coding (a higher rate of firing implies a higher output) to give their output. This generation was more biologically realistic and computationally powerful than the first - it did require more processing time though.
It was then discovered though that the cortex was able to perform at far greater speeds than possible with rate coding, when Thorpe et Al found that it only took 100ms for a human to analyse and clarify visual input (for instance, in facial recognition). The third generation thus had to find a new way to transmit information, and this was found in the form of pulse coding, with each spike being important on its own basis. This generation begun using features which take into account the exact time that a spike is triggered and arrives at the postsynaptic synapse such as axonal conductance delays and spike-timing dependant plasticity,. This was the birth of the Spiking Neuron.
The first neuronal model that could be considered a spiking neural network was the Hodgkin 's & Huxley model. It was created after several years of electrical analysis of squids neurons. It was a highly detailed model, correctly known as a bio--physiological model which took into account such things as the transfer of molecules across the cell membrane along the axon. It was able to demonstrate the vast array of different neuro-computational properties that are out there, some of which are shown in figure one. The majority of spiking neuron models have since sacrificed these though in the name of simplicity and computational efficiency. The integrate-and-fire model, probably the most popular - and most computationally simple, has only three different behaviours 1. The Izhikevich model on the other hand has the full range of neuro-computational properties, at the same time it remains quite simple, and computationally light - only requiring 13 floating point operations during each ms of simulating time.
The model was created using bifurcation methodologies, which allowed the Hodgkin 's & Huxley model to be reduced to, and represented as standard differential equations. These are 4:
Where is membrane potential and is membrane recovery.
Synaptic & injected currents are inserted via variable and are dimensionless parameters describes the time scale of the recovery of describes the sensitivity of to subtle changes in is the after spike reset value of affects the after spike recovery of represents where is the time.
These formulas create a spike, which affect the two key variables, as such:
By altering the values of a, b, c and d the different types of behaviour discussed above can be created. For instance, to create a Regular Spiking neuron you would use the values, a = 0.02, b = 0.2, c = -65 and d = 2. You could get the other types in accordance with these charts:
In its simplest terms the axonal conductance delay is the length of time required for a postsynaptic neuron to become aware that a spike occurred. In the neocortex it can vary from 0.1ms to 44ms, and can be precisely reproduced between two neurons. The majority of the computational neuroscience community has typically ignored its existence in the cortex, where it has typically been treated as a complicating nuisance. But in recent times the argument has been repeatedly driven in that the brain would not maintain different delays with such precision if spike timings were not important.
Izhikevich proved in 2005 5 that delays could lead to an unprecedented information capacity in neural networks and allows stable firing patterns that are not possible without the delay. Because of this, it is now increasingly considered an important feature of spiking neural networks, and will be included in this simulation. The idea can be illustrated simply as follows:
STDP, or as it 's also known, Hebbian Temporally Asymmetric Synaptic Plasticity, is a form of neural learning which has developed in response to the growth of Spiking Neural Networks. It is essentially half Hebbian learning and half Anti-Hebbian - Hebbian learning being the formally accepted idea that 'those who fire together, wire together'. It relies on that idea that synaptic plasticity (the strength - or weight - or a synaptic connection) depends on the relative timing of pre and post synaptic spikes.6 It was formalized into a mathematical system by Gal Chechik in 2003 and will be the learning system employed in this simulation.
It can be summarized simply as follows. If the presynaptic spike arrives at the postsynaptic neuron before the postsynaptic neuron fires - for example, it causes the firing - the synapse is potentiated. Its weight is increased according to the positive part of the STDP curve in Figure 6 but does not allow growth beyond a cut-off value, which is a parameter in the model. If the presynaptic spike arrives at the postsynaptic neuron after it fired, that is, it brings the news late, the synapse is depressed. Its weight is decreased according to the negative part of the STDP curve.
Spiking Neural Networks require much greater amounts of processing than their earlier cousins, such as the McCulloch-Pitts neuron. Because of this, when we create large networks of them, it becomes impossible to run them in real time on a serial processing computer. As many of the purposes we wish to assign neural networks to, rely on being able to process real time data - such as analyzing continuous visual and audio streams, which require networks of around the magnitude of 10 6. They become significantly less useful, even useless, when they are unable to do this. Some tasks may not require real time simulation, but some, such as whole brain simulations, certainly require speeds higher than those currently available.
Having realized this, there are a few options - to either make the serial computer more powerful, to develop a specialised serial computer built for the task, or, develop the network in parallel hardware. Due to the very nature of neural networks, this is the most efficient, and logical way to perform the task. There are other reasons beyond the ability to process real time as to why we want dedicated SNN hardware. If they are to be commercialized and integrated into appliances and working robots then it will be necessary to produce them en mass, in an easily adaptable form, small enough, quick enough and cheaply enough to justify using them. They will also have to drain limited amounts of power in the majority of circumstances - something certainly not done by a modern workstation.
Neural networks based on the first two generations never really begun to be implemented in hardware to any great extent - it was never necessary due to the limited amount of calculation required. However, with the move to spiking neural networks it has become more important, and thus more heavily researched. The majority of work done at converting the networks to hardware has been with FPGA 's (Field Programmable Gate Arrays) 7, although recently eyes have begun being cast towards the emerging technology of FPAA (Field Programmable Analogue Arrays). This has been due to the flexibility of implementing such a solution without commiting to a costly silicon ASIC fabrication. Using them it is effectively possible to simply develop the network as a program then directly translate it into silicon.
There have also been two key sides from which the problem has been approached, in whether it is implemented in an analogue form with continuous values in continuous time, or digital form with discrete values valid at specific instants. Analogue has drawn a lot of attention from some sides but there are many disadvantages. There are serious restrictions due to pin-count, limiting the number of neurons per chip, and consequently the size of the network. Values are prone to inaccuracy and signals are slow to change. Along with this, most VLSI design and fabrication systems are designed for digital and so the analogue designer is often not using the best systems.
Digital systems have the advantages of high speed adders, multipliers and functions, but are disadvantaged in that the same things are quite large, consume large amounts of power and on top of that - the input and output need to be digitized. Digital systems usually share certain components, and are thus limited in speed by the number of simulated neurons times the speed of the signal. Digital memory is also far more advanced than its analogue counterparts, which is important due to constant and alterable values of the system such as delay lengths and synaptic weights.
As this technology is on the whole new, and still well within the fields of research there are no existing standards as to how things are done, even once it has been decided whether a project will be done in analogue or digital form. There are however, some broad overall trends - such as there tends to be a controller, which routes signals, and a neuron chip, which performs essential calculations. Beyond this it is n't really possible to make broad sweeping statements, and thus I shall just discuss two existing systems about which information is available.
There are some techniques used broadly across development of hardware, for example, it was found by Roth et al in 1995, that there was no significant degradation of network performance - in terms of image processing - by limiting weights to eight bit and internal potentials to 16 bits - because of this, most projects used fast fixed point arithmetic instead of floating point arithmetic, which results in fairly significant speeds increases.
The first project we shall look at was developed at the California Institute of Technology, with the aim of using spiking neural nets for automotive applications, such as analysing data from numerous subsystems and determining when something is wrong 8. The existing pressure on car processing systems prompted efforts to development a VLSI design which would facilitate the implementation of SNN in high volume products. The design constraints for this project called for the development of an inexpensive, fully autonomous, and commercially viable electronic chip. This single chip implementation was required to be extremely compact in size and accurate. It was also desired that the chip should be useable for numerous applications beyond the current scope.
The design consists of a global controller, a pool of 16 neurons (three layers - 7 inputs, 1 output and 8 hidden), a ROM-based look-up table, neuron state registers and a synaptic weight ram. Inputs and outputs are stored in the state registers. When triggered by the controller, each of the neurons takes in their inputs and synaptic weights, and performs the necessary functions. As the system uses layers which calculate their data one level at a time, it is possible for neurons on different layers to share mathematical processors. It is setup such that the chip is able to perform fully parallel calculations, under the supervision of the global controller. The task of the controller is essentially to avoid memory access issues and orchestrate data movements on-chip and off chip.
The second project is known as MASPINN (Memory optimized Accelerator for Spiking Neural Networks)9. It is the latest of a series of network accelerators based on two previous configurations called SPIKE128k and NESPINN. SPIKE 128k was developed in the university of Padeborn. It is a FPGA based structure which leads to a limited speed. The next step is the NESPINN-system, which takes advantage of the VLSI-technology using Application Specific Integrated Circuits (ASICs). MASPINN has the purpose of allowing a sufficient resolution of processed images. In comparison to the NESPINN-architecture, the memory organization and dataflow of MASPINN is optimized in order to meet this challenge.
It is important to note the MASPINN is an accerator designed for connection of a potentially very large number of the boards up to a PC. The board itself is split into three main sections, the neuron unit, the connection unit and the spike event list. The actions of the individual sections is fairly self explanatory in a broad sense but, being designed for acceleration of large networks uses a large number of tricks to allow it to run fast - because of this is gets exceedingly complex and instead of trying to explain how each part works I will give an overview of its techniques used to achieve high speeds:
The event list is local to each board but directly spoken to by the host computer; it concerns which neuron has fired and those to be hit.
Several mathematical shortcuts are used, such as, due to decay, many action potential are effectively zero. In cases where this is true, instead of performing equations involving the action potential (Known as IP in MASPINN documentation), established values are inserted into them.
The board is capable of simulating neurons with different sets of parameters (allowing for instance, regular spiking and fast spiking neurons). It has been setup so that this is optimized, and similar neurons work together.
The amount of information available about simulations of SNN hardware is limited. There was only information available about simulation of various parts of the MASPINN system created during and prior to its silicon implementation. One paper documented the simulation of the entire neuro-pipe chip - which is the backbone of the board 10, the other detailed the development of the decay module for the chip 9. They seem to have used to simulations to sketch out implementation methods, and by profiling it, worked out how to make it run as quickly and efficiently as possible. Their simulations seemed to go to a great depth, with classes developed so as to allow realistic simulation of VLSI parts. They did such things as setup signal codes representing knowledge being passed around in the form of high/low signals through pins. It all seems to have been done with an eye of enhancing running speeds of the finished product.
The design plan for this project is far from linear, and cannot simply be laid out straight from the start. The first step is to reverse engineer the original Izhikevich SNN program, and work out exactly how it works. With this done it will be possible to lay down the core designs for how the simulation will work overall, with emphasis on the modularization of subsystems. This first design will only be for a simulation in an abstract sense, with the basis laid down for further add-ons which will enhance it as a simulation.
The original source code for a Izhikevich type neural network was available in two forms, as either MATLAB or C code 11. Reverse Engineering the C code was a necessary step to this project, as I had never had any experience with SNN 's. By going through the code, examining each section and rewriting it as pseudo-code, I was able to tell what was simply an artefact of Izhikevich programming style, what was a core feature of SNN 's and what was necessary to the Izhikevich model. The pseudo code I created is available in the appendix.
Having now analysed what spiking neurons are and a large range of related systems - hardware, simulations and purely software, it is now possible to lay down a set of rules which the final system must obey. Matters such as how input should be gathered and entered, and what network topography should be used will be left to the next sections. Here are the initial requirements:
The hardware version should be able to work in parallel with the aim of allowing real time simulation - these parts should be based on dedicated hardware.
It should be able to support any number of neurons.
The hardware should be expandable to suit networks of different sizes.
Different types of neurons, such as Regular Spiking and Resonators should be supported and the variety and number of which are in any system should be editable by the user.
The ratio of excitory to inhibitory neurons should be variable.
Inhibitory neurons should not be able to change to excitory and visa versa.
Floating point inputs and outputs should be entered and taken from the system.
The SNN model should used Axonal Delays, Spike-Timing Dependant Plasticity and the Izhikevich internal potential equations.
There should be a user interface allowing selection of network choices, input of data and viewing of network output.
Should each 'neuron' have its own piece of hardware?.
The first of the hardware projects looked at had a separate chunk for each neuron, and had areas of memory dedicated to each. MASPINN on the other hand was simply an accelerator and only had one of each component and was able to run any number of neurons. As it is important that this hardware/simulator be able to simulate any number of neurons, it should not have a physical limitation - a low one anyway - on the number of neurons that can be run on it per board. As the simulations for one neuron can be run at a far greater speed than required for real time simulations anyway, the number of neurons run per board could instead be based on the speed of the operations and the amount of memory on the chip, and the extent to which these enable x number of neurons to run at real-time. Implementing this is however, not strictly necessary for the first version, and thus shall be left till later.
Should each 'neuron' be able to directly call each other neuron?.
Neither of the projects looked at had direct connections between neurons. Having such connections would only really have any value or purpose in an analogue system or a digital system where each neuron was physically connected - something that could only happen in a system with very few neurons. In a digital system however, where multiple neurons are run from a single chip, it makes more sense to have a 'global controller' channelling communications between neurons.
Should it all be run on dedicated hardware? Having analysed the original code I feel that it is not necessary to run the entire system of dedicated hardware, in fact I feel it would unnecessarily complicate matters. As long as all the mathematics could be kept onto dedicated hardware, it is best to have a central computer which would route signals between neurons. This idea was used to an extent in the two hardware projects analysed above - in the first, in the form of the global controller, and in MASPINN in the form of the PCI Interface, taking spike data from the neurons and adding it to the event list. The PCI interface is not worth implementing in the first version though.
What network topographies should be used?.
In neural networks, there has always been the concept of network topography. Networks normally take the form of several layers, the first taking in input then feeding this into one or more 'hidden layers', these will then feed this information on into an output layer. The first hardware project looked at used one hidden layer, and MASPINN was designed to merge the two types together - focusing on a general layered topology but with many random connections, of a primarily inhibitory nature. I feel though, that as the original spNET code was developed to work in random topography, with no enforced order, I should do the same with the first version to aid testing. Adding a layered topography should be considered an important next step though.
Should axonal connections and delays etc be explicitly stated by the user? While this would be a useful and interesting feature for proof of theory in very small networks of neurons, the complexity of building a network by explicitly stating connections would be an implausible waste of time, and effectively impossible to comprehend. Although it might be a useful tool in testing I do not feel it is currently worth the time required to implement it. Instead they should be randomly generated when the network is initialized.
Should it be able to handle input & output?.
Complicated input and output are beyond the scope of getting an initial version to work, although again, it may be useful for testing on some levels, the added complexity of the user interface that would be required would haul back testing time significantly. This should therefore be left till after.
Having now considered the requirements and associated issues an initial specification for the first version can be put together.
The hardware system would consist of a piece of software run on a host computer. Connected to the host, would be a number of boards - each capable of simulating any number of neurons - working in unison to host the network. The simulator will consider this as a control module and a neuron module - they should communicate directly.
All mathematical processing should be done in the neural module, and the work done in the control module constrained to simple data channelling tasks.
The learning model used by the system should be Spike-Timing Dependant Plasticity, and the activation model, the Izhikevich type neuron. These should be hard coded into the system.
The system should be able to accommodate for user specified numbers of neurons, and the ratio of their types.
The system should generate random inputs, and feed them into the network at random locations. No specific output beyond firing rate should be given to the user.
A random topology should be used by the network, and the connections between the neurons within the network should be randomly generated.
The nature of this program is such that it could not be usefully represented by a Jason Structure Diagram, and so I have used a basic structure diagram to represent it. The diagram shows the modules as containers, core functions as squares, memory stores as rounded squares and data movements between them as directed arrows:
The action of each function is as follows:
CONTROL LOOP:
HIT MANAGEMENT:
STDP FUNCTIONS:
ACTIVATION FUNCTION:
INITIALIZATION FUNCTION:
The key problems with implementing this program lay with the validation testing, and ensuring that it was functioning as required. This was a great problem because it was essentially a learning process - I only knew how the network and its dynamic parts should behave, once I had examined it in numerous different situations, with different variables and configurations. Comparing output between the original program and that which I had developed was an important method, but was complicated when I discovered a flaw in the original program - it then became a case of guessing which was right, based on assumptions of desired values (i.e. firings rates should ideally be below 1% of network capacity).
Even had I known exactly what trends and patterns I should have been looking for in the data it would still have been an immense task. The problem was that numerous features only really emerged when the program was configured for large networks - in the order of 500-1000 neurons. When you run a network of this size, with time splits of millisecond size, the amount of data generated is colossal. As the standard debuggers were no real use for examining the multidimensional, dynamic arrays used to store most the data, all testing had to be hard coded, and data sent to the display. As most the time, many variables for each neuron had to be considered side by side, millisecond after millisecond, and cross referenced, backwards and forward; it frequently got frustrating and confusing.
Beyond the above issues, implementation was standard - a few bugs as ever, and a few cases of having to learn new facets of the language. One thing that I found quite interesting was that I had never had to use the class libraries for dynamic arrays (i.e. vector or deque) before as I had always written my own. However, it was simply not possible to use my classes with this, as they slowed the program down immensely. Instead I had to select the appropriate type of array for each task, based on whether they could perform their core functions required of them in constant or linear time.
As noted above, the testing caused great problems, and was in all, far harder than writing the program in the first place. It was not possible to use a standard testing framework, and thus, I cannot simply what my test plan was, along with expected results, actual results and required changes - it was instead necessary that I concocted a new test every time I uncovered a new lead, on the path to tracing problems back to their source. Here is an overview of how this worked:
Problem: - No explicit problem.
Plan: - Set up 10 neurons - 8 excitory, 2 inhibitory. Before control loop starts display:
The test should then be repeated with larger numbers of neurons, and checked that it roughly still works. Here is what data I expected to get from this:
There would be one or two synapses per neuron.
The synapse numbers would be 0 and/or 1.
The synaptic targets would be between 0 & 9 and extensive.
Weights should be 6 for excitory neurons and -5 for inhibitory.
Axonal conductance delays should be between 1 & 20 and not necessarily extensive.
All preneurons should have synapses leading to all post neurons.
All post synapse will correspond to a preneuron.
Predelay data corresponds with post-delay data.
The first thing I noticed upon running it was that neurons had either zero or one synapse - this I fixed by making the line of code which selects the number of neurons, add one onto the number it generates. The next thing I noticed was that not all neurons had post neurons. To fix this I added a check into the main initialization function to ensure all neurons had post synapses - if they are found lacking, then a post synapse is added. It was also immediately apparent that predelays were being added ten times over - this was because the code section where they were added to the array was inside an external loop. By removing the section to outside that loop, the bug was fixed.
Although the original problem was now solved, I was still faced with appalling speeds (two minutes and over to simulate a single second). The firing rates were also up in the hundreds. I performed the next batch of tests:
The program had by now stopped running for networks smaller than 100 neurons. I continued on, planning to lose this problem on the way:
At this point I decided to simply try outputting all the information I could get about each neuron to the screen, as best formatted as possible, and to just stare at it till patterns jumped out at me. Through doing this, I discovered firstly and most importantly, that inhibitory neurons were having no effect (there were no negative inputs), delays were two ms shorter than they should be and voltages and recovery time can hit negative infinity and remain there for ever, after the 19 th millisecond.
I found the solutions to these were to adjust the line of code which reads from the hit list, and to ensure the hit list was properly initialized to zero (previously the 20 th value had been random garbage). By fixing the problem with delays, the problem with inhibitory neurons not hitting was solved. Fixing this problem, in turn solved the problem of extremely high firing rates, which in turn, solved the problem of extremely high run times.
There was still a strange problem though - synaptic weights always seemed to reach their limits rather quickly and on average they tended to levitate around them. While I appreciate the erroneous nature of this, the network still functions reasonably with it - albeit causing a higher firing rate than strictly required. Due to time constraints I was forced to leave solving this to my own free time.
With the core part of the programme now working, improvements can now be made with two key aims: making the network more useful, and more of a simulation than a simple program. The three key ways I plan to do this are mentioned in section 2.4.1. The issues at hand are implementing data input and output, a layered topology such that the network becomes more useful, and building a PCI Controller class through which all communication between the host computer and the neuronal board must be channelled. There are many other changes that could be made, but time constrictions are such that implementing all of these changes are unlikely.
Here is the final specification:
The hardware system would consist of a piece of software run on a host computer. Connected to the host, would be a number of boards - each capable of simulating any number of neurons - working in unison to host the network. The simulator will consider this as a control module and a neuron module - they should communicate through a PCI bus.
All mathematical processing should be done in the neural module, and the work done in the control module constrained to simple data channelling tasks.
The learning model used by the system should be Spike-Timing Dependant Plasticity, and the activation model, the Izhikevich type neuron. These should be hard coded into the system.
The system should be able to accommodate for user specified numbers of neurons, and the ratio of their types.
Inputs should be read from a file, and inserted into 'input' neurons in the form of hits, where the input is the weight.
Output, in the form of spike timings for each output neuron, should be written to file each second.
A layered topology should be used by the network, and the connections between the neurons within the network should be generated with respect to this. Input neurons should have no preneurons, output neurons should have no post neurons. The user should be able to specify a number of hidden layers, between which information will be passed mainly forward, but with some backwards transfers. Each neuron on one layer should have a connection to every neuron on the next layer.
The structure diagram remains almost identical, but now with a PCI function separating the two main components, and two file stores for I/O:
As a conceptual aid the following diagram can be used to think of the system as hardware:
I feel that the best way to implement input initially will be to have a separate program generate a file containing a list of floating point number, based on some function, the length of the main control loop. Output can simply be churned out as a list of neurons and every occasion on which they fired - I feel it is more useful to do so for every neuron, as this allows for easier neurocomputational studies on the system. Implementing layers can be done fairly simply, I believe, by just making a few small changes to the section of done which handles synapse selection, which limit where the synaptic target is drawn from - this assumes it is possible to setup layers as sets of neurons, whose indices are continuous - allowing for the divide in values between excitory and inhibitory indices.
By looking at all the function headers for transfer between functions in the host computer and on the board, I was able to determine that there was a generic structure used in most of the transfers - there was a mention of the neuron referred to, a sub-reference and then a piece of floating point data. Even if the exact pattern was not exactly met in all circumstances, the data could be manipulated to fit the form - Boolean represented as 0 & 1 integer values, integer values represented as floats etc. Even requests for data from the other side can be made using this function, as I have imbued the PCI controller with a basic form of processing power, allowing it to direct and pull information. Having determined this, I plan to implement the PCI bus with one function being involved in all data transfers, it shall have the form:
PCI(function,index,subref,data).
Here is a screen shot of the displayed output:
To show the input and output as text is realistically not possible, as it would fill a small library if the program was merely left to run for five minutes. I would display it as a sequence of graphs, but have not found time to write the relevant MATLAB code yet. Here is a brief look at what some of the output looks like:
The values on the left are the number of the neuron, and those on the right are the time of each spike in milliseconds.
When I tried generated an input file the same length as the main control file, the file reached ridiculous sizes in the region of 50 megabytes. I therefore settled for a quick fix of simply reducing the number of seconds simulated. In many circumstances though, inputs would be generated either randomly, based on some function made internal to the program or fed in from some external sensor generated at real time. Because of this I do n't consider this a great problem.
After adding the PCI calls, the code became a lot harder to read and understand, as the "English speak" function names were now replaced by a block standard function call, and while it is possible to differentiate between them by their function numbers - it 's simply, a lot harder. This added even further to the complexity of debugging and testing this new version of the program. The result of this highly complex testing and a lack of time has been that I ran out of time to finish implementing this version. The neural network still has a bug in it, resulting in neurons not being fully affected by spikes hitting it, and also the layered topology has not been implemented at all.
Testing with this new version progressed in much the same way as the last, with each step leading onto the next possible step along the trail. As pieces of data were taking long journeys from being created through the neurons, up an axon, across the PCI board, into the hit list, into the activation function etc, finding out what part was n't actually working was not approachable in terms of standard testing, and noting each individual test down would have slowed it down to an unworkably low speed. Suspicions just had to be clarified and leads traced back to their roots, as and when they popped up. While I did solve some errors - such as a mix-up in the PCI unit over whether a 0 or 1 signified an excitory neuron was causing the neurons to be setup wrong. In the end though, it was not possible to fix all the bugs before the time was up.
When all is said and done, I am very proud of this project, even though it is not really complete and has much work left to go on it. It has been a fascinating task and I have learnt a huge amount from working on it. When I embarked upon it, my knowledge of spiking neural networks was next to non-existent and it was the perfect way to learn more about them. While battling through mountains of data trying to debug it did get extremely frustrating, it has given me a good, clear understanding of the processes that occur within the neurons.
Due to the enourmous amounts of data generated, and calculations done by the program it forced me to deal with certain issues I had never had to consider before in my time as a programmer. How to optimize the speeds of processes, selecting functions that can operate at the lowest order of time possible - never before had I had the need to really consider these things, and it brought into perspective all that I have learnt recently in the algorithms module. And while last programs of mine have generated multidimensional, dynamic arrays of regularly changing data - it has never quite been on this scale before. The methods I found and used to order my thoughts, and the data in such a way that I could handle debugging was an important lesson for me.
While a few of the requirements and parts of the specification have not quite been met - for instance, the interface is not quite up to the standard suggested, only four types of neurons are possible and a layered topology is not implemented, I consider this to be simply part of the nature of the program. There will always be more work that can be done with it - and as soon as I can find the time, I have every intention of continuing through with it, to add as many of them in as I can.
There are many more features that I wish to add to it yet. A full SDL interface, real time graph plotting of spikes, multithreading for the different neurons, more user control over network setup and types of neurons and different ways of entering input are just the start of what I want to do with them - and they can only really be done once I've filled in the holes within the current specification.
There are still two key bugs within the program, but these should be easily dealt with. I will be removing the PCI interface from my version, and returning it to a purely software oriented program - this will remove the 'no-secondary-fire' problem from the second version. The other problem - that synaptic weights quickly reach their limits, can be dealt with by making changes to them occur through the use of a derivative, which slows down the rate at which they change, stretching it slowly across a few seconds.
I do feel that the level and range of the actual programming demonstrated within the program was unfortunately fairly low, but due to the huge amount of research, testing and debugging required I did not find time to really push the limits, as this would have required a greater range of functionality to make room, and require a wider range of programming skills.
If I was to do it again I would make it easier to perform the testing. By adding in a separate function or so, which could adjust certain values, and output strings of values, which again and again I found myself having to adjust and code for, I think I could have cut a large amount of the development time. But as they say - we live and learn. Or more precisely - our neural nets do.
