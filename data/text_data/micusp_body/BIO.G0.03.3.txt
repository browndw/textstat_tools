The development of a testable hypothesis is an important part of the scientific method and is a key characteristic of good science. Hypotheses help to define the focus of research and create experiments that can provide answers to meaningful questions. Without well-defined hypotheses researchers run the risk of making erroneous conclusions from data that was not collected in an experiment appropriately designed to test for those conclusions.
In this study I examined the difference in frequency of hypothesis testing between two journals. I hypothesized that the best quality journals would be most selective in accepting submissions and therefore would be most likely to publish articles which strongly adhere to the scientific method. I predicted that the highest quality journals would have more articles with a clearly defined hypothesis than lower quality journals.
As a quantitative measure of journal quality I used the journal impact factor which is defined by ISI as, "a measure of the frequency with which the 'average article' in a journal has been cited in a particular year or period" (Thomson Scientific website). The rating is calculated by dividing the number of times articles from the previous two years in the journal were cited in the current year by the total number of articles published in the previous two years. The use of the journal impact factor as an indicator of journal quality is controversial in the scientific community, as a number of concerns have been raised about it; however, for the purposes of this study it provides the best objective and quantifiable measure of quality.
In order to test my prediction that articles from higher-impact journals would have clearer a priori hypotheses, I examined five articles from each of two different journals. For the high-impact journal I selected Ecology, which is in the top ten of ecology journals with an impact factor of 3.7. As a comparatively lower-impact journal I selected Plant Ecology, with an impact factor of 1.28. Articles were selected based on time of publication (2000-2005 only) and subject matter (invasive plant species).
In evaluating each article based on the clarity of the hypothesis, I assigned each article a rating from 1 to 3. A rating of three is indicative of an indisputably clear hypothesis stated in the article abstract or introduction. A two signifies that some background theory and research questions were addressed, but no specific hypotheses were provided. A rating of one designates those articles that did not have any indication of a hypothesis being tested.
Table 1 shows the ratings I assigned to each article as a quantification of the clarity of the hypothesis being tested in the experiment. Articles are numbered according to the order that they are presented in the literature cited section at the end. All five articles in the journal Ecology received the highest rating, indicative of clear hypotheses unambiguously stated in the article's introduction. Hypothesis testing in Plant Ecology varied from 1-3.
The data appear to support the hypothesis and prediction, as all articles from the high-impact journal had clearly stated hypotheses compared with only two from the lower-impact journal. Admittedly, this is not a conclusive finding as the sample size used was quite small and the ranking system was not completely objective or scientific.
For the articles given a rating of 3, hypotheses were explicitly stated, generally in the form "we hypothesized that..." or the typical "if...then" format. All of these articles addressed their hypothesis in the discussion, stating whether it was supported or contradicted. In all cases where the results were inconsistent with the hypothesis, alternatives hypotheses were proposed along with ideas for future investigation into the a posteriori hypothesis.
The article that received a two (article 7) listed a series of questions the study was designed to address but did not suggest an answer to any of these questions a priori. In the discussion at the end, the authors addressed each question individually and proposed several a posteriori hypotheses based on the data they collected. Interestingly, this illustrated some of the problems associated with this style of experimentation. The authors acknowledged that they were unable to provide an answer to their first question because their experimental design was unsuitable for doing so. Similarly, few concrete answers were found for the subsequent questions as the authors were primarily only able to provide speculation based on data and observations. Had they designed experiments to test specific a priori hypotheses, perhaps they would have been able to make more definitive conclusions.
The two articles that received a rating of one broadly defined the aims of their experiment but did not narrow the scope enough by providing a specific hypothesis. The introductions of article 10 and article 8 explained what they hoped to determine as a result of the experiment but did not provide any predictions or indicate any expected results. Both papers were more exploratory in nature but both achieved results which seemed to satisfy the objective of their study.
All papers seemed to be logical and well-organized, partly due to the fact that all scientific papers follow a similar format: abstract followed by introduction, materials and methods, and so on. This essentially forces the author(s) to structure the paper in a way that is logical for the reader. The only way in which the presentation of papers might be improved is by making a more explicit connection between a hypothesis and the experiment designed to test that hypothesis. When papers have multiple hypotheses in the introduction and multiple tests described in the methods section, it would be helpful to know which test was done specifically to address a particular hypothesis.
Hypothesis testing has always been presented as a cornerstone of science so it is surprising that a significant number of science articles are being published without clear hypotheses. The results of this study suggests that there is a possible positive correlation between a journal's impact factor and hypothesis testing within its articles, though more study needs to be done. In order to test this hypothesis more thoroughly it would be important to use larger sample sizes and select articles from additional publications with both high and low impact factors.
