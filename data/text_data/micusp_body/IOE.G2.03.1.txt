The entire course was centered around a single problem dealing with the manufacturing of printed circuit boards. Printed circuit boards are made up of many different types of components, including transistors, diodes, and capacitors, which are arranged in different numbers in different places on a substrate, or blank board, to form the final printed circuit board. The components are stored in sleeves in a pipe-organ setup above the area where the substrates are held, with a retrieval arm that moves between the sleeves to pick the correct component. Each of these different elements needs to be placed individually on the substrate by a pick-and-place machine. This process involves two steps: first, the correct element must be retrieved from the correct sleeve (corresponding to the type of component needed); second, the substrate needs to be moved below the insertion point so that the element is placed in the correct location.
This is a rather simplified version of the problem, however, because the setups can be torn down and reassigned, for a time cost of σ. We can immediately see that if σ is very large in comparison to the overall production time, then it will be in our best interest to put all the boards together in one setup and manufacture them according to the optimal setup for all boards. If σ is very small, or zero, then we will tear down the setup between every board and manufacture each board according to its own optimal setup. From here on out, we will denote an individual board's optimal setup cost by where b is the number of the particular board. The difficulty arises when σ is neither 0 nor exceedingly large, but somewhere in the middle. Now the problem becomes not how to order the components in the sleeves, since the optimal configuration is uniquely determined by the boards being produced together, but exactly which boards to group together into a cluster to be produced under a single setup. Since there are n ways to group n boards into clusters, or sets, of 1 or n-1,
Our first step to try and get a handle on the problem was to try and come up with a good heuristic solution. In this assignment, as in many of the others that involved implementation, the most difficult and time consuming piece was arriving at a suitable data structure. I could not figure out a way to store the clusters as vectors of the boards included, since the dimensions would change from cluster to cluster, so I settled on a system of storing the clusters as a 24-element vector with the first component corresponding to the number of the cluster that contained the first board. This could be arbitrarily set to one, but the random-generation code that I wrote was easier if it could just assign random numbers between 1 and t where t was the number of clusters into which you wanted to partition the boards. Looking back, much of the implementation would have been easier had I used the system of 24-element vectors of zeros and ones, where a one signifies that that board is in the cluster. But, using my rather clumsy data structures, I was still able to create a randomized loop which would generate 500,000 instances for clusters of size two to twenty-three, and then would save both the average cost across all instances, and the lowest cost along with the vector that generated that lowest cost.
The best answer we were able to generate from this method was 906,231, with an optimality gap of 2.22%. This optimality gap is misleading, however, because we were comparing it with the only lower bound we had -- manufacturing all boards alone according to their optimal setup with no tear-down cost -- which is clearly an infeasible solution. So in a little over two hours, through our random generation, we were able to get a solution barely 2% away from an impossibly good solution. In reality, our answer was only 0.23% off the actual optimal solution of 904,145. In nearly all industrial applications being 0.23% away from the optimal solution would be more than enough. However, though this worked fairly quickly for the particular data set we were given, we have absolutely no guarantee that it would work for other data sets, or larger data sets. On the other hand, we have seen that having a good feasible solution to start from can result in substantial decreases in the time required to solve the problem, so using a random-generation algorithm such as this might prove useful in obtaining good initial solutions. We also briefly discussed using genetic algorithms in combination with this randomized procedure to generate even better solutions. Unfortunately I do not know enough about genetic algorithms to actually be able to implement them in any consistent fashion, but from the little I do know, my guess is that we could have improved our final solution by breeding several near-optimal solutions together and looking at their offspring over several generations. Additionally, we talked several times about Dushant's local neighborhood search, and the potential it has to help generate good initial feasible solutions. I know even less about neighborhood searches than I do about genetic algorithms, but it seems like it might be an interesting area to look into, especially since outside of the realm of academia, many people are more interested in "good" solutions than in provably optimal solutions. However, being firmly ensconced in the realm of academia, a heuristic solution is not enough to satisfy our unceasing hunger for provable optimality. So we endeavored on.
In order to find a provably optimal solution we must first have a formulation. The first formulation we looked at was George Polak's model (the Polak model) from his published paper.
Since the original model was so difficult to understand, we were given the task of coming up with new models, in the hopes that we could come up with something a little easier to understand. The most intuitive way to construct a model would be to have if boards i and j are manufactured together. Unfortunately, there is no easy way to determine which boards are in a cluster, nor any way that we could come up with to sort them, given this definition of x. Oddly enough, this idea of defining whether two boards are paired together or not turns out to be a very effective branching strategy later on for the master problem, though in a different form. Setting that particular idea aside, Shital and I came up with three models, none of which held any great hope of yielding a solution. Our first model was a slight adjustment to the Polak model, using all the same variables.
This model presents a slightly more intuitive definition of where if boards k and m are in the same cluster. Unfortunately, in order to achieve this, we had to introduce a non-linearity into the model in the form of absolute value. So while it may be slightly more intuitive, the non-linearity excludes it from being truly useful in solving the problem. But thinking about this model led us to our second model which eliminates the issue of determining how many setups there are by assuming that we know there are S setups. Thus, this second model would need to be solved once for S=1,...,N, where N is the number of boards.
Here if cluster r contains board k, and 0 otherwise, returning us to the counterintuitive "0 means yes" definitions from the earlier Polak model. The first two constraints are the same as in the Polak model, and the third simply says that each board must be contained in exactly one cluster. The confusion arises from the fourth constraint; however, if we look more closely we can see that the constraint is only meaningful in the case when , i.e. boards k and m are in the same cluster, thus their setups must be the same. This is exactly what is implied by the second part of the constraint, since then it must be true that , and whether they both are zero or one is immaterial. Additionally, when the constraint drops out and becomes meaningless. Thus, this fourth constraint guarantees that all boards in the same cluster have the same setup. This model seems much more elegant and intuitive than the original model. Unfortunately, it needs to be solved N times (where N=number of boards). As we were running the random-generation code, we noticed that nearly all of the best solutions had between six and nine clusters. If this was a structure that was common across all data sets, a formulation such as this one would allow us to exploit that and only look at the solutions with the likely number of clusters. However, in order to be able to prove that a solution obtained in this manner is optimal, we must be able to show that the function of optimal solution value vs. number of clusters in the solution is convex, which we will address in the next section. The final model we tried took an entirely different view of the problem, setting a new variable if board b is manufactured according to setup s, and 0 otherwise.
Here,
Throughout the process of trying to come up with alternate formulations for this problem we realized just how difficult it can be to effectively model a problem that can be described in only a few sentences. Later, as we looked at modeling the subproblem, this was really hit home -- translating problems into math programs can be exceedingly difficult, even when the problem itself seems relatively easy to describe.
If we knew that the optimal value was convex in the number of clusters in the solution we could take formulations such as Model 2 above, and solve them for increasing numbers of clusters until we saw an increase in function value. If the function is convex, once you see an increase the function value will continue to increase, implying that our previous solution is, in fact, optimal. This has the potential to save a great deal of computation time if the optimal number of clusters is small relative to the number of boards. If the optimal number of clusters is close to the number of boards, then even if the function is convex, it will not save us a great deal of time since we will have to solve the problem for all, or nearly all values of S. Working with Shankara, we tried to find a counterexample using a set of only three boards with three components. Our flaw was assuming symmetric sleeve costs; we tried all possible combinations of boards and found that all of them (under the assumption of symmetric sleeve costs) were convex. Others in the class rejected the assumption of symmetric sleeve costs and thought they had found a counterexample, though it was later proved to be incorrect. Further, their counterexample was an extremely pathological case, with strongly asymmetrical sleeve costs. All the evidence we could find supported the conclusion that with symmetric sleeve costs, the function would indeed be convex; but we were unable to provide a proof, so it is all merely conjecture. Yet even if we could prove convexity, it would mean that we would need to solve an impossibly difficult problem slightly fewer times, but we still do not know how to solve the problem even for a given number of clusters.
Another dimension of the solution involves the integrality of the solution. Clearly we cannot have fractional amounts of clusters in an optimal solution; but solving the linear programming relaxation of the problem is vastly easier than solving the integer program. The question then becomes: If we solve Polak's model with the x variables relaxed, will the solution still be integer? If the constraint matrix is totally unimodular (TU) then yes, the solution to the LP relaxation will be integer. If the constraint matrix is not TU, then the solution may or may not be integer. So we set out to prove that the constraint matrix of the Polak model was TU. Unfortunately, our proof turned out to be flawed and we were unable to correct for the flaw, though it does seem that in most instances the constraint matrix will be totally unimodular. At the end of the course, looking at the solutions using the rank-cluster-price algorithm, most of them did turn out to be integer, especially for small problem instances, implying that we might well be correct that the formulation is indeed TU. Regardless, the solutions do turn out to be integral in many instances.
The integrality of the solution is related to the strength of the LP relaxation. If the LP relaxation is close to the convex hull of the integer program, then the solution will often be integer. Another way of conveying the idea of total unimodularity, is that a constraint matrix that is TU describes exactly the convex hull of the integer feasible region, and so all the extreme points will be integer. In fact, we do not particularly care if all the extreme points are integer, only if the extreme points near the optimal solution are integer. But we were unable to prove that even just the extreme points near the optimal solution were integer, though it might be a more efficient way to approach the proof. Regardless, the better your LP relaxation is, the fewer nodes there will be in your branch and bound tree once you start trying to solve the integer program. As we saw later, this can have a profound impact on the solvability of a problem.
Having nearly exhausted the store of knowledge in the class without additional information, we turned our attention to Dantzig-Wolfe decomposition, which motivates the technique known as column generation. The idea is that we can separate the problem into a set of subproblems which are then linked together in a "master problem." Typically these subproblems have some special structure, such as an assignment problem, that allows them to be solved quickly and easily. This technique is especially useful if you have a set of constraints that affects only a small group of variables, thus those variables and constraints can be separated out and put into subproblems, with "linking constraints" in the master problem which link all the subproblems together. These subproblems then generate a set of extreme points of the solution. And we know that the optimal solution can be expressed as a convex combination of the extreme points of the subproblems, so we can replace the variables in the master problem with these convex combinations of extreme points, and solve. If the subproblems are easily solved, this can prove to be a very powerful mechanism for solving large-scale problems. The key, however, is that the subproblems need to be easy to solve.
With the column generation technique, motivated by the Dantzig-Wolfe decomposition, in mind we set out to define a master problem and subproblem to which we could apply these ideas. We fairly quickly found our master problem:
Here, we change notation slightly where K is the set of all clusters being considered. if cluster k is in the optimal solution, and 0 otherwise. is the cost of cluster k including setup cost σ. is an index variable which equals one if board b is in cluster k. Thus the only constraint in our master problem is that each board must be contained in exactly one cluster, and we want the set of clusters with minimum cost. It seems simple enough, and the idea behind the subproblem is fairly simple. We even know what the subproblem is: to generate the most negative reduced cost column. Then we would take that column from the subproblem, enter it into the master problem, get new duals from the master problem and hand those off to the subproblem and repeat until our subproblem reports back that there are no negative reduced cost columns, showing that our current solution is optimal. Regrettably, this turns out to be much easier said than done.
The reduced cost of a column (cluster) is its true cost minus the dual associated with the boards included in that cluster. The true cost is the manufacturing cost of all the boards included in the cluster plus the setup cost, σ. Thus, we define variable if component c is retrieved from location l for board b in this particular cluster, and 0 otherwise. Additionally, we use if board b is included in the cluster, and if component c is assigned to location l. This results in the first version of the subproblem,
This model minimizes the reduced cost, subject to the following constraints: each component is assigned to exactly one location; each location is assigned exactly one component; if component c is not assigned to location l, then component c cannot be retrieved from location l for any board b; finally, if xb is one then each component must be retrieved from a location for board b. Together these constraints define a feasible column, and the problem will find the feasible column with the most negative reduced cost. Unfortunately, this problem would not yield an answer in less than 2 hours with either the x, or the y, or the x and y variables relaxed. When we relaxed all the variables, we ran into a different problem: the LP relaxation is weak. When you solve the LP relaxation with all variables (x, y, p) relaxed, the optimal solution is fractional. The reason the solution turns out to be fractional, while not immediately obvious, is because the model is splitting the components for the boards it wants into different sleeves and only paying for the cheapest one. And while it will solve to an integer solution with some dual values, we only got it to solve with made-up dual values, or the dual values from the near-optimal solution. Unfortunately, after only a few iterations of shuffling between the master and subproblem starting with the near-optimal solution, the duals become such that the subproblem will no longer solve to integrality. In fact, even with all the variables relaxed we could not get it to solve with the duals from solving the master problem with the identity matrix.
All these frustrations drove us to look at an entirely different model,
This model is exceedingly difficult to understand. In this model the xcl variables take the place of the ycl variables in the previous model, the yb's replace the xb's, and the is data simply taking the place of in the previous model. When M is large enough, and yb is one, implying that the board is in the cluster, the constraint says that
Unfortunately, as we have seen, introducing a big M into a model almost always results in a terrible LP relaxation. Here, the problem arises because the model has two constraints to determine the value of θb:
Despite all these problems, we were able to get a single instance of this version of the subproblem to solve with the duals from the identity matrix in just over twenty minutes. But returning to the big picture, this means it takes twenty minutes to solve one instance of the subproblem to optimality. In order to solve the root node of the master problem, we might need to solve the subproblem millions, or tens of millions, of times. And then, we have no guarantee that the solution to the root node of the master problem will be integer, so we might have to branch on the master problem, meaning we would have to solve potentially hundreds or thousands of versions of the master problem before we arrived at a truly integer solution to our overall problem. Given the complexity of the overall problem, a subproblem that takes twenty minutes to solve is simply worthless in the overall context. And since there is really no way to remove the big M from the model, we were forced to chuck this rather creative model, returning our focus to the seemingly less-promising Subproblem Model 1
Looking at Subproblem Model 1, we noticed that the variables were simply taking the place of xbycl, and serve only to eliminate the nonlinearity in the model. So what if we left the nonlinearity in the model? We end up with a new, nonlinear model:
This model boils down to an assignment problem, which anyone can understand quickly. Additionally, assignment problems typically solve very quickly. We used the MINOS package to solve this model, and the subproblem solved in mere seconds. Shuffling back and forth between the subproblem (in MINOS) and the master problem (in CPLEX) was regrettably too time consuming to really be able to see if the nonlinear model would yield an answer to the subproblem, however it does appear that it would be a good method to generate good columns quickly. To prove optimality in any of our other models, we need to show that there exist no more negative reduced cost columns. The worry is that this nonlinear model might return a positive reduced cost column when there were still negative reduced cost columns out there. But at the very least, we would only have to solve the long linear model a few times, possibly only once, to verify that the nonlinear model had given us all the available negative reduced cost columns. Unfortunately, before we can actually test this theory, we would need to be able to write a .run file which would allow us to switch between MINOS and CPLEX to solve the subproblem and master problem, respectively, without having to do it by hand. It would seem as though there would have to be a way to accomplish this, although we were unable to find anyone who knew how to tackle this particular issue. With a better implementation, this method would seem to hold great promise for solving this problem quickly. I think this is perhaps one of the most exciting avenues we ignored.
Since we could not find a way to get the nonlinear model to solve automatically, we were forced to return to our original model, Subproblem Model 1. We could not get Model 1 to solve with the duals from the identity matrix, so we tried solving it with the duals from the many iterations we did by hand using the nonlinear model, thinking that these duals might be "better" in some sense. Unfortunately, they did not yield a solution either. Then we had the brilliant idea of using the near-optimal solution from the first week as a starting solution for our master problem -- this eight column solution gave us duals that allowed the subproblem to solve lickety-split. This solution raised issues about the validity of the duals from a solution that did not form a basis. In order to form a basis, you need as many variables as you have constraints; here we had only eight variables, one for each column (or cluster) and twenty-four constraints, one for each board. What does this mean for the validity of the duals? After much thought, we decided that even though we do not have a full basis to start with, the duals are nonetheless meaningful, since we end up with exactly eight (the number of columns we started with in the initial solution) non-zero duals. All the other duals, corresponding to the extra variables that are needed to form a basis are simply set to zero. However, this does yield a strongly degenerate solution, where most of the variables in the solution are set to zero. This means that when we took those duals and plugged them into the subproblem the objective value stayed exactly the same over many iterations, because of the degeneracy. The algorithm was simply pivoting in different variables, which were all set to zero, so the objective function did not change. This is the definition of a degenerate pivot. Degeneracy kept coming up again and again throughout this course, and often drastically increased our computation time.
After a few iterations of switching back and forth between the master and subproblem, starting with the near-optimal solution, eventually resulted in duals that the subproblem could no longer solve quickly, at roughly the same point as it reached a full rank basis. We hypothesized that the speed of the subproblem was due to the reduced size of the problem -- with only eight non-zero duals, you really only have to consider eight boards, which makes the problem much faster. And indeed when we solved the master problem with the eight near-optimal columns plus the identity matrix, the subproblem took more than a few hours to solve, supporting our hypothesis that it was the zeros in the duals which made the near-optimal instance easier to solve. So how are we ever going to get this problem to solve? We need it to give us an integer solution, and we have already seen that the LP relaxation will always give us xb values close to ½, so eventually we're going to have to put it into a branch and bound tree. Perhaps looking at the branching strategy will give us some insight, we thought.
When we started thinking about branching, little did we know we would be spending weeks and weeks on it, and what a big deal it would turn out to be. Our first step in looking at branching was to observe what happened when we branched on different variables. We initially started branching on the xb variables, since they seemed to be the ones we ultimately cared the most about -- whether or not a board was included in a cluster. Branching on the x's changed the one particular value you set to be integer, but left all the other values fractional. As soon as you set the very last x to be integer then all the y's and p's would suddenly become integer, but you need to get all the way to the bottom of the tree before you seen any major changes, which could result in 224 (in our instance with 24 boards) branches before you get a solution. 224 is a large number, and not something you want to sort through every time you want to solve the subproblem (which might need to be solved millions of times to generate an answer to the root node of the master problem, which may or may not be integer). So we decided branching on the x's was probably a bad strategy, all things considered.
If branching on the x's will not work, then what about the y's? Setting a ycl variable to be one means that component c is assigned to sleeve l, which automatically sets all other y's in that row and column to be zero, since only one component can be assigned to each sleeve and vice versa. Additionally, you also set all pclb values containing that component or location, but not in conjunction with each other, to zero. If component c is assigned to sleeve l, c cannot be retrieved from any other location for any board; likewise no other component can be retrieved from location l for any board. So branching on the y's holds promise. Unfortunately, when we look at setting a y variable to zero, almost nothing happens; we see an infinitesimally small decrease in the objective function, nothing like what you see when you set y to be one. This is a potentially hazardous branching strategy, since it results in a strongly unbalanced tree. And here again we have the same problem that we need to completely enumerate the y values before we see the x values become integer. The objective jumps the most when setting y to be one, and the least when setting y to be zero, so perhaps there is no clear cut winner. If we have to completely enumerate the y's it will only be 216 instead of 224 branches, but 216 is still too many branches for a subproblem. But what about the p's?
Setting pclb=1 has an even greater impact than setting ycl=1, since pclb=1 then sets ycl=1 with all the concomitant effects, in addition to setting xb=1, and many other pclb's to zero. Surely this is the largest impact that any one variable can have on the problem. But there are 24*216 pclb variables, and again if we look at the other side of the tree, where we set pclb=0, there is practically no change in the objective, and no other variables are set to be integer. In the end, there is no clear winner -- all the branching strategies have their flaws, and the problem was starting to look completely hopeless. Since we were not getting anywhere with this approach we decided to go back and see if we could figure out why the optimal solution was fractional and if we could fix it in any way.
We had seen earlier that the reason the LP relaxation was fractional was because the model wanted to split the boards between several different columns and then only pay for the cheapest one. We noticed, as we were working through the branching strategies, that when you set ycl to be zero, meaning that you can no longer assign component c to sleeve l, it did not change the objective value, because the model simply moved all the components that were in sleeve l and moved them to the symmetric sleeve that had the same cost as sleeve l, changing nothing, really. This means that we were branching twice as many times as we really needed to. So how can we eliminate this waste? By creating a model that assigns two components to each sleeve, and only has half as many sleeves; then you can randomly pick which of the two components goes in each of the paired sleeves because they result in equivalent costs. This lead us to the next model.
This model differs from Subproblem Model 1 only in that the sum of ycl over all components for each location is two here, instead of one as in the previous model. Everything else remains the same. Since this problem has only half as many sleeves, the number of variables is reduced by just less than half, and results in roughly half as many branches as the original, the assumption was that it would solve faster. We ran a check to make sure that both models gave us the same answer when applied to a smaller, randomly generated data set, and indeed they did, showing that the models, in their integer forms, are equivalent. Hopefully the LP relaxation of the non-symmetric model will be stronger than that of the original. Unfortunately, even with the fewer number of variables, we were still unable to get the non-symmetric model to solve with any of the variables set to be integer using the duals from the identity matrix, implying that while it may be faster, it is still not fast enough. So again, we examined potential branching strategies. Here, instead of looking at what happened near the top of the tree following a breadth-first search, we decided to see what happened near the bottom of the tree and followed a depth-first search. We confirmed that indeed none of the x values became integer until all the y values had been set. More than that, we discovered that there was an order of magnitude jump between the next-to-last branch and the very last branch where all the values suddenly became integer. This is unfortunate because it means that pruning will be nearly impossible -- none of the nodes in the branch and bound tree will ever be above the upper bound of the incumbent solution until they are themselves integer -- implying that we are still facing the potential of full enumeration to solve the subproblem. We also tried branching on the p variables, but since every time we set a p to be one we were forcing the corresponding xb and ycl variables to be one as well, we were in effect randomly picking x's, which resulted in a very poor solution. Again we saw the problem of the imbalance in the tree -- setting whichever variable you choose to be one has a far greater impact that setting that same variable to zero, so the tree is much larger on the zero side. Eventually we decided that examining the branching strategy might not actually be fruitful, and the course came full circle as we turned our energies back towards heuristics.
Heuristics are generally not considered as elegant as traditional linear programming methods, but we were hoping that we could find a heuristic that would generate negative reduced cost columns quickly. This might be an effective strategy because we do not particularly care about finding the most negative reduced cost column; we simply need a single negative reduced cost column (or many if we want to put in multiple columns at a time). The first thing we noticed was that we can automatically throw out boards with negative or zero duals, since there is no way their contribution to the reduced cost can be negative, since we still need to pay for the manufacturing. We also know that including boards with larger duals will be more likely to yield a negative reduced cost.
The first heuristic we came up with started by sorting the duals in descending order. We automatically included the two boards with the largest duals, since we know any new column will have at least two boards (since we started with the identity matrix where each board is by itself). Then, in decreasing order of the duals, we examine each board. If adding the board to the previous cluster maintains a negative reduced cost column, we add it and move on to the next board. If the board makes the reduced cost greater than zero, then we do not include the board and output the previous cluster. This would seem to be a smart heuristic because it will almost always yield a negative reduced cost column, but it fails to take into account any measure of similarity between the boards. If boards 1 and 2 have very large duals but are entirely dissimilar, it might not be a good idea to put them together into a cluster. Additionally, we discovered that this heuristic generated very similar columns at every iteration, because the duals do not change much when you start with the identity matrix. We saw other situations where the duals changed much more rapidly, so there is a chance that this heuristic would work in different circumstances; however, it is not a good bet when starting from the identity.
Our second heuristic harkened back to the random-generation code of the very beginning. We simply generated random clusters, checked their reduced cost, and saved the smallest. If the random generation failed to produce a negative reduced cost column, then we applied our first heuristic. Though we consistently produced negative reduced cost columns, the objective value was decreasing by only a little more than 1% per iteration, so using this method would take a long time to reach a near-optimal solution.
The other idea motivating us to find good initial solutions was the thought that we could then use them in AMPL as better upper bounds, allowing CPLEX to prune the branch and bound tree faster. What we discovered with both of our heuristics was that AMPL generated an integer solution with a better objective value than that of our heuristics in less than a minute, so we were not really improving the pruning by giving it our heuristic upper bound. Perhaps had we been able to generate solutions with much lower objective values it would have made a difference, but given what we had, we made no impact at all. AMPL is much smarter than we give it credit for, it would appear.
Still fixating on the issue of solving the subproblem with the duals from the identity matrix, we decided to test whether or not it was a special property of the identity matrix, or simply the fact that we had a full rank basis. So several people tried multiple randomly generated sets of 24 clusters, and found that often the clusters did not provide a feasible solution. Adding the columns from the near-optimal solution so that a feasible solution existed, however, resulted in a problem that still took just as long to solve as the one using the duals from the identity matrix. Thus, we can firmly state that the faster speeds observed using the near-optimal solution stem not from the near-optimality of the solution, but the fact that the columns do not form a full-rank basis. This exercise did not really provide any profound insight, but at least confirmed that our suspicions were correct.
So now we've decided we cannot come up with a way to generate good upper bounds (or at least upper bounds that are better than those that AMPL arrives at within minutes), we cannot find any inspiration from branching strategies, and we are sure that this is our most promising model. It is at this point that we were all grateful that this was not our dissertation research. So what do you do?
Thankfully, we had our intrepid professor to help us along, and refocus our attention on pruning. Previously, we had agreed that any board with a negative or zero dual would not be included in an optimal solution, but perhaps there was a tighter bound we could use. If we define to be the cost of manufacturing board b according to the optimal setup for cluster C, we know that
Perhaps the most intuitive pruning technique is to look at the tree and see if adding a board to a particular cluster increases the reduced cost. If so, then you can prune that branch of the tree. But this is only a valid method of pruning if you can prove that there is no other combination of boards including the original cluster, the additional board and some subset of the remaining boards which will have a lower reduced cost than that of the original cluster. In other words, can we prove that if
So we have shown that we cannot prune the tree going down, but perhaps we can gain some insight about the future of a branch based on what is left to explore. If the reduced cost of a node is positive, and the lower (or best) bound on the contribution of all the boards left to examine does not outweigh the current reduced cost, then there is no way the branch will ever be negative and we can prune it. We define the potential of a node to be the lower bound on the reduced cost contribution of the remaining boards, or , where R is the set of all remaining boards. Since this is a lower bound on the contribution of the remaining boards, if the current reduced cost plus the potential is not negative, then we can prune the current node. This will hopefully save us some time when we near the bottom of the tree. We attempted to implement this pruning scheme, but unfortunately were stymied by our lack of object-oriented programming knowledge, and we could not figure out a way to store all the data of the tree in Matlab. Here again, formulating the data structures proved to be the most difficult part of the implementation, which is itself the most difficult part of testing the idea.
With this notion of potential in hand, we are now prepared to understand the final solution, though it probably would have taken us another semester (or at least another half a semester) to arrive at it on our own. The final solution involved a slightly modified version of this potential. The potential we defined above assumes each board is manufactured according to its own optimal setup, which we know is almost guaranteed not to happen in an actual solution. So how do you take that into account? We could say that we must account for the cost of manufacturing all the remaining boards together, i.e. if we had three boards left to examine, the potential would be
Why would the algorithm take so very long to find a solution that it already had to begin with? (All instances started with an initial set of columns consisting of the identity, the exhaustive set, all 2 board clusters, all 3 board clusters, all n-2 board clusters, and all n-3 board clusters.) The answer belongs to our old friend, degeneracy. In a situation where the optimal solution is the exhaustive set, you have n-1 degenerate variables with value zero and 1 variable (the exhaustive set) with value one. The algorithm will continue to find negative reduced cost clusters, because it thinks it can make the objective value better, except all those negative reduced cost columns enter into the basis with value zero, and so have no effect on the objective value. There are nearly as many combinations of n-1 boards as there are of n boards, and you can cycle through most of them without seeing any change in the objective. The problem is that you have to find those degenerate columns and pivot them in, otherwise you will never know that you are optimal, because there will still be negative reduced cost columns out there. So how can you avoid this seemingly endless degenerate pivoting? You can't, basically, without exploiting some sort of problem structure.
In this instance, we noticed that the cost of the exhaustive column is
So we have a solution, or more rather, a way to obtain a solution. Was that not the point of the class? Well, we have a way to find a solution to the root node of the master problem, but what if the root node is fractional? Then you have to branch (and bound) to get an integer solution. Surprisingly, branching within the confines of column generation turns out to be quite challenging. The traditional way to branch takes a fractional variable, x, and says it has to be less than or equal to
Branching in this problem turns out to be much simpler than one might initially think. If we have a cluster, say boards 2 and 3, that is fractional, instead of setting x2,3 to be zero or one, we can define the branching in terms of "togetherness." If we want to set x2,3 to one, another way of saying that is we always want boards 2 and 3 together, so whenever we add board 2, we automatically add board 3 in with it. Continuing in this fashion, we can create a togetherness web -- board 1 is with board 4, boards 2 and 3 are with board 7, board 5 is not with 4, etc. And this will eventually allow us to determine our integer solution without violating the needs of the subproblem.
It has been a long, crazy journey on the road to this solution. But in this journey as in most, what is most important is not the destination, but the path taken to arrive there and the experiences gained along the way. The two most important things I learned from this course were the importance of thinking flexibly about problems, and that implementation is a pain. We explored the depths of frustration, we mounted the pinnacles of success, and saw that research is truly an up and down process -- some of the things you try work out nicely, most of them will not; and you've got to learn to roll with the punches.
On the more practical side, this course really gave us a firm handle on what exactly column generation is and what sorts of problems we can adapt to fit it. Column generation is a massively powerful technique, yet elegant in its simplicity. I very much enjoyed getting my hands dirty with the nitty gritty of it all. We learned about degeneracy, and what a pain that can be. We learned about LP relaxations and how important a good formulation can be if you ever want to solve your problem. We saw that big M's are almost always bad news, which will definitely encourage me to look for other ways to model my problems from here on out. One of the things that will really help me in the future is the ability to look at a fractional solution, and think about why it is fractional, step-by-step. Because many of the problems you model as integer programs will, in fact, turn out to be fractional, and many of them will make no sense as to why they are fractional. Being able to analyze a problem in the detail we did in this course, I think will help me a great deal in my future research.
We also saw how to exploit problem structure, and what great advantages that can give you. From now on, I will know to always look for symmetry in my models, to see if there is a way I can reduce the number of variables, and thus the complexity of my problem. We learned about branching -- what had formerly seemed like a fairly straightforward process is now even more of a mystery. I can't say that I gained any deep insight into branching strategies, except that I more fully understand their vast complexity. I did learn, however, how to critically attack branching strategies, and the process one would go through to try and evaluate them, which I think will probably be useful in the future. We saw that using a programming language without good references can be frustrating in the extreme. The majority of my AMPL questions were answered not by the manual, but by other students in the department who had wrestled with these exact same questions.1 But perhaps most importantly, we now have a whole toolbox full of skills to use when we are attacking a problem that just will not budge -- you can take the dual of the problem and see if that gets you anywhere, you can try to reformulate it using entirely different variables, you can look to exploit problem structure, you can closely examine the branching strategy and see if you can gain any insight... Many of these approaches I never would have considered before taking this course. I feel like I am much better prepared to face the frustrations and difficulties that inevitably face researchers now. (Which is not to say I am prepared, per se, simply more prepared than I was previously.)
I really wish we had someone who knew how to write a .run file that would allow us to test the non-linear model. I would be very interested to see how it behaves and if, in fact, it would get us close enough to an optimal solution that then we could throw it at the symmetric model and prove optimality. There is no real way to know without writing that .run file, but it seems like it would be so simple, and the results so fascinating. I also feel like we had some interesting intuition about other branching strategies right there at the end that might have been interesting to explore. Not that it would have been particularly fruitful (given all the other complexities of the problem) but I would think with a little more work we might be able to prove the convexity of the optimal function; it definitely seems like it should be convex, and my dreams would be sweeter knowing that it was. Additionally, I would have liked to revisit the idea of genetic algorithms to see if we could concoct a heuristic using breeding that would get us close to the optimal solution. And while I am sure there could be multiple semesters taught on Dushant's local neighborhood search mechanisms, I would have enjoyed at least a little glimpse into that world. It also would have been interesting to look at the lagrangian relaxation -- I do not know much about lagrangian relaxations, except what I saw in 518, Intro to Integer Programming, but they seemed to be powerful tools to help solve integer programs with nasty constraints. I feel like I finally have a handle on degeneracy for the first time in my life, which is rather exciting. That said, I still would have enjoyed looking at different situations where we can exploit problem structure to avoid degeneracy, since I feel that is one of the more important skills I gained from this class. People have talked about Bender's decomposition, but I had never even heard of Bender's decomposition until this class. Dantzig-Wolfe decomposition blew my mind -- I hazard to guess what Bender's decomposition would have done to me.
And that, as they say, is all she wrote.
