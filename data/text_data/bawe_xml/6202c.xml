<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE TEI.2 SYSTEM "tei_bawe.dtd">
<TEI.2 id="_6202c" n="version 1.0">
<teiHeader>
<fileDesc>
<titleStmt>
<title>Parkinsonian Tremor Prediction using Neural Networks</title>
</titleStmt>
<extent/>
<publicationStmt>
<distributor>British Academic Written English (BAWE) corpus</distributor>
<availability>
<p>The British Academic Written English (BAWE) corpus was developed at the Universities of Warwick, Reading and Oxford Brookes, under the directorship of Hilary Nesi and Sheena Gardner (formerly of the Centre for Applied Linguistics [previously called CELTE], Warwick), Paul Thompson (Department of Applied Linguistics, Reading) and Paul Wickens (Westminster Institute of Education, Oxford Brookes), with funding from the ESRC. Subject to the rights of the these institutions in the BAWE corpus, and pursuant to the ESRC agreement, the BAWE corpus is available to researchers for research purposes PROVIDED THAT the following conditions are met:</p>
<p>1. The corpus files are not distributed in either their original form or in modified form.</p>
<p>2. The texts are used for research purposes only; they should not be reproduced in teaching materials.</p>
<p>3. The texts are not reproduced in full for a wider audience/readership, although researchers are free to quote short passages of text (up to 200 running words from any given text).</p>
<p>4. The BAWE corpus developers (contact: BAWE@warwick.ac.uk) are informed of all projects, dissertations, theses, presentations or publications arising from analysis of the corpus.</p>
<p>5. Researchers acknowledge their use of the corpus using the following form of words: "The data in this study come from the British Academic Written English (BAWE) corpus, which was developed at the Universities of Warwick, Reading and Oxford Brookes under the directorship of Hilary Nesi and Sheena Gardner (formerly of the Centre for Applied Linguistics [previously called CELTE], Warwick), Paul Thompson (Department of Applied Linguistics, Reading) and Paul Wickens (Westminster Institute of Education, Oxford Brookes), with funding from the ESRC (RES-000-23-0800)."</p>
</availability>
</publicationStmt>
<notesStmt>
<note resp="British Academic Written English (BAWE) corpus project">Page footer contains: page number. </note>
</notesStmt>
<sourceDesc>
<p n="level">4</p>
<p n="date">2007-03</p>
<p n="module title">Advanced Neural Networks</p>
<p n="module code">CYMN2</p>
<p n="genre family">Methodology recount</p>
<p n="discipline">Cybernetics &amp; Electronic Engineering</p>
<p n="disciplinary group">PS</p>
<p n="grade">M</p>
<p n="number of authors">1</p>
<p n="number of words">1807</p>
<p n="number of s-units">73</p>
<p n="number of p">27</p>
<p n="number of tables">1</p>
<p n="number of figures">7</p>
<p n="number of block quotes">0</p>
<p n="number of formulae">0</p>
<p n="number of lists">1</p>
<p n="number of paragraphs formatted like lists">12</p>
<p n="abstract present">abstract present</p>
<p n="average words per s-unit">24.8</p>
<p n="average s-units per p">2.7</p>
<p n="macrotype of assignment">simple assignment</p>
</sourceDesc>
</fileDesc>
<encodingDesc>
<p>TEI P4 (documented in: BAWE.documentation.pdf)</p>
</encodingDesc>
<profileDesc>
<particDesc>
<person>
<p n="gender">m</p>
<p n="year of birth">1983</p>
<p n="first language">Hindi</p>
<p n="education">OSa</p>
<p n="course">Cybernetics MSc</p>
<p n="student ID">6202</p>
</person>
</particDesc>
</profileDesc>
</teiHeader>
<text>
<front>
<titlePage>
<docTitle>
<titlePart>Parkinsonian Tremor Prediction using Neural Networks</titlePart>
</docTitle>
<titlePart>
<name type="student name"/> (<hi rend="underlined">
<name type="other"/>
</hi>) MSc Cybernetics 15022221</titlePart>
</titlePage>
<div1 type="front text">
<head rend="bold">Keywords: </head>
<p>Parkinson Disease (PD), Artificial Neural Networks, Back Propagation, Multi Layer Perceptron, Movement Disorder, Signal Processing</p>
</div1>
</front>
<body>
<div1 type="abstract">
<head rend="bold">Abstract</head>
<p n="p1.27">
<s rend="italic" n="s1.2;p1.27">In order to understand and closely predict abnormal spontaneous electrical activity in brains of patients suffering from movement disorders (PD in our case) this report shows how Artificial Neural Networks are employed to achieve this goal. </s>
<s rend="italic" n="s2.2;p1.27">In this assignment I have used MatLab's Neural Network Toolbox to extract relevant portions of data provided to us and work upon it by testing it on two different Artificial Neural Networks as required. </s>
</p>
</div1>
<div1 type="section">
<head rend="bold">Introduction </head>
<p n="p2.27">
<s n="s1.8;p2.27">PD is a degenerative disorder of the central nervous system that often impairs the sufferer's motor skills and speech. </s>
<s n="s2.8;p2.27">This takes place as a result of degeneration of neurons in a region of the brain that controls movement, causing shortage of brain signaling chemical <hi rend="italic">dopamine</hi>
<ref target="BAWE_6202c-ftnote.001"/>. </s>
<s n="s3.8;p2.27">Although formally recognized and documented in the early nineteenth century by British Physician James Parkinson<ref target="BAWE_6202c-ftnote.002"/>, The oldest description of it is found in the ancient Indian medical system of <hi rend="italic">Ayurveda</hi> under the name <hi rend="italic">Kampavata</hi> around 2500 BC. The first and most common symptom of PD is tremor (trembling or shaking) of a limb, especially when the body is at rest. </s>
<s n="s4.8;p2.27">Tremor often begins on one side of the body, frequently in one hand. </s>
<s n="s5.8;p2.27">Other symptoms include slow movement (<hi rend="italic">Bradykinesia</hi>), an inability to move (<hi rend="italic">Akinesia</hi>), rigid limbs, a shuffling gait and a stooped posture. </s>
<s n="s6.8;p2.27">People with PD often show reduced facial expressions and speak in a soft voice. </s>
<s n="s7.8;p2.27">The disease also causes depression, personality changes, dementia, sleep disturbances, speech impairments and sexual difficulties. </s>
<s n="s8.8;p2.27">Severity of PD related symptoms worsen over time. </s>
</p>
<note place="foot" id="BAWE_6202c-ftnote.001">
<p n="pn1.1">
<s n="s1.1;pn1.1">Dopamine is a Naturally produced Chemical in the body and functions as a neurotransmitter activating receptors helping us "command" our body. </s>
</p>
</note>
<note place="foot" id="BAWE_6202c-ftnote.002">
<p n="pn1.1">
<s n="s1.1;pn1.1">An Essay on the Shaking Palsy - JP, Royal College of Surgeons 1817. </s>
</p>
</note>
<p n="p3.27">
<s n="s1.2;p3.27">There are many theories about the causes of PD ranging from Environmental factors to Inherited susceptibility. </s>
<s n="s2.2;p3.27">There is no test that can clearly identify the disease Although Tests such as brain scans, can help doctors decide if a patient has true Parkinson's Disease or some other disorder that resembles it . </s>
</p>
<p n="p4.27">
<s n="s1.1;p4.27">Some experts also believe that PD is something of an "<hi rend="italic">iceberg; phenomenon</hi>" lurking in as many as 20 people for each known to have PD. There is no cure for PD and hence can only be controlled (partially). </s>
</p>
</div1>
<div1 type="section">
<head rend="bold">Background</head>
<p n="p5.27">
<s n="s1.1;p5.27">In this assignment we have been provided with a data file (from hereon referred to as <hi rend="bold">dat</hi>) containing actual statistics (split into two columns for convenience)from a patient with the </s>
</p>
<list type="bulleted">
<item>First column being the nucleus local field potentials(<hi rend="bold">LFP</hi>) recorded from the <hi rend="italic">Subthalamus</hi>(the ventral part of the thalamus regulating movements produced by skeletal muscles) &amp; the</item>
<item>Second column being envelop signal of the <hi rend="bold">EMG</hi> (<hi rend="italic">ElectroMyoGram</hi> recording physiologic properties of muscles at rest and while contracting).</item>
</list>
<p n="p6.27">
<s n="s1.3;p6.27">
<hi rend="bold">dat</hi> is recorded over 100 seconds (as it has a frequency of 800 Hz and has a total of 80000 observations) with both variable having a diverse range of amplitudes . </s>
<s n="s2.3;p6.27">In other words the way we load any data into a Neural Network would be dependent on how we would want to Interpret it as . </s>
<s n="s3.3;p6.27">This will not only help us in getting a good response but will also aid our understanding of the whole procedure . </s>
</p>
<figure id="BAWE_6202c-fig.001">
<head>Figure 1 Raw Data</head>
</figure>
</div1>
<div1 type="section">
<head rend="bold">Aim</head>
<p n="p7.27">
<s n="s1.2;p7.27">Our Purpose here is to devise a neural network such that when trained with a certain occurrence(s) of input LFP with target EMG ,it is able to correctly predict EMG if provided with just the LFP when running. </s>
<s n="s2.2;p7.27">In other words we need to model an autonomous system in which only the input LFP is enough to predict when an EMG change will occur so that a countering action is generated to reduce the amplitude of EMG so that a large tremor doesn't take place. </s>
</p>
</div1>
<div1 type="section">
<head rend="bold">Procedure</head>
<p n="p8.27">
<s n="s1.3;p8.27">There are several ways by which we can bring the participating data up-to a consistent standard that is easy to manipulate , recognizable and is comparatively smaller. </s>
<s n="s2.3;p8.27">The best way that would suit this purpose initially is to create a mean value for each participant for every certain number of occurrences. </s>
<s n="s3.3;p8.27">The main factors needs to be taken into consideration are </s>
</p>
<p rend="bulleted" n="p9.27">
<s n="s1.3;p9.27">That it acknowledges that LFP has both positive and negative value and hence there is a strong possibility that means at some interval would be 0 leading to inconsistencies . </s>
<s n="s2.3;p9.27">(That is a reason we would have to use a variance (σ<hi rend="sup">2</hi>) based approach for LFP . </s>
<s n="s3.3;p9.27">This ensures that there is a "visible accentuated difference" between the two participant .) </s>
</p>
<p rend="bulleted" n="p10.27">
<s n="s1.2;p10.27">EMG (green line in Fig 2) is something which would suit us best if it is 'fuzzified' . </s>
<s n="s2.2;p10.27">In other words, rather that having an intensity based EMG model , It is far more simpler to have and implement it based on an 'on-off' approach </s>
</p>
<p rend="bulleted" n="p11.27">
<s n="s1.1;p11.27">There is a tiny 'blip' in our observation at the beginning where our EMG slightly increases at the 15 second mark (around the 13000th observation).This means that it would have to be appropriately thresholded . </s>
</p>
<p n="p12.27">
<s n="s1.3;p12.27">We then plot the simplest way in which the data can be plotted with a total of 80,000/800 = 100 observations . </s>
<s n="s2.3;p12.27">Visually we are able to observe in this case that as LFP goes down EMG activates. </s>
<s n="s3.3;p12.27">Our main focus now is to have an appropriate approach to realize a solution . </s>
</p>
<figure id="BAWE_6202c-fig.002">
<head>Fig 2 Normalised Data</head>
</figure>
</div1>
<div1 type="section">
<head rend="bold">Approach</head>
<p n="p13.27">
<s n="s1.7;p13.27">Artificial Neural Networks can be summarized as <hi rend="italic">connectionist paradigm</hi> based programming on interconnecting neurons (or information processing units) in order to arrive at an overall response . </s>
<s n="s2.7;p13.27">An ANN is usually designed to change its structure based on external or internal information flowing through it to get a 'desirable' output . </s>
<s n="s3.7;p13.27">Understanding ways in which ANNs work is beyond the scope of this assignment (as the report assumes reader to have prior knowledge) and the report focusses on how accurately is a tremor predicted . </s>
<s n="s4.7;p13.27">Normalised <hi rend="bold">dat</hi> is split into a ratio of 1:1:1 (Fig 3)with each being for training , validation and testing . </s>
<s n="s5.7;p13.27">There are several ways in which the data can be split for this purpose but the reason I employed this approach was because the initial blip is capable of <hi rend="italic">training</hi> the network well so that it is adjusted according to its error also the <hi rend="italic">validation</hi> bit is used to measure network generalisation , and to halt training when generalisation stops improving . </s>
<s n="s6.7;p13.27">
<hi rend="italic">Testing</hi> has no effect on training and so provide an independent measure of network performance during and after training . </s>
<s n="s7.7;p13.27">The intention is to have final plots with the prediction for all the three portions (even if they have been trained with). </s>
</p>
<figure id="BAWE_6202c-fig.003">
<head>Fig 3. Normalised data splitted into three relevant portions (ratio = 1:1:1)</head>
</figure>
</div1>
<div1 type="section">
<head rend="bold">Conventional Back Propagation</head>
<p n="p14.27">
<s n="s1.10;p14.27">Back propagation is a somewhat ad hoc process. </s>
<s n="s2.10;p14.27">The initial step is to check that the output is correct. </s>
<s n="s3.10;p14.27">If it is not correct, an algorithm goes back and modifies the weights of the inputs to the output layer, such that a correct response would have been received. </s>
<s n="s4.10;p14.27">These inputs are, in turn, outputs of a middle layer, and the weighted inputs of the middle layer must now also change to reflect the new output. </s>
<s n="s5.10;p14.27">This process continues successively until input weights in each of the layers have changed. </s>
<s n="s6.10;p14.27">There are essentially two passes through a neural network. </s>
<s n="s7.10;p14.27">This process continues until the input weights no longer change. </s>
<s n="s8.10;p14.27">Back propagation is a common technique; however this is not a method that biological systems utilize . </s>
<s n="s9.10;p14.27">The rough figure shows a simple BPN Feed-forward (with one hidden layer trained with gradient descent Backpropagation) used for this purpose . </s>
<s n="s10.10;p14.27">Learning mode used was <hi rend="italic"> Gradient descent with momentum weight and bias learning function</hi>. </s>
</p>
<p n="p15.27">
<s n="s1.2;p15.27">Fig 4 Shows that the Prediction(shown in blue) and the original value (red dash dot lines) are close . </s>
<s n="s2.2;p15.27">The arrows marked on the Fig show that : </s>
</p>
<p rend="ordered" n="p16.27">
<s n="s1.1;p16.27">i) Although not visible (due to printing constraints) the Prediction is very close to the desired target value . </s>
</p>
<p rend="ordered" n="p17.27">
<s n="s1.1;p17.27">ii) The twin peaks are a result of momentum and are a common problem when using simple BPNs . </s>
</p>
<p rend="ordered" n="p18.27">
<s n="s1.1;p18.27">iii) Due to no limits being present the momentum causes the plot to go down into a negative area . </s>
</p>
<p rend="ordered" n="p19.27">
<s n="s1.1;p19.27">iv) &amp; v) are due to <hi rend="italic">overfitting</hi> . </s>
</p>
<figure id="BAWE_6202c-fig.004">
<head>Fig 4. Prediction from Back Propagation Network</head>
</figure>
<figure id="BAWE_6202c-fig.005"/>
</div1>
<div1 type="section">
<head rend="bold">Multilayer Perceptron</head>
<p n="p20.27">
<s n="s1.10;p20.27">A Multilayer Perceptron can be considered as a advanced Feedforward Backpropagation model but this is where the similarity ends as a MLP has high capacity of input-output mapping . </s>
<s n="s2.10;p20.27">In addition to this MLPs can far outperform other neural networks (such as RBF) and also requires smaller number of parameters in case of nonlinear input-output mapping for the same degree of accuracy. </s>
<s n="s3.10;p20.27">As a consequence MLPs tend to be far more accurate than other complex ANNs (although there is a tradeoff as far as training time concerned as MLPs take the longest to train) . </s>
<s n="s4.10;p20.27">Architecture of MLPs are not completely constrained by the problem in focus.This is because the number of inputs nodes is constrained by the problem and the number of neurons in the output layer is constrained by the number of outputs which is required by the problem, whereas the number of hidden layers and the number of neurons in the hidden layers depend on designer . </s>
<s n="s5.10;p20.27">As we know that there will be one input node in the input layer one output neuron corresponding to a target vector . </s>
<s n="s6.10;p20.27">There isn't a set procedure to arrive at a suitable number of hidden layers but after experimenting this was set at two . </s>
<s n="s7.10;p20.27">The activation function selected for this purpose is a logistic activation function . </s>
<s n="s8.10;p20.27">A modified conjugate gradient algorithm based on the Levenberg-Marquart approach is utilized as the preferred learning algorithm . </s>
<s n="s9.10;p20.27">Fig 5 shows the result observed in this case . </s>
<s n="s10.10;p20.27">In this two interesting observations are made as (i) is very close to the predicted error but (ii) makes for an erratic viewing this is a trend we observed in the previous ANN. The peaks are generated due to overfitting . </s>
</p>
<figure id="BAWE_6202c-fig.006">
<head>Fig 5. Prediction from MLP</head>
</figure>
</div1>
<div1 type="section">
<head rend="bold">Comparison</head>
<p n="p21.27">
<s n="s1.2;p21.27">When comparing the errors in the networks (Fig 6.) we get an expected result . </s>
<s n="s2.2;p21.27">An MLP implementation of the problem is roughly 22 times more accurate than a conventional BPN one . </s>
</p>
<table id="BAWE_6202c-tab.001">
<head rend="italic">Table 1. Showing the MSE between the target value and predicted value. </head>
<row>
<cell/>
</row>
</table>
</div1>
<div1 type="section">
<head rend="bold">Conclusion</head>
<p n="p22.27">
<s n="s1.1;p22.27">The conclusion can be summarized in the following points : </s>
</p>
<p rend="bulleted" n="p23.27">
<s n="s1.2;p23.27">A lot actually depends on the way we manipulate and work with the given data . </s>
<s n="s2.2;p23.27">This means what we decide to partition and where has a major impact on the overall decision (in this case at least). </s>
</p>
<p rend="bulleted" n="p24.27">
<s n="s1.1;p24.27">The solution discussed in this report is highly context specific and cannot be generalized whatsoever unless we have a training data from either several patients or different plots from the same patient. </s>
</p>
<figure id="BAWE_6202c-fig.007">
<head>Fig 6. Comparison of Errors</head>
</figure>
<p rend="bulleted" n="p25.27">
<s n="s1.1;p25.27">ANNs at this stage are not very reliable , In addition to that any response needs to be thresholded properly . </s>
</p>
<p rend="bulleted" n="p26.27">
<s n="s1.1;p26.27">Although we got a MLP to work relatively well , Its use in a real situation depends upon several factors such as criticality of the system or whether an ANN is used as the sole method used for prediction or an aid . </s>
</p>
<p rend="bulleted" n="p27.27">
<s n="s1.1;p27.27">There is a strong possibility that such a system can be put into use in the future as it is fairly reliable at the moment . </s>
</p>
</div1>
</body>
<back>
<div1 type="bibliography">
<head rend="bold">Reference</head>
<p>M.N. Gasson, S.Y. Wang, T.Z. Aziz, J.F. Stein, K Warwick, 'Towards a Demand Driven Deep-Brain Stimulator for the Treatment of Movement Disorders', MASP2005, 3rd IEE International Seminar on Medical Applications of Signal Processing, London, UK, pp.83-86, Nov. 2005. </p>
<p>S.S. Haykin, 'Neural networks: a comprehensive foundation', Upper Saddle River, N.J.: Prentice Hall, 1998.</p>
<p>K. Papik, B. Molnar, R. Schaefer, Z. Dombovari, Z. Tulassay and J. Feher, 'Application of neural networks in medicine-a review', Med. Sci. Monit., 4(3), pp. 538-546, 1998. </p>
<p> R. Nayak, L.C. Jain and B.K.H. Ting, 'Artificial neural networks in biomedical engineering:A review', In Proc. Asia-Pacific Conference on Advance Computation, 2001. </p>
<p>G. Zhang, B.H. Patuwo and M.Y. Hu, 'Forecasting with artificial neural networks: The state of the art', International Journal of Forecasting, 14, pp. 35-62, 1998. </p>
<p>L. Tarassenko, 'A guide to neural computing applications', London: Arnold, 1998 </p>
</div1>
</back>
</text>
</TEI.2>
