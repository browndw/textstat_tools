In my first paper for Complex Systems this term, I discussed Kauffman's theory of autocatalytic sets, which proposes, among other things, that given enough different pre-organic molecules floating around in the puddles and ponds of the early earth, it was nearly certain that life would emerge from the brew. By strength of numbers, even the limited catalytic potential of unevolved peptides or polynucleotides would eventually pass a collective critical threshold and produce a self replicating system.i In this framework, each and every molecule in the system is a necessary cog -- at the outset, the loss of any one produces a catastrophic failure. As time went on, this set of unique and individually critical components could evolve in response to changes in the environment and, over time, perhaps produce life as we know it. Life as we know it, however, does not consist of a system of completely interconnected, equally indispensable parts. Rather, organisms, especially multicellular organisms, consist of rough hierarchies, wherein components at a given level interact more often and more strongly with each other than with components at higher or lower levels in the hierarchy.ii This concept, termed near decomposability by Herbert Simon, but more often called modularity in the biological literature, can be shown to contribute to the evolvability of complex systems. In turn, the prevalence of modularity itself can be explained by evolution.iii Furthermore, an explanation of how modularity can arise in the context of genotype-phenotype mapping leads to the prediction of two other commonly observed properties of biological systems that are not well accounted for by the Darwinian mechanisms of random heritable mutation and natural selection: namely, punctuation and irreversibility.iv
As mentioned above, near decomposability (ND) or modularity is a property of systems which contain hierarchies of components, and where the components at one level of hierarchy interact more strongly with each other than with components at other levels. ND does not in any way prohibit interactions across levels of the hierarchy, but asserts that these interactions will be either less frequent or weaker than interactions among components at the same level. This idea of ND is sensitive to time scale. This is shown in the metaphor prosed by Simon of a building with several large rooms, each of which is divided up into cubicles. If some disturbance produces large temperature variation between each of the rooms and each of the cubicles, and between the inside of the building and the outside, the temperature of all the cubicles in a room will fairly rapidly return to equilibrium, relatively unaffected by the temperatures in other room or outside the building. The cubicles are thus analogous to components at a particular level of hierarchy, interacting more strongly with each other than components at other levels. However, at a longer time scale, the rooms of the building will come to an equilibrium temperature despite the thicker walls between them, and this will, of course affect the temperature of the cubicles in those rooms. Eventually, the entire building will come into equilibrium with the outdoor temperature. Nonetheless, in the short run, the effect on the cubicles of other rooms can be ignored. This concept of modularity implies that components at a given level can be studied by scientists without taking into account the minor effects of components at other levels, at least on certain time scales, which is certainly reassuring, given the amount of effort that has been put into reductionist science over the centuries. Simon claims that the ND property of biological systems also has certain implications for evolution. Next, I will discuss several of the specific claims he makes.
Simon first proposes that:
This assertion relies on the idea that the independent elements, once combined, remain at least mostly independent. However, it seems entirely plausible that in evolving biological systems, the elements, once combined, may form close interdependencies that destroy the original modularity. The question of how modularity arises and how it evolves will be addressed in more detail later, but I will provide one example here of an ambiguous modularity. The theory of endosymbiosis is well known and well supported -- it proposes that the origin of the mitochondria and chloroplasts found within eukaryotic cells originated as free living prokaryotes, which formed an endosymbiotic relationship with their host. Presumably, at first, these new residents in the eukaryotic cell were quite independent of the cell they inhabited, but stayed due to some advantages provided -- perhaps protection, or access to nutrients. In turn, the ancestor of today's eukaryotes must have been a fairly successful creature, but found that hosting the proto-mitochondria provided an advantage in energy production, and the photosynthetic chloroplast captured the energy of the sun and yielded sugars that the host could burn, store, or convert to structural components. Over evolutionary time, however, both organelles lost most of their genomes to the host nuclear genome and are no longer capable of even semi-autonomous existence. On the other side, even small errors in mitochondrial processes can produce devastating effects of disease in humans and other modern day eukaryotes. It is clear both that these components were once independent, and that they no longer can be described as such in any meaningful sense. Depending on the rate at which independent components become so interdependent as to no longer merit their "ND" status, the prevalence of ND systems observed may not be as high as Simon suggests.
To illustrate the relevance of the ND property to evolution, Simon introduces the parable of two watchmakers.v Hora and Tempus both make fine watches, but their methods of manufacture differ extensively. Hora creates each watch as a combination of ten stable sub-assemblies, each of which, in turn, consists of ten sub-sub-assemblies of ten individual parts each. Thus each watch has a total of 1,000 parts. Tempus's watches also contain 1,000 parts, but there are no sub-assemblies -- each watch must be assembled start to finish, and if the process of assembly is interrupted before the final piece is put into place, the entire watch falls apart. This is somewhat like Kauffman's autocatalytic sets -- all the pieces must be in place to function at all. Importantly, each of Hora's sub-assemblies and sub-sub-assemblies, once put together, can be safely set aside until it is later incorporated into the next level of the watch-making process. Here's the problem: since each of these manufacturers produces such fine timepieces, their phones ring constantly with new orders, which interrupts the construction. For Hora, this is a minor inconvenience -- on average, perhaps 5 pieces of assembly are lost to each new order. But for Tempus, if the interruption comes just as the 990th piece is put into place, the loss is much greater.
It is clear that Hora will be able to assemble more watches than will Tempus in a given time with a comparable number of interruptions. Simon concludes from this that "watches built as ND structures will be fitter than those built on other architectures and will win out in evolutionary competition." (Simon, 200, p 591) However, in this passage he is discussing the origin of complex systems by evolution, not their subsequent reproductive capacities. It may be correct to then infer that biological systems can arise more quickly by employing ND architectures than otherwise, but this in no way implies that their fitness once they have arisen will be any better than those with other architectures. It is just as easy to imagine that Tempus's watches are better than Hora's as it is to conceive of the reverse. We have said nothing (so far) about the inherent fitness of ND architectures in a particular fitness landscape. I will discuss later work by Wagner and Altenberg that implies modularity is a phenomenon that applies more to the subsequent evolution of biological systems than to their origin. At this point however, I will offer a few examples that might stabilize the non-ND modes of evolution. Just as Tempus might hire an answering service, so biological systems often arrive by chance or by evolution at mechanisms to avoid interruptions of growth and of evolution. For the origin of life, enclosure in a lipid membrane or a small cavity in a rock or some other sheltering space may have provided the time necessary for molecules to interact without being swept away from each other in the currents of the ocean. For the origin of new species, an island often serves as an incubator, away from the pressures of competition on the mainland. And, in the reproductive processes of existing species, gestation often occurs in a protected space, be it the pouch of a daddy seahorse or a bird's egg snug in its nest with mama bird sitting on top ready to bring worms as soon as baby hatches. All of these examples show that mechanisms do exist which protect processes that, like the manufacture of Tempus's watches, must be undertaken from start to finish without interruption.
Simon also claims that existing ND systems will evolve faster than ND systems of comparable complexity. Here he uses that parable of a clicking safe. If a safe has 10 dials, each with 100 possible settings, the problem of finding the correct combination of settings on each dial is nearly impossible. If, on the other hand, each dial clicks when the right setting is found, the problem of opening the safe is relatively easy. However, if the correct setting for any one dial depends on the settings of the other nine -- that is, if you find a correct setting, but then change one of the other dials, the setting you found is not correct anymore -- then it is no easier to open that safe than one that doesn't click at all. The safe that's easy to open is like an ND organism, that can find the best solution for each of its ten modular parts (or genes), without dependence on the states of the other modules. On the other hand, the clicking safe where the correct setting of each dial depends on the settings of all the others, is a completely interdependent architecture. A change in any one of the dials perturbs the other parts of the system and removes them from whatever local optima they had previously attained. Simon states that ND systems are able to improve their fitness quickly, and although they may not attain global maximum fitness, they can out-compete non-ND systems that have excessive interdependence among their parts. This makes sense, given that no evolutionary algorithm guarantees global maxima, and that evolution frequently takes the form of an "arms race" where one species must constantly evolve new defenses or new attacks to keep up with the changes in its predators or its prey. It is not the globally optima that win these races, but rather satificing strategies that aim for immediate improvement. The truism that "possession is nine-tenths of the law" also applies here -- if a species has occupied a niche with a good enough strategy, it will be more difficult for any other species to come along, even with a more optimally evolved fitness, and oust the already resident species from its niche. So, from the point of virew that ND architecture speeds up evolution, it does seem to be an asset to biological systems possessed of such an architecture.
However, there are inherent trade-offs to ND architecture as well. Egidi and Merengovi discuss the sub-optimality of ND architectures in the context of decision making, but their analysis is equally applicable to biological evolution. While decomposition may result in more manageable problems, to which solutions can be more rapidly found by evolution, it also imposes a sort of overhead, whereby the parts must be coordinated in order to produce a coherent whole. This coordination of the parts may take the form of inputs and outputs. In order to not disrupt the whole system, the input and output of any one component must be constrained within a given range. Simon suggests that this can be achieved by a process analogous to top-down computer programming by focusing on the I/O and not "micro-managing" the detailed internal structure of the modules. In this way, he suggests, efficiencies can be made at the intra-modular level without disturbing the balance of the system. In general, this may be true, however, if the word "efficiency" is interpreted to mean the rate at which inputs are changed to output, say in metabolism, then changes in the efficiency of a part may increase the rate of output in time, or may increase the output quantitatively if, for example, waste in the internal process is reduced. Therefore, simply restricting the module-level changes to efficiencies may not have the desired result of maintaining stable I/O.
Although the concept of modularity is not without its problems, it does seem to exist quite commonly in biological systems. So, we may ask if it is an intrinsic property of life as we know it, or if it is something that evolved later. This question is addressed by Wagner and Altenberg in some detail. It has been observed that taxa with more independently variable traits are found more recently in the fossil record. That is to say, taxa that have more modular characters evolved more recently, so modularity was an evolved property. Furthermore, Wagner and Altenberg distinguish between two modes by which modularity might arise: by parcellation, where characters that were previously highly interdependent become less so, and integration, where characters that were once independent become somewhat independent, thus forming modules. Although integration clearly occurs, as in endosymbiosis, since the fossil reocrd shows more variation as time goes on, rather than less, it seems parcellation is the dominant force.
There are several plausible ways in which evolution might promote this process of increasing modularity. First, similar to the clicking safe, when one character is free in constraints placed on it by other characters, it can more easily find optimal states. This can occur when developmental constraints are broken, leaving individual modules free to vary and find better optima than would be possible if every change in one developmental process impacted all other such processes. Second, if a given process depends on fewer others, say it depends on 10 genes rather than 100, then it may take fewer mutations to make improvements, leading to faster response to environmental conditions. Finally, and most convincingly in my opinion, the known process of gene duplication often results in the decay of one copy of the gene to a pseudogene with no effect on the fitness of the organism. The decay into a pseudogene will likely occur if the extra copy has no effect (neutral), or if its effect is deleterious. A gene that is tightly linked with many others is more likely to produce a deleterious effect on at least one of those others. A gene that is tightly controlled by many others is more likely to have no effect at all. Therefore, those genes that are only linked to a few others (say in a module) is most likely to be fixed in the genome, where it can then mutate and acquire new functions. Thus, genomes grow the parts that are modular, whereas the highly interconnected genes may remain more stagnant.
It is clear that the evolution of modularity is at least plausible. What types of modules should we expect to see? This question is addressed in part by Stadler, Stadler, Wagner and Fontana in their review of the possible patterns that evolution can produce. Phenotypes are produced by the action of the genotype in a given environment. It is often assumed that any random mutation in the genotype can produce and equivalent change in the phenotype, but in fact, some changes in genotype are silent -- producing no change in phenotype, whereas some (likely rare) mutation can produce large jumps in phenotypic space. Thus, certain phenotypes are relatively stable to mutation, but nonetheless, we may expect to sometimes see big changes in a short period of time. Furthermore, some phenotypes may be "closer" together in mutational space than others, and these distances are not necessarily symmetrical. For example, it may be an easy thing to disrupt a delicate protein structure by a single point mutation, but more difficult to by chance arrive at the single mutation that, from a precursor state -- will build that same delicate structure back again. Taken together, these properties of genotype-phenotype mapping predict stable modules, as well as punctuated equilibria and irreversibility in evolution.
Although my discussion here has been focused on biological systems, the concept of near decomposability of modularity is certainly applicable more broadly in the study of complex systems. In physics, the hierarchical structures of particles and atoms and molecules exhibit the same sort of interactions that are strong at the same hierarchical level and weaker between levels. In business and economics, similar forces are at work. The name "near" decomposability hints at the limitation of the concept -- the modules in any of these systems are not completely independent, and the complexity of the systems arises in part of the interactions of the modules at different levels. Nonetheless, analysis of complex systems in terms of modules is likely to be a profitable approach as it can simplify some problems and make modeling or experimentation more practical.
