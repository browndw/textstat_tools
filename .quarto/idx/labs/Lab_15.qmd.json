{"title":"Lab 15","markdown":{"yaml":{"title":"Lab 15","author":"My Name","output":{"pdf_document":{"fig_caption":"yes","number_sections":true}},"header-includes":[""]},"headingText":"Lab 15: Geospacial Mapping: POP? SODA? or COKE?","headingAttr":{"id":"","classes":["unnumbered"],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n```{r, include=FALSE}\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#>\",\n  tidy.opts=list(width.cutoff=70),\n  tidy=TRUE\n)\n```\n\n\n\nIn 2013 a statistics (!) grad student at NC State named Joshua Katz put up a series of maps on an R Studio server that plotted the geographic distributions of lexical variants across the US. These turned out to be wildly popular and he published versions at Business Insider:\n\n<https://www.businessinsider.com/american-english-dialects-maps-2018-1>\n\nAnd at the New York Times, where he now works:\n\n<https://www.nytimes.com/interactive/2014/upshot/dialect-quiz-map.html>\n\nHis maps are based on data collect by Bert Vaux and Scott Golder for the Harvard Dialect Survey in 2003:\n\n<http://www4.uwm.edu/FLL/linguistics/dialect/index.html>\n\nKatz's work is an interesting application of KNN to geospatial data. There is a nice introduction to the KNN algorithm here:\n\n<https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/>\n\nWe're going to re-create Katz's pop vs. soda plot. This exercise requires a number of packages.\n\n\n```{r setup, message = FALSE, error=FALSE, warning=FALSE}\n# A couple for data wrangling...\nlibrary(tidyverse)\nlibrary(data.table)\n\n# Our plots will be based on the results of knn...\nlibrary(kknn) # for knn\n\n# We have some relatively intensive processing tasks that can be sped up... \nlibrary(foreach) # for iterating over elements in a collection\nlibrary(doParallel) # for parallel processing\n\n# And we'll be working with spatial data, so we need some specialized packages...\nlibrary(sf) # for spatial data\nlibrary(rnaturalearth) # map data for plotting\nlibrary(housingData) # geolocation data\nrequire(rgeos)\n```\n\n# Prepare the Data\n\n## Load the pop vs. soda data\n\nThe data can be downloaded from the **Data** folder on Canvas.\n\n```{r warning=FALSE, message=FALSE}\npvs_df <- read_csv(\"../data/pop_vs_soda.csv\", show_col_types = F)\n```\n\nNote how the data is structures.\n\n```{r echo=FALSE}\npvs_df[,1:10] %>% head(10) %>% knitr::kable(format=\"latex\", booktabs = TRUE) %>%\n  kableExtra::kable_styling(latex_options=c(\"scale_down\", \"HOLD_position\"))\n```\n\n[...]\n\n```{r echo=FALSE}\npvs_df[,11:20] %>% head(10) %>%  knitr::kable(format=\"latex\", booktabs = TRUE) %>%\n  kableExtra::kable_styling(latex_options=c(\"scale_down\", \"HOLD_position\"))\n```\n\n## Convert FIPS codes to longitude and latitude\n\nWe have counts by state and county. Importantly, those counts are identified by Federal Information Processing System (FIPS) codes. To convert that information to something mappable, we need GIS data for those FIPS codes. For that we can use data from the housingData package.\n\n```{r}\nhd <- housingData::geoCounty\nhd$fips <- as.character(hd$fips)\n```\n\nNow let's select the columns we need from our data, and put that data in a long format.\n\n```{r}\npvs_df <- pvs_df %>% select(fips = FIPS_combo, POP = SUMPOP, SODA = SUMSODA, \n                       COKE = SUMCOKE, OTHER = SUMOTHER)\n\npvs_df <- pvs_df %>% \n  pivot_longer(!fips, names_to = \"var\", values_to = \"freq\") %>% \n  filter(freq != 0) \n```\n\n```{r echo=FALSE}\npvs_df %>% arrange(fips) %>% head(10) %>% knitr::kable(booktabs = TRUE) %>%\n  kableExtra::kable_styling(latex_options=\"HOLD_position\")\n```\n\nFor the task, we need to convert counts into instances. For example, if there are 3 *soda* users in a county, we want\n*soda*, *soda*, *soda* in the variable column. For that, we can use the data.table package.\n\n```{r}\nsetDT(pvs_df)\npvs_df <- pvs_df[rep(seq(.N), freq), !\"freq\"]\n```\n\n```{r echo=FALSE}\npvs_df %>% arrange(fips) %>% head(10) %>% knitr::kable(booktabs = TRUE) %>%\n  kableExtra::kable_styling(latex_options=\"HOLD_position\")\n```\n\nNow let's join this with the GIS data to add longitudes and latitudes, and drop any empty rows.\n\n```{r}\npvs_df <- left_join(pvs_df, select(hd, fips, lon, lat), by = \"fips\")\npvs_df <- pvs_df[!is.na(pvs_df$lon), ]\n```\n\n```{r echo=FALSE}\npvs_df %>% arrange(fips) %>% head(10) %>% knitr::kable(booktabs = TRUE) %>%\n  kableExtra::kable_styling(latex_options=\"HOLD_position\")\n```\n\n## Prepare a map\n\nNow let's get the information we need for our map. We'll start with a map of the US states.\n\n```{r}\nstates_us <- rnaturalearthdata::states50\n```\n\nThere are several map options. We're going to select *iso_a2*. We'll also drop Alaska and Hawaii, so we'll only plot the Continental US.\n\n```{r}\nstates_us <- states_us[states_us$iso_a2 == 'US',]\nstates_us <- states_us[ !grepl( \"Alaska|Hawaii\" , states_us$name ) , ]\n```\n\nFinally we want to convert this into an sf object for later plotting. This creates a *simple features* object. For more about *simple features* see here:\n\n<https://r-spatial.github.io/sf/articles/sf1.html>\n\n```{r}\nstates_us <- st_as_sf(states_us)\n```\n\n## Transform the data for plotting\n\nNow we'll similarly convert the coordinates in our pop vs. soda data. Note that we point the *st_as_sf()* function to our \"lon\" and \"lat\" columns. Additionally, we need to specify the coordinate reference system (crs). This can be tricky... See here for a discussion:\n\n<https://ryanpeek.github.io/2017-08-03-converting-XY-data-with-sf-package/>\n\nWe're specifying The World Geodetic System 1984 or WGS84.\n\n```{r}\npoint_data <- pvs_df %>% st_as_sf(coords = c(\"lon\", \"lat\"), crs = \"+proj=longlat +ellps=WGS84\")\n```\n\nFinally, we need to specify a projection. Remember we are \"projecting\" geographical space into 2 dimensions. So we have to decide what convention to use. We'll use the North America Lambert Conformal Conic projection:\n\n<https://en.wikipedia.org/wiki/Lambert_conformal_conic_projection>\n\n```{r}\nnalcc <- \"+proj=lcc +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs\"\n```\n\nNow, we can transform our US data to fit this projection. And likewise our pop vs. soda data.\n\n```{r}\nus <- states_us %>% st_transform(nalcc)\npoint_data <- point_data %>% st_transform(nalcc)\n```\n\nNext, we'll make a data.frame with 3 columns. And we'll convert our variable to a factor.\n\n```{r}\ndialects_train <- data.frame(dialect = point_data$var, \n                             lon = st_coordinates(point_data)[, 1], \n                             lat = st_coordinates(point_data)[, 2]) %>%\n  mutate(dialect = as.factor(dialect))\n```\n\n```{r echo=FALSE}\ndialects_train %>% head(10) %>% knitr::kable(booktabs = TRUE) %>%\n  kableExtra::kable_styling(latex_options=\"HOLD_position\")\n```\n\nTo get a sense of our data, we can take a random sample from our training data and plot it.\n\n```{r warning=FALSE, message=FALSE, fig.height=4, fig.width=7}\ndialects_sample <- dialects_train %>% sample_n(1000)\n\nggplot(data = dialects_sample) +\n  geom_sf(data = us) +\n  geom_point(aes(x = lon, y = lat, color = dialect), alpha = 1) +\n  scale_color_brewer(palette = \"Dark2\",  name = \"Dialect\")\n```\n\n\n# KNN classification\n\n## Case study: Allegheny and Philadelphia counties\n\nThe implementation of the knn classifier is a little different from earlier classification problems, so let's walk through the basic idea with a quick example. First, let's subset out the data for 2 counties: Allegheny and Philadelphia counties (identified by their FIPS codes).\n\nThen, we'll construct a simple data frame with 3 columns.\n\n```{r}\ndialects_penn <- point_data[grepl(\"42003|42101\", point_data$fips), ]\n\ndialects_penn <- data.frame(dialect = dialects_penn$var, \n                            lon = st_coordinates(dialects_penn)[, 1], \n                            lat = st_coordinates(dialects_penn)[, 2]) %>%\n mutate(dialect = as.factor(dialect))\n```\n\nFrom that, we can split the data into a training and test set.\n\n```{r}\nset.seed(123)\nvalid_split <- rsample::initial_split(dialects_penn, .75)\n\npenn_train <- rsample::analysis(valid_split)\npenn_test <- rsample::assessment(valid_split)\n```\n\nFor more conventional classification tasks, we would want to determine the optimal k for our model using the **train.kknn()** function:\n\n<https://rpubs.com/bonibruno/svm-knn>\n\nFor our purposes, we'll use a quick rule of thumb and use the square-root of our *n* observations. Now we can call a model in which we try to predict the dialect variant (*soda*, *pop*, *coke*, *other*) based on geolocation.\n\n```{r}\nknn_penn <- kknn::kknn(dialect ~ ., \n                       train = penn_train, \n                       test = penn_test, \n                       kernel = \"gaussian\", \n                       k = 61)\n```\n\nFor conventional classification tasks, we would be interested in our confusion matrix, and our model's accuracy\n\n```{r}\ntab <- table(knn_penn$fitted.values, penn_test$dialect)\n\naccuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}\naccuracy(tab) %>% knitr::kable(booktabs = TRUE, digits = 2, col.names = \"Accuracy\")\n```\n\nFor our purposes, we're interested in the probabilities of each variant being preferred based on location. Allegheny county is at the top of our table, with a very strong preference for *pop*.\n\n```{r}\nknn_penn$prob %>% head(1) %>% knitr::kable(booktabs = TRUE, digits = 2)\n```\n\nAnd Philadelphia county is at the bottom of our table with a strong preference for *soda*.\n\n```{r}\nknn_penn$prob %>% tail(1) %>% knitr::kable(booktabs = TRUE, digits = 2)\n```\n\n## Spatial interpolation\n\nThose probabilities can be used to select both color (for the most probable variant) and the alpha of the color (based on the value of the probability).\n\nSounds easy enough, but we don't have values for the entire map. We need to use our known values to estimate those at other unknown points. This is process called spatial interpolation:\n\n<https://docs.qgis.org/2.18/en/docs/gentle_gis_introduction/spatial_analysis_interpolation.html>\n\nWhat follows in wholely indebted to Timo Grossenbacher.\n\n<https://timogrossenbacher.ch/2018/03/categorical-spatial-interpolation-with-r/>\n\nEssentially, we need to configure an empty grid and interpolate that grid with our point data. Another complication is that this process is quite memory and computationally intensive. So we're ultimately going to split up that grid into batches.\n\nWe'll begin setting up our grid by specifying its width in pixels.\n\n```{r}\nwidth_in_pixels <- 300\n```\n\nThe width of a grid cell is calculated from the dimensions of our US map and is defined as dx.\n\n```{r}\ndx <- ceiling( (st_bbox(us)[\"xmax\"] - st_bbox(us)[\"xmin\"]) / width_in_pixels)\n```\n\nThe height of a grid cell is the same because we'll use quadratic grid cells, **dx == dy**.\n\n```{r}\ndy <- dx\n```\n\nCalculate the height in pixels of the resulting grid.\n\n```{r}\nheight_in_pixels <- floor( (st_bbox(us)[\"ymax\"] - st_bbox(us)[\"ymin\"]) / dy)\n```\n\nAnd finally construct the grid with the **st_make_grid()** function.\n\n```{r}\ngrid <- st_make_grid(us, cellsize = dx, n = c(width_in_pixels, height_in_pixels), what = \"centers\")\n```\n\nNext, we'll set our k at 1000. This will result is a smoother looking map.\n\n```{r}\nk <- 1000\n```\n\nNow, we can define a **compute_grid()** function that returns the probability of the most likely variant.\n\n```{r}\ncompute_grid <- function(grid, dialects_train, knn) {\n  # create empty result data frame\n  dialects_result <- data.frame(dialect = as.factor(NA), \n                                lon = st_coordinates(grid)[, 1], \n                                lat = st_coordinates(grid)[, 2])\n  # run KKNN\n  dialects_kknn <- kknn::kknn(dialect ~ ., \n                              train = dialects_train, \n                              test = dialects_result, \n                              kernel = \"gaussian\", \n                              k = knn)\n  # bring back to result data frame\n  # only retain the probability of the dominant dialect at that grid cell\n  dialects_result <- dialects_result %>%\n    # extract the interpolated dialect at each grid cell with the \n    # kknn::fitted() function\n    mutate(dialect = fitted(dialects_kknn),\n           # only retain the probability of the interpolated dialect,\n           # discard the others\n           prob = apply(dialects_kknn$prob, 1, function(x) max(x)))\n  \n  return(dialects_result)\n}\n```\n\nWe can set the parameters for parallel and batch processing. Adjust cores and batch size accordingly.\n\n```{r}\nregisterDoParallel(cores = 4) # if you have more processing power you can up this number\n\n# Specify number of batches and resulting size of each batch (in grid cells).\nno_batches <- 60 # increase this number if you run into memory problems\nbatch_size <- ceiling(length(grid) / no_batches)\n```\n\nFinally, we'll generate our result using parallel processing. This will take at least a couple of minutes.\n\n```{r}\ndialects_result <- foreach(.packages = c(\"sf\", \"tidyverse\"),\n  batch_no = 1:no_batches, \n  # after each grid section is computed, rbind the resulting df into one big dialects_result df\n  .combine = rbind, \n  # the order of grid computation doesn't matter: this speeds it up even more\n  .inorder = FALSE) %dopar% {\n    # compute indices for each grid section, depending on batch_size and current batch\n    start_idx <- (batch_no - 1) * batch_size + 1\n    end_idx <- batch_no * batch_size\n    # specify which section of the grid to interpolate, using regular subsetting\n    grid_batch <- grid[start_idx:ifelse(end_idx > length(grid), \n                                        length(grid),\n                                        end_idx)]\n    # apply the actual computation to each grid section\n    df <- compute_grid(grid_batch, dialects_train, k)\n  }\n```\n\nWe'll now convert the result into an **sf** object for mapping.\n\n```{r}\ndialects_raster <- st_as_sf(dialects_result, \n                            coords = c(\"lon\", \"lat\"),\n                            crs = nalcc,\n                            remove = F)\n```\n\nRemember that we've calculated a rectangular grid. As a last step, we can use the outline of the US to mask the borders of our map.\n\n```{r}\ndialects_raster <- dialects_raster[us, ]\n```\n\nAnd plot the result... (Note that we're dropping *other*.)\n\n```{r warning=FALSE, message=FALSE, fig.height=4, fig.width=7}\nggplot(data = dialects_raster %>% filter(dialect != \"OTHER\")) +\n  geom_raster(aes(x = lon, y = lat, fill = dialect, alpha = prob)) +\n  scale_fill_manual(values = c(\"forestgreen\", \"steelblue\", \"tomato\")) +\n  scale_alpha(guide = 'none') +\n  geom_sf(data = us, alpha = 0, size = 0.25) +\n  theme_void() +\n  theme(legend.position = c(0.1, 0.2), legend.box = \"horizontal\") +\n  theme(legend.title=element_blank())\n```\n\nHow does our result compare to Katz's?\n\n<https://www.foodrepublic.com/2013/07/29/a-final-word-on-the-soda-vs-pop-vs-coke-nomenclature-debate/>\n\n","srcMarkdownNoYaml":"\n\n```{r, include=FALSE}\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#>\",\n  tidy.opts=list(width.cutoff=70),\n  tidy=TRUE\n)\n```\n\n# Lab 15: Geospacial Mapping: POP? SODA? or COKE? {-}\n\n\nIn 2013 a statistics (!) grad student at NC State named Joshua Katz put up a series of maps on an R Studio server that plotted the geographic distributions of lexical variants across the US. These turned out to be wildly popular and he published versions at Business Insider:\n\n<https://www.businessinsider.com/american-english-dialects-maps-2018-1>\n\nAnd at the New York Times, where he now works:\n\n<https://www.nytimes.com/interactive/2014/upshot/dialect-quiz-map.html>\n\nHis maps are based on data collect by Bert Vaux and Scott Golder for the Harvard Dialect Survey in 2003:\n\n<http://www4.uwm.edu/FLL/linguistics/dialect/index.html>\n\nKatz's work is an interesting application of KNN to geospatial data. There is a nice introduction to the KNN algorithm here:\n\n<https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/>\n\nWe're going to re-create Katz's pop vs. soda plot. This exercise requires a number of packages.\n\n\n```{r setup, message = FALSE, error=FALSE, warning=FALSE}\n# A couple for data wrangling...\nlibrary(tidyverse)\nlibrary(data.table)\n\n# Our plots will be based on the results of knn...\nlibrary(kknn) # for knn\n\n# We have some relatively intensive processing tasks that can be sped up... \nlibrary(foreach) # for iterating over elements in a collection\nlibrary(doParallel) # for parallel processing\n\n# And we'll be working with spatial data, so we need some specialized packages...\nlibrary(sf) # for spatial data\nlibrary(rnaturalearth) # map data for plotting\nlibrary(housingData) # geolocation data\nrequire(rgeos)\n```\n\n# Prepare the Data\n\n## Load the pop vs. soda data\n\nThe data can be downloaded from the **Data** folder on Canvas.\n\n```{r warning=FALSE, message=FALSE}\npvs_df <- read_csv(\"../data/pop_vs_soda.csv\", show_col_types = F)\n```\n\nNote how the data is structures.\n\n```{r echo=FALSE}\npvs_df[,1:10] %>% head(10) %>% knitr::kable(format=\"latex\", booktabs = TRUE) %>%\n  kableExtra::kable_styling(latex_options=c(\"scale_down\", \"HOLD_position\"))\n```\n\n[...]\n\n```{r echo=FALSE}\npvs_df[,11:20] %>% head(10) %>%  knitr::kable(format=\"latex\", booktabs = TRUE) %>%\n  kableExtra::kable_styling(latex_options=c(\"scale_down\", \"HOLD_position\"))\n```\n\n## Convert FIPS codes to longitude and latitude\n\nWe have counts by state and county. Importantly, those counts are identified by Federal Information Processing System (FIPS) codes. To convert that information to something mappable, we need GIS data for those FIPS codes. For that we can use data from the housingData package.\n\n```{r}\nhd <- housingData::geoCounty\nhd$fips <- as.character(hd$fips)\n```\n\nNow let's select the columns we need from our data, and put that data in a long format.\n\n```{r}\npvs_df <- pvs_df %>% select(fips = FIPS_combo, POP = SUMPOP, SODA = SUMSODA, \n                       COKE = SUMCOKE, OTHER = SUMOTHER)\n\npvs_df <- pvs_df %>% \n  pivot_longer(!fips, names_to = \"var\", values_to = \"freq\") %>% \n  filter(freq != 0) \n```\n\n```{r echo=FALSE}\npvs_df %>% arrange(fips) %>% head(10) %>% knitr::kable(booktabs = TRUE) %>%\n  kableExtra::kable_styling(latex_options=\"HOLD_position\")\n```\n\nFor the task, we need to convert counts into instances. For example, if there are 3 *soda* users in a county, we want\n*soda*, *soda*, *soda* in the variable column. For that, we can use the data.table package.\n\n```{r}\nsetDT(pvs_df)\npvs_df <- pvs_df[rep(seq(.N), freq), !\"freq\"]\n```\n\n```{r echo=FALSE}\npvs_df %>% arrange(fips) %>% head(10) %>% knitr::kable(booktabs = TRUE) %>%\n  kableExtra::kable_styling(latex_options=\"HOLD_position\")\n```\n\nNow let's join this with the GIS data to add longitudes and latitudes, and drop any empty rows.\n\n```{r}\npvs_df <- left_join(pvs_df, select(hd, fips, lon, lat), by = \"fips\")\npvs_df <- pvs_df[!is.na(pvs_df$lon), ]\n```\n\n```{r echo=FALSE}\npvs_df %>% arrange(fips) %>% head(10) %>% knitr::kable(booktabs = TRUE) %>%\n  kableExtra::kable_styling(latex_options=\"HOLD_position\")\n```\n\n## Prepare a map\n\nNow let's get the information we need for our map. We'll start with a map of the US states.\n\n```{r}\nstates_us <- rnaturalearthdata::states50\n```\n\nThere are several map options. We're going to select *iso_a2*. We'll also drop Alaska and Hawaii, so we'll only plot the Continental US.\n\n```{r}\nstates_us <- states_us[states_us$iso_a2 == 'US',]\nstates_us <- states_us[ !grepl( \"Alaska|Hawaii\" , states_us$name ) , ]\n```\n\nFinally we want to convert this into an sf object for later plotting. This creates a *simple features* object. For more about *simple features* see here:\n\n<https://r-spatial.github.io/sf/articles/sf1.html>\n\n```{r}\nstates_us <- st_as_sf(states_us)\n```\n\n## Transform the data for plotting\n\nNow we'll similarly convert the coordinates in our pop vs. soda data. Note that we point the *st_as_sf()* function to our \"lon\" and \"lat\" columns. Additionally, we need to specify the coordinate reference system (crs). This can be tricky... See here for a discussion:\n\n<https://ryanpeek.github.io/2017-08-03-converting-XY-data-with-sf-package/>\n\nWe're specifying The World Geodetic System 1984 or WGS84.\n\n```{r}\npoint_data <- pvs_df %>% st_as_sf(coords = c(\"lon\", \"lat\"), crs = \"+proj=longlat +ellps=WGS84\")\n```\n\nFinally, we need to specify a projection. Remember we are \"projecting\" geographical space into 2 dimensions. So we have to decide what convention to use. We'll use the North America Lambert Conformal Conic projection:\n\n<https://en.wikipedia.org/wiki/Lambert_conformal_conic_projection>\n\n```{r}\nnalcc <- \"+proj=lcc +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs\"\n```\n\nNow, we can transform our US data to fit this projection. And likewise our pop vs. soda data.\n\n```{r}\nus <- states_us %>% st_transform(nalcc)\npoint_data <- point_data %>% st_transform(nalcc)\n```\n\nNext, we'll make a data.frame with 3 columns. And we'll convert our variable to a factor.\n\n```{r}\ndialects_train <- data.frame(dialect = point_data$var, \n                             lon = st_coordinates(point_data)[, 1], \n                             lat = st_coordinates(point_data)[, 2]) %>%\n  mutate(dialect = as.factor(dialect))\n```\n\n```{r echo=FALSE}\ndialects_train %>% head(10) %>% knitr::kable(booktabs = TRUE) %>%\n  kableExtra::kable_styling(latex_options=\"HOLD_position\")\n```\n\nTo get a sense of our data, we can take a random sample from our training data and plot it.\n\n```{r warning=FALSE, message=FALSE, fig.height=4, fig.width=7}\ndialects_sample <- dialects_train %>% sample_n(1000)\n\nggplot(data = dialects_sample) +\n  geom_sf(data = us) +\n  geom_point(aes(x = lon, y = lat, color = dialect), alpha = 1) +\n  scale_color_brewer(palette = \"Dark2\",  name = \"Dialect\")\n```\n\n\n# KNN classification\n\n## Case study: Allegheny and Philadelphia counties\n\nThe implementation of the knn classifier is a little different from earlier classification problems, so let's walk through the basic idea with a quick example. First, let's subset out the data for 2 counties: Allegheny and Philadelphia counties (identified by their FIPS codes).\n\nThen, we'll construct a simple data frame with 3 columns.\n\n```{r}\ndialects_penn <- point_data[grepl(\"42003|42101\", point_data$fips), ]\n\ndialects_penn <- data.frame(dialect = dialects_penn$var, \n                            lon = st_coordinates(dialects_penn)[, 1], \n                            lat = st_coordinates(dialects_penn)[, 2]) %>%\n mutate(dialect = as.factor(dialect))\n```\n\nFrom that, we can split the data into a training and test set.\n\n```{r}\nset.seed(123)\nvalid_split <- rsample::initial_split(dialects_penn, .75)\n\npenn_train <- rsample::analysis(valid_split)\npenn_test <- rsample::assessment(valid_split)\n```\n\nFor more conventional classification tasks, we would want to determine the optimal k for our model using the **train.kknn()** function:\n\n<https://rpubs.com/bonibruno/svm-knn>\n\nFor our purposes, we'll use a quick rule of thumb and use the square-root of our *n* observations. Now we can call a model in which we try to predict the dialect variant (*soda*, *pop*, *coke*, *other*) based on geolocation.\n\n```{r}\nknn_penn <- kknn::kknn(dialect ~ ., \n                       train = penn_train, \n                       test = penn_test, \n                       kernel = \"gaussian\", \n                       k = 61)\n```\n\nFor conventional classification tasks, we would be interested in our confusion matrix, and our model's accuracy\n\n```{r}\ntab <- table(knn_penn$fitted.values, penn_test$dialect)\n\naccuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}\naccuracy(tab) %>% knitr::kable(booktabs = TRUE, digits = 2, col.names = \"Accuracy\")\n```\n\nFor our purposes, we're interested in the probabilities of each variant being preferred based on location. Allegheny county is at the top of our table, with a very strong preference for *pop*.\n\n```{r}\nknn_penn$prob %>% head(1) %>% knitr::kable(booktabs = TRUE, digits = 2)\n```\n\nAnd Philadelphia county is at the bottom of our table with a strong preference for *soda*.\n\n```{r}\nknn_penn$prob %>% tail(1) %>% knitr::kable(booktabs = TRUE, digits = 2)\n```\n\n## Spatial interpolation\n\nThose probabilities can be used to select both color (for the most probable variant) and the alpha of the color (based on the value of the probability).\n\nSounds easy enough, but we don't have values for the entire map. We need to use our known values to estimate those at other unknown points. This is process called spatial interpolation:\n\n<https://docs.qgis.org/2.18/en/docs/gentle_gis_introduction/spatial_analysis_interpolation.html>\n\nWhat follows in wholely indebted to Timo Grossenbacher.\n\n<https://timogrossenbacher.ch/2018/03/categorical-spatial-interpolation-with-r/>\n\nEssentially, we need to configure an empty grid and interpolate that grid with our point data. Another complication is that this process is quite memory and computationally intensive. So we're ultimately going to split up that grid into batches.\n\nWe'll begin setting up our grid by specifying its width in pixels.\n\n```{r}\nwidth_in_pixels <- 300\n```\n\nThe width of a grid cell is calculated from the dimensions of our US map and is defined as dx.\n\n```{r}\ndx <- ceiling( (st_bbox(us)[\"xmax\"] - st_bbox(us)[\"xmin\"]) / width_in_pixels)\n```\n\nThe height of a grid cell is the same because we'll use quadratic grid cells, **dx == dy**.\n\n```{r}\ndy <- dx\n```\n\nCalculate the height in pixels of the resulting grid.\n\n```{r}\nheight_in_pixels <- floor( (st_bbox(us)[\"ymax\"] - st_bbox(us)[\"ymin\"]) / dy)\n```\n\nAnd finally construct the grid with the **st_make_grid()** function.\n\n```{r}\ngrid <- st_make_grid(us, cellsize = dx, n = c(width_in_pixels, height_in_pixels), what = \"centers\")\n```\n\nNext, we'll set our k at 1000. This will result is a smoother looking map.\n\n```{r}\nk <- 1000\n```\n\nNow, we can define a **compute_grid()** function that returns the probability of the most likely variant.\n\n```{r}\ncompute_grid <- function(grid, dialects_train, knn) {\n  # create empty result data frame\n  dialects_result <- data.frame(dialect = as.factor(NA), \n                                lon = st_coordinates(grid)[, 1], \n                                lat = st_coordinates(grid)[, 2])\n  # run KKNN\n  dialects_kknn <- kknn::kknn(dialect ~ ., \n                              train = dialects_train, \n                              test = dialects_result, \n                              kernel = \"gaussian\", \n                              k = knn)\n  # bring back to result data frame\n  # only retain the probability of the dominant dialect at that grid cell\n  dialects_result <- dialects_result %>%\n    # extract the interpolated dialect at each grid cell with the \n    # kknn::fitted() function\n    mutate(dialect = fitted(dialects_kknn),\n           # only retain the probability of the interpolated dialect,\n           # discard the others\n           prob = apply(dialects_kknn$prob, 1, function(x) max(x)))\n  \n  return(dialects_result)\n}\n```\n\nWe can set the parameters for parallel and batch processing. Adjust cores and batch size accordingly.\n\n```{r}\nregisterDoParallel(cores = 4) # if you have more processing power you can up this number\n\n# Specify number of batches and resulting size of each batch (in grid cells).\nno_batches <- 60 # increase this number if you run into memory problems\nbatch_size <- ceiling(length(grid) / no_batches)\n```\n\nFinally, we'll generate our result using parallel processing. This will take at least a couple of minutes.\n\n```{r}\ndialects_result <- foreach(.packages = c(\"sf\", \"tidyverse\"),\n  batch_no = 1:no_batches, \n  # after each grid section is computed, rbind the resulting df into one big dialects_result df\n  .combine = rbind, \n  # the order of grid computation doesn't matter: this speeds it up even more\n  .inorder = FALSE) %dopar% {\n    # compute indices for each grid section, depending on batch_size and current batch\n    start_idx <- (batch_no - 1) * batch_size + 1\n    end_idx <- batch_no * batch_size\n    # specify which section of the grid to interpolate, using regular subsetting\n    grid_batch <- grid[start_idx:ifelse(end_idx > length(grid), \n                                        length(grid),\n                                        end_idx)]\n    # apply the actual computation to each grid section\n    df <- compute_grid(grid_batch, dialects_train, k)\n  }\n```\n\nWe'll now convert the result into an **sf** object for mapping.\n\n```{r}\ndialects_raster <- st_as_sf(dialects_result, \n                            coords = c(\"lon\", \"lat\"),\n                            crs = nalcc,\n                            remove = F)\n```\n\nRemember that we've calculated a rectangular grid. As a last step, we can use the outline of the US to mask the borders of our map.\n\n```{r}\ndialects_raster <- dialects_raster[us, ]\n```\n\nAnd plot the result... (Note that we're dropping *other*.)\n\n```{r warning=FALSE, message=FALSE, fig.height=4, fig.width=7}\nggplot(data = dialects_raster %>% filter(dialect != \"OTHER\")) +\n  geom_raster(aes(x = lon, y = lat, fill = dialect, alpha = prob)) +\n  scale_fill_manual(values = c(\"forestgreen\", \"steelblue\", \"tomato\")) +\n  scale_alpha(guide = 'none') +\n  geom_sf(data = us, alpha = 0, size = 0.25) +\n  theme_void() +\n  theme(legend.position = c(0.1, 0.2), legend.box = \"horizontal\") +\n  theme(legend.title=element_blank())\n```\n\nHow does our result compare to Katz's?\n\n<https://www.foodrepublic.com/2013/07/29/a-final-word-on-the-soda-vs-pop-vs-coke-nomenclature-debate/>\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":{"pdf_document":{"fig_caption":"yes","number_sections":true}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"Lab_15.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.549","editor":"visual","title":"Lab 15","author":"My Name","header-includes":[""]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":[]}