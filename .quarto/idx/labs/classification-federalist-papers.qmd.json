{"title":"Mosteller & Wallace","markdown":{"headingText":"Mosteller & Wallace","containsRefs":false,"markdown":"\n\nWe going to (mostly) replicate a famous classification task that was first done by Frederick Mosteller and David L. Wallace in the early 1960s [-@mosteller1963inference]. They undertook this task without the benefit of modern text processing. It was truly a pioneering work of computational linguistics and information theory. Keep that in mind and we do in minutes what took them months.\n\n\n```{r}\n#| message: false\n#| error: false\n#| warning: false\n\nlibrary(quanteda)\nlibrary(rsample)\nlibrary(tidyverse)\nlibrary(gt)\n```\n\n## Variables\n\nBecause of computational limits, they needed to identify potentially productive variables ahead of building their regression model. This is not how we would go about it now, but it was a constraint at the time. They ended up creating 6 bins of likely words, and those are reported in 3 groupings in their study.\n\nTheir first group contains 70 tokens...\n\n```{r}\n\nmw_group1 <- c(\"a\", \"all\", \"also\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"been\", \"but\", \"by\", \"can\", \"do\", \"down\", \"even\", \"every\", \"for\", \"from\", \"had\", \"has\", \"have\", \"her\", \"his\", \"if\", \"in\", \"into\", \"is\", \"it\",  \"its\", \"may\", \"more\", \"must\", \"my\", \"no\", \"not\", \"now\", \"of\", \"on\", \"one\", \"only\", \"or\", \"our\", \"shall\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"the\", \"their\", \"then\", \"there\", \"things\", \"this\", \"to\", \"up\", \"upon\", \"was\", \"were\", \"what\", \"when\", \"which\", \"who\", \"will\", \"with\", \"would\", \"your\")\n```\n\n\nTheir second an additional 47...\n\n```{r}\n\nmw_group2 <- c(\"affect\", \"again\", \"although\", \"among\", \"another\", \"because\", \"between\", \"both\", \"city\", \"commonly\", \"consequently\", \"considerable\", \"contribute\", \"defensive\", \"destruction\", \"did\", \"direction\", \"disgracing\", \"either\", \"enough\", \"fortune\", \"function\", \"himself\", \"innovation\", \"join\", \"language\", \"most\", \"nor\", \"offensive\", \"often\", \"pass\", \"perhaps\", \"rapid\", \"same\", \"second\", \"still\", \"those\", \"throughout\", \"under\", \"vigor\", \"violate\", \"violence\", \"voice\", \"where\", \"whether\", \"while\", \"whilst\")\n```\n\nAnd their third another 48 (though they identify some by lemmas and another \"expence\" doesn't appear in our data, possibly because of later editing done in our particular edition)...\n\n```{r}\n\nmw_group3 <- c(\"about\", \"according\", \"adversaries\", \"after\", \"aid\", \"always\", \"apt\", \"asserted\", \"before\", \"being\", \"better\", \"care\", \"choice\", \"common\", \"danger\", \"decide\", \"decides\", \"decided\", \"deciding\", \"degree\", \"during\", \"expense\", \"expenses\", \"extent\", \"follow\", \"follows\", \"followed\", \"following\", \"i\", \"imagine\", \"imagined\", \"intrust\", \"intrusted\", \"intrusting\",\"kind\", \"large\", \"likely\", \"matter\", \"matters\", \"moreover\", \"necessary\", \"necessity\", \"necessities\", \"others\", \"particularly\", \"principle\", \"probability\", \"proper\", \"propriety\", \"provision\", \"provisions\", \"requisite\", \"substance\", \"they\", \"though\", \"truth\", \"truths\", \"us\", \"usage\", \"usages\", \"we\", \"work\", \"works\")\n```\n\nAll together, they list 165 candidate variables, though it works out to be 180 unlemmatized tokens as potential variables for their model.\n\nWe'll concatenate a vector of all their variables into a single vector.\n\n```{r}\n\nmw_all <- sort(c(mw_group1, mw_group2, mw_group3))\n```\n\n## *The Federalist Papers*\n\nFor their task, Mosteller & Wallace were interested in solving a long-standing historical debate about the disputed authorship of 12 of the Federalist Papers.\n\nThe Federalist Papers are made up of 85 articles and essays written by Alexander Hamilton, James Madison, and John Jay under the pseudonym \"Publius\" to promote the ratification of the United States Constitution.\n\nAuthorship of the articles has been disputed since their publication, with Hamilton providing a list to his lawyer before his death, and Madison another disputed some of Hamilton's claims.\n\nWe're going to work from the generally accepted authorship designations, which assign authorship of 51 articles to Hamilton, 14 to Madison, 5 to Jay, and 3 to joint authorship. The other 12 are in doubt as to whether they were written by Hamilton or Madison.\n\nSo let's begin. First, we'll get the metadata.\n\n```{r}\n\nload(\"../data/federalist_meta.rda\")\nload(\"../data/federalist_papers.rda\")\n```\n\n\n```{r}\nfed_meta <- federalist_meta %>%\n  dplyr::select(doc_id, author_id)\n```\n\nAnd we're going to read in ALL of the data. Why do it this way? We could build out separate data sets for training, validating, and predicting. HOWEVER, we need our data to be identical in structure at every step. This can become tedious if you're forever wrangling data.frames to get them as the need to be. It's much easier to begin with one dfm and subset as necessary for the classification process.\n\nSo let's read in the text.\n\n```{r}\nfed_txt <- federalist_papers\n```\n\n## Preparing the Data\n\nNow, we'll tokenize the data.\n\n```{r}\nfed_tokens <- fed_txt %>%\n  corpus() %>%\n  tokens(remove_punct = T, remove_symbols = T, what = \"word\")\n```\n\nAnd create a weighted dfm. The 3rd line preps the column so it can be merged with our metadata. The 4th orders the tokens by their mean frequencies. This isn't necessary here, but can be useful when doing quick sub-setting of variables. And the 5th changes the column name for easy joining.\n\n```{r}\nfed_dfm <- fed_tokens %>% dfm() %>% dfm_weight(scheme = \"prop\") %>%\n  convert(to = \"data.frame\") %>%\n  select(doc_id, names(sort(colMeans(.[,-1]), decreasing = TRUE)))\n```\n\nNow let's join the author_id from the metadata.\n\n```{r}\n#| message: false\n\nfed_dfm <- fed_dfm %>% \n  right_join(fed_meta) %>% \n  dplyr::select(doc_id, author_id, everything()) %>% \n  as_tibble()\n```\n\n### Training and testing data\n\nNow we can subset out our training and testing data.\n\n```{r}\ntrain_dfm <- fed_dfm %>% filter(author_id == \"Hamilton\" | author_id == \"Madison\")\ntest_dfm <- fed_dfm %>% filter(author_id == \"Disputed\")\n```\n\nFor the next step we're going to again separate our training data. We want a subset of known data against which we can validate our model.\n\nFor this, we'll use some handy functions from the **rsample** package. First, we make an 80/20 split. From that we create a new, smaller training set, and a validation set.\n\n```{r}\nset.seed(123)\nvalid_split <- initial_split(train_dfm, .8)\ntrain_dfm_v2 <- analysis(valid_split)\ntrain_valid <- assessment(valid_split)\n```\n\nNext, we'll select only those 70 tokens from Mosteller & Wallace's first group. We also need to convert the author_id into a 2-level factor, and to move the text_id to row names. The same for the validation data, but we don't need to worry about the factor conversion.\n\n```{r}\ntrain_dfm_v2_1 <- train_dfm_v2 %>% \n  dplyr::select(doc_id, author_id, all_of(mw_group1)) %>%\n  mutate(doc_id = factor(doc_id)) %>%\n  column_to_rownames(\"doc_id\")\n\ntrain_valid_1 <- train_valid %>% \n  dplyr::select(doc_id, author_id, all_of(mw_group1)) %>%\n  column_to_rownames(\"doc_id\")\n```\n\n## Lasso\n\nFor our regression, we're going to take advantage of lasso regression. This is a form of penalized logistic regression, which imposes a penalty to the logistic model for having too many variables. This results in shrinking the coefficients of the less contributive  variables toward zero. This is also known as regularization.\n\nFor this, we'll use the **glmnet** package.\n\n```{r}\n#| message: false\n#| error: false\n#| warning: false\n\nlibrary(glmnet)\n```\n\n### Ridge & lasso regression\n\nLeast squares fits a model by minimizing the sum of squared residuals.\n\n$$RSS = \\sum_{i=1}^{n}(y_{i}-\\beta_0-\\sum_{j=1}^{p}\\beta_jx_ij)^2$$\n\nRidge Regression is similar, but it includes another term.\n\n$$\\sum_{i=1}^{n}(y_{i}-\\beta_0-\\sum_{j=1}^{p}\\beta_jx_ij)^2 + \\lambda \\sum_{j=1}^{p}\\beta_{j}^{2} = RSS + \\lambda \\sum_{j=1}^{p}\\beta_{j}^{2}$$\n\nIn order to minimize this equation $\\beta_1,...\\beta_p$ should be close to zero and so it shrinks the coefficients. The tuning parameter, $\\lambda$, controls the impact.\n\nRidge regression does have some disadvantages.\n\n* Unlike subset selection, ridge regression includes all *p* predictors.\n* The penalty term will shrink all of the coefficients towards zero, but none of them will be exactly zero.\n* Such a large model often makes interpretation difficult.\n\nThe lasso helps overcome these problems. It is similar to ridge regression, but the penalty term is slightly different.\n\n$$\\sum_{i=1}^{n}(y_{i}-\\beta_0-\\sum_{j=1}^{p}\\beta_jx_ij)^2 + \\lambda \\sum_{j=1}^{p}|\\beta_{j}| = RSS + \\lambda \\sum_{j=1}^{p}|\\beta_{j}|$$\n\nLike ridge regression it shrinks the coefficients towards zero. However, the lasso allows some of the coefficients to be exactly zero.\n\nFor more detail on lasso regression you can look here:\n\n<https://eight2late.wordpress.com/2017/07/11/a-gentle-introduction-to-logistic-regression-and-lasso-regularisation-using-r/>\n\nThis is a very useful technique for variable selection and can reduce the likelihood of overfitting. This is particularly helpful in linguistic analysis where we're often working with many variables making the implementation of functions like **step()** sometimes tedious.\n\n### Using **glmnet**\n\nTo help you decide which lambda to use, the `cv.glmnet()` function does cross-validation. The default sets **alpha=1** for lasso. If we wanted ridge, we would set **alpha=0**.\n\n```{r}\n\ncv_fit <- cv.glmnet(as.matrix(train_dfm_v2_1[, -1]), train_dfm_v2_1[, 1], family = \"binomial\")\n```\n\nWe can plot the log of the resulting lambdas.\n\n```{r}\n#| fig-height: 4\n#| fig-width: 7\n\nplot(cv_fit)\n```\n\nThe plot displays the cross-validation error according to the log of lambda. The left dashed vertical line indicates that the log of the optimal value of lambda is approximately -6, which is the one that minimizes the prediction error. This lambda value will give the most accurate model. \n\nThe exact value of lambda can also be viewed. We'll save our regression coefficients.\n\n```{r}\n\nlambda_min <- cv_fit$lambda.min\nlambda_lse <- cv_fit$lambda.1se\n```\n\nBy filtering those variables with coefficients of zero, we see only the variables have been included in the model. Ours has 13.\n\n```{r}\n\ncoef(cv_fit, s = \"lambda.min\") |>\n  as.matrix() |>\n  data.frame() |>\n  rownames_to_column(\"Variable\") |>\n  filter(s1 !=0) |>\n  dplyr::rename(Coeff = s1)  |>\n  gt() |>\n  fmt_number(columns = \"Coeff\",\n             decimals = 2)\n```\n\n## Validate the model\n\nTo validate the model, let's create a model matrix from the texts we've split off for that purpose.\n\n```{r}\n\nx_test <- model.matrix(author_id ~., train_valid_1)[,-1]\n```\n\nFrom our model, we'll predict the author_id of the validation set.\n\n```{r}\n\nlasso_prob <- predict(cv_fit, newx = x_test, s = lambda_lse, type = \"response\")\n```\n\nFrom the probabilities, we can return the predicted authors.\n\n```{r}\n\nlasso_predict <- ifelse(lasso_prob > 0.5, \"Madison\", \"Hamilton\")\n```\n\n```{r}\n#| code-fold: true\n\nlasso_predict |> \n  data.frame() |>\n  dplyr::rename(Predict = s1) |>\n  tibble::rownames_to_column(\"Test\") |>\n  gt()\n```\n\nRetrieve what they actually are and calculate our model accuracy.\n\n```{r}\n\ntable(pred=lasso_predict, true=train_valid_1$author_id)\n```\n\n```{r}\npaste0(mean(lasso_predict == train_valid_1$author_id)*100, \"%\")\n```\n\nOurs is 100% accurate. Not bad. Note that if you wanted to really test the model, we could create a function to run through this process starting with the sampling.That way, we could generate a range of accuracy over repeated sampling of training and validation data.\n\n## Mosteller & Wallace's Experiment\n\nLet's repeat the process, but this time we'll start with all of Mosteller & Wallace's candidate variables.\n\n### Create a new training and validation set\n\n```{r}\ntrain_dfm_v2_2 <- train_dfm_v2 %>% \n  dplyr::select(doc_id, author_id, all_of(mw_all)) %>%\n  mutate(author_id = factor(author_id)) %>%\n  column_to_rownames(\"doc_id\")\n\ntrain_valid_2 <- train_valid %>% \n  dplyr::select(doc_id, author_id, all_of(mw_all)) %>%\n  column_to_rownames(\"doc_id\")\n```\n \n### Carry out cross-validation\n \n```{r}\ncv_fit <- cv.glmnet(as.matrix(train_dfm_v2_2[, -1]), train_dfm_v2_2[, 1], family = \"binomial\")\n```\n \nLook at our coefficients... 17 this time...\n \n```{r}\ncoef(cv_fit, s = \"lambda.min\") |>\n  as.matrix() |>\n  data.frame() |>\n  rownames_to_column(\"Variable\") |>\n  filter(s1 !=0) |>\n  dplyr::rename(Coeff = s1) |>\n  gt() |>\n  fmt_number(columns = \"Coeff\",\n             decimals = 2)\n```\n\nSave our minimum lambda and our regression coefficients.\n\n```{r}\nlambda_min <- cv_fit$lambda.min\nlambda_lse <- cv_fit$lambda.1se\n```\n\n### Create a matrix from the validation set\n\n```{r}\nx_test <- model.matrix(author_id ~., train_valid_2)[,-1]\n```\n\n### Predict the **author_id** of the validation set.\n\n```{r}\nlasso_prob <- predict(cv_fit, newx = x_test, s = lambda_lse, type = \"response\")\n```\n\n### Return the predicted authors.\n\n```{r}\nlasso_predict <- ifelse(lasso_prob > 0.5, \"Madison\", \"Hamilton\")\n```\n\n### Check confusion matrix\n\n```{r}\n table(pred=lasso_predict, true=train_valid_1$author_id)\n```\n\nThe model looks good... So let's proceed with the data in question.\n\n### Prepare full training set\n\n```{r}\ntrain_dfm <- train_dfm %>% \n  dplyr::select(doc_id, author_id, all_of(mw_all)) %>%\n  mutate(author_id = factor(author_id)) %>%\n  column_to_rownames(\"doc_id\")\n```\n\n### Prepare test data\n\n```{r}\ntest_dfm <- test_dfm %>% \n  dplyr::select(doc_id, author_id, all_of(mw_all)) %>%\n  column_to_rownames(\"doc_id\")\n```\n\n### Carry out cross-validation\n\n```{r}\ncv_fit <- cv.glmnet(as.matrix(train_dfm[, -1]), train_dfm[, 1], family = \"binomial\")\n```\n\nAs we would expect, this is close to what we saw previously.\n\n```{r}\ncoef(cv_fit, s = \"lambda.min\") |>\n  as.matrix() |>\n  data.frame() |>\n  rownames_to_column(\"Variable\") |>\n  filter(s1 !=0) |>\n  dplyr::rename(Coeff = s1) |>\n  gt() |>\n  fmt_number(columns = \"Coeff\",\n             decimals = 2)\n```\n\n### Run lasso\n\n```{r}\n\nlasso_fit <- glmnet(as.matrix(train_dfm[, -1]), train_dfm[, 1], alpha = 1, family = \"binomial\", lambda = cv_fit$lambda.min)\n```\n\n### Create a matrix from the test set and predict author\n\n```{r}\n\nx_test <- model.matrix(author_id ~., test_dfm)[,-1]\nlasso_prob <- predict(cv_fit, newx = x_test, s = lambda_lse, type = \"response\")\nlasso_predict <- ifelse(lasso_prob > 0.5, \"Madison\", \"Hamilton\")\n```\n\n### Check results\n\n```{r}\n\ndata.frame(lasso_predict, lasso_prob) |>\n  dplyr::rename(Author = s1, Prob = s1.1) |>\n  gt() |>\n  fmt_number(columns = \"Prob\",\n             decimals = 2)\n```\n\nOur model predicts all but 55 were written by Madison. Our model is not particularly confident about that result. This hews pretty closely to Mosteller & Wallace's findings, through they come down (sort of) on the side of Madison for 55. However, they also acknowledge that the evidence is weak and not very convincing.\n\nIt's worth noting, too, that other studies using other techniques have suggested that 55 was authored by Hamilton. See, for example, here:\n\n<https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0054998>\n\n::: callout-important\n## Pause for Lab Set Question\n\nComplete [Tasks 1 and 2 in Lab Set 1](../lab_sets/LabSet_01.qmd#mosteller-wallace).\n:::\n\n## Works cited\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"classification-federalist-papers.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.549","editor":"source"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":[]}