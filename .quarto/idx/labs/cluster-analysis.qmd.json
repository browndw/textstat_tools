{"title":"Cluster Analysis","markdown":{"headingText":"Cluster Analysis","containsRefs":false,"markdown":"\n\nIn this lab we'll practice two types of cluster analysis\n\n1. Hierarchical agglomerative clustering\n2. K-means clustering\n\n\n```{r setup}\n#| message: false\n#| error: false\n#| warning: false\n\nlibrary(quanteda)\nlibrary(tidyverse)\nlibrary(udpipe)\nlibrary(cluster)\nlibrary(factoextra)\nlibrary(gt)\n```\n\n## Hierarchical Agglomerative Clustering\n\nWe'll start with hierarchical agglomerative clustering (pgs. 154, 159 & 236 in Brezina). Hierarchical cluster analysis is visualized with a dendrogram. If you are unfamiliar with these plots, there is a nice explanation here:\n\n<https://wheatoncollege.edu/wp-content/uploads/2012/08/How-to-Read-a-Dendrogram-Web-Ready.pdf>\n\n### Prepare the data\n\nOur clustering will be based on part-of-speech counts, so we need to parse some data using **udpipe**. First, we'll get data from the micusp_mini corpus and subset out the Biology and English papers.\n\n```{r}\nload(\"../data/micusp_mini.rda\")\n```\n\n\n```{r}\nsub_df <- micusp_mini %>%\n  filter(str_detect(doc_id, \"BIO|ENG\"))\n```\n\nAnd parse the data using **udpipe**. This will take a couple of minutes.\n\n```{r}\nud_model <- udpipe_load_model(\"../models/english-ewt-ud-2.5-191206.udpipe\")\nannotation <- udpipe_annotate(ud_model, x = sub_df$text, doc_id = sub_df$doc_id, parser = \"none\")\n```\n\nNow, we're going to do something new. We're going to combine our upos and xpos columns.\n\n```{r}\nanno_edit <- annotation %>%\n  as_tibble() %>%\n  unite(\"upos\", upos:xpos)\n```\n\nNext we create a named list from the new, concatenated column.\n\n```{r create_split}\nsub_tokens <- split(anno_edit$upos, anno_edit$doc_id)\n```\n\nThis is what the data looks like:\n\n```{r}\nsub_tokens$BIO.G0.02.1[1:10]\n```\n\nNow, we'll use that as our tokens object and filter out a few of the tokens to simplify our feature matrix.\n\n```{r tokens}\nsub_tokens <- as.tokens(sub_tokens)\nsub_tokens <- tokens_remove(sub_tokens, \"^punct_\\\\S+\", valuetype = \"regex\")\nsub_tokens <- tokens_remove(sub_tokens, \"^sym_\\\\S+\", valuetype = \"regex\")\nsub_tokens <- tokens_remove(sub_tokens, \"^x_\\\\S+\", valuetype = \"regex\")\n```\n\nFrom that, we'll generate a dfm. We'll weight the raw counts, and convert the result to a data frame.\n\n```{r dfm}\nsub_dfm <- sub_tokens %>%\n  dfm() %>%\n  dfm_weight(scheme = \"prop\") %>%\n  convert(to = \"data.frame\")\n```\n\nFinally, we're going to convert the first row (doc_id) into row names. And, for convenience, we'll order our columns alphabetically.\n\n```{r}\nsub_dfm <- sub_dfm %>% column_to_rownames(\"doc_id\") %>% \n  dplyr::select(order(colnames(.)))\n```\n\nAs we did with factor analysis, we'll scale our variables. Scaling the variables transforms them such that they have a mean of roughly zero, and a standard deviation of 1. See Brezina pg. 152-153. We can check the noun column, for example.\n\n```{r scale}\nsub_dfm <- sub_dfm %>% scale() %>% data.frame()\n```\n\n```{r}\nround(mean(sub_dfm$noun_nn), 5)\nsd(sub_dfm$noun_nn)\n```\n\n### Create a distance matrix\n\nWe can use some base R functions to create our dendrogram from the following steps. First, we need to create a difference matrix based on distances. The two most common distance measures are **euclidean** and **manhattan**, which are described on pg. 153. Note, however, that there are other options, many of which are described here:\n\n<https://numerics.mathdotnet.com/Distance.html>\n\nA detailed defense of **manhattan** distance is located here:\n\n<http://rstudio-pubs-static.s3.amazonaws.com/476168_58516a3d6685427badf52a263e690975.html>\n\nAnd a comparative study of distance measure is published here:\n\n<https://arxiv.org/ftp/arxiv/papers/1411/1411.7474.pdf>\n\nWe'll start with **euclidean** distance.\n\n```{r distance}\nd <- dist(sub_dfm, method = \"euclidean\")\n```\n\n### Clustering and linkage methods\n\nThe next step is to determine the linkage method. Brezina details these on pg. 154-159. Here they are in summary:\n\n* Maximum or complete linkage clustering: It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the largest value (i.e., maximum value) of these dissimilarities as the distance between the two clusters. It tends to produce more compact clusters.\n* Minimum or single linkage clustering: It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the smallest of these dissimilarities as a linkage criterion. It tends to produce long, “loose” clusters.\n* Mean or average linkage clustering: It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the average of these dissimilarities as the distance between the two clusters.\n* Centroid linkage clustering: It computes the dissimilarity between the centroid for cluster 1 (a mean vector of length p variables) and the centroid for cluster 2.\n* Ward’s minimum variance method: It minimizes the total within-cluster variance. At each step the pair of clusters with minimum between-cluster distance are merged.\n\nWe'll carry out the hierarchical clustering using Ward.\n\n```{r hclust}\nhc <- hclust(d, method = \"ward.D2\")\n```\n\nAnd plot the result.\n\n```{r}\n#| fig-width: 7\n#| fig-height: 4\n#| fig-cap: \"Cluster dendrogram (using Euclidean distances and Ward's method) of sample papers in English and Biology.\"\n\nplot(hc, cex = 0.6, hang = -1, main = \"\", sub = \"\", xlab = \"\")\n```\n\nNote that height is the value of the criterion associated with the clustering method for the particular agglomeration. In this case, Ward's criterion is the total within-cluster error sum of squares, which increases as you go up the tree and make the clusters bigger.\n\nWe can follow these same steps using functions from the cluster package, too. These provide us with a few additional options, like **get_dist()**, which we'll use to create a distance matrix.\n\n```{r}\nd <- get_dist(sub_dfm)\n```\n\nNow let's visualize that matrix.\n\n```{r}\n#| fig-width: 7\n#| fig-height: 4\n#| fig-cap: \"A distance matrix (using Euclidean distances) of sample papers in English and Biology.\"\n\nfviz_dist(d, gradient = list(low = \"tomato\", mid = \"white\", high = \"steelblue\"))\n```\n\n### Clustering structure\n\nWe create our plot using the **agnes()** function, this time. Agglomerative Nesting is fully described in chapter 5 of Kaufman and Rousseeuw (1990), *Finding Groups in Data: An Introduction to Cluster Analysis*, which is available online through the CMU library:\n\n<https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019521734604436>\n\nCompared to other agglomerative clustering methods such as hclust, agnes has the following feature: it yields the agglomerative coefficient (see agnes.object) which measures the amount of clustering structure found.\n\n```{r agnes}\nhc <- agnes(d, method = \"ward\" )\n```\n\nThis can be plotted in a similar way.\n\n```{r}\n#| fig-width: 7\n#| fig-height: 4\n#| fig-cap: \"Cluster dendrogram (using Euclidean distances and Ward's method) of sample papers in English and Biology.\"\n\nplot(as.hclust(hc), cex = 0.6, hang = -1, main = \"\", sub = \"\", xlab = \"\")\n```\n\nBut we can also retrieve an agglomerative coefficient, which measures the amount of clustering structure found  (values closer to 1 suggest strong clustering structure).\n\n```{r}\nhc$ac\n```\n\nThus, we can see how the structure changes with different linkage methods, First, we can create a vector and a simple function.\n\n```{r}\nm <- c( \"average\", \"single\", \"complete\", \"ward\")\nnames(m) <- c( \"average\", \"single\", \"complete\", \"ward\")\n\nac <- function(x) {\n  agnes(d, method = x)$ac\n}\n```\n\n```{r}\n#| code-fold: true\n#| tbl-cap: \"Agglomerative coefficients for various linkage methods.\"\n\nmap_dbl(m, ac) |>\n  tibble::enframe() |>\n  gt() |>  \n  cols_label(\n    name = md(\"**Method**\"),\n    value = md(\"**Coeff**\")\n  )\n```\n\n### Cutting a dendrogram\n\nWe can also \"cut\" our dendrogram in any number of clusters. The question is: How many clusters are optimal. Here, we can use some plotting functions that are part of the **factoextra** package. The first is the familiar \"elbow\" method.\n\n```{r}\n#| fig-width: 7\n#| fig-height: 4\n#| fig-cap: \"A scree plot for within-sum-of-squares.\"\n\nfviz_nbclust(sub_dfm, FUN = hcut, method = \"wss\")\n```\n\nWith our data, the result isn't particularly helpful. We can then try the \"silhouette\" methods. The average silhouette approach measures the quality of a clustering. That is, it determines how well each object lies within its cluster.\n\nA high average silhouette width indicates a good clustering. The average silhouette method computes the average silhouette of observations for different values of k. The optimal number of clusters k is the one that maximizes the average silhouette over a range of possible values for k.\n\n```{r}\n#| fig-width: 7\n#| fig-height: 4\n#| fig-cap: \"A silhouette plot.\"\n\nfviz_nbclust(sub_dfm, FUN = hcut, method = \"silhouette\")\n```\n\nWe can use the result to choose how we want to \"cut\" our dendrogram. Here we'll cut it into two clusters.\n\n```{r}\n#| fig-width: 7\n#| fig-height: 4\n#| fig-cap: \"Cluster dendrogram (using Euclidean distances and Ward's method) of sample papers in English and Biology cut into 2 clusters.\"\n\nplot(as.hclust(hc), cex = 0.6, hang = -1, main = \"\", sub = \"\", xlab = \"\")\nrect.hclust(hc, k = 2)\n```\n\n___\n\\begin{center}\nSTOP!\\\\\nCOMPLETE TASK 1\n\\end{center} \n___\n\n\n## K-means\n\nAlthough it's not covered in Brezina, another very common clustering method is called k-means. The basic idea behind k-means clustering consists of defining clusters  so that the total intra-cluster variation (known as total within-cluster variation) is minimized.\n\nThe k-means algorithm can be summarized as follows:\n\nBy the analyst:\n\n* Specify the number of clusters (k) to be created\n\nBy the algorithm:\n\n* Select randomly k objects from the data set as the initial cluster centers or means\n* Assign each observation to their closest centroid, based on the Euclidean distance between the object and the centroid\n* For each of the k clusters update the cluster centroid by calculating the new mean values of all the data points in the cluster. The centroid of a kth cluster is a vector of length p containing the means of all variables for the observations in the kth cluster; p is the number of variables.\n* Iteratively minimize the total within sum-of-squares. That is, iterate steps 3 and 4 until the cluster assignments stop changing or the maximum number of iterations is reached. By default, the R software uses 10 as the default value for the maximum number of iterations.\n\nWe've already determined that our data is best divided into 2 clusters. So we specify \"centers\" to be 2.\n\n```{r}\nkm <- kmeans(sub_dfm, centers = 2, nstart = 25)\n```\n\nNote that we can access important information about our clusters. For example, we can return the within sum-of-squares:\n\n```{r}\nkm$withinss\n```\n\nOr the between sum-of-squares:\n\n```{r}\nkm$betweenss\n```\n\nPlotting the result is easy with **fviz_cluster()**.\n\n```{r}\nfviz_cluster(km, data = sub_dfm)\n```\n\nBut there a variety of ways to make effective plots. Let's make one that gives us more control over the details.\n\n### Plotting and dimension reduction\n\nDimension reduction for plotting k-means is typically done using PCA. So lets start there.\n\n```{r}\nkm_pca <- prcomp(sub_dfm)\n```\n\nWe can check the percent of variance explained by looking at the eigen values.\n\n```{r}\n#| code-fold: true\n#| warning: false\n#| message: false\n\nround(get_eigenvalue(km_pca), 1) |>\n  head(10) |>\n  gt()\n```\n\nWe can also extract the coordinates for the 2 principal components and create a data frame. We'll also add columns for discipline and cluster membership.\n\n```{r}\ncoord_df <- data.frame(km_pca$x[,1:2]) %>%\n  mutate(Discipline = str_extract(rownames(sub_dfm), \"^[A-Z]+\")) %>%\n  mutate(Cluster = as.factor(paste0(\"Cluster \", km$cluster)))\n```\n\n```{r}\n#| fig-width: 7\n#| fig-height: 4\n#| fig-cap: \"Cluster dendrogram (using Euclidean distances and Ward's method) of sample papers in English and Biology cut into 2 clusters.\"\n\nggplot(coord_df) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  geom_point(aes(x = PC1, y = PC2, fill = Discipline), size = 1, shape = 21, alpha = .75) +\n  viridis::scale_fill_viridis(discrete = T, direction = -1) +\n  xlab(paste0(\"Dimension 1\")) +\n  ylab(\"Dimension 2\") +\n  theme_linedraw() +\n  theme(panel.grid.major.x = element_blank()) +\n  theme(panel.grid.minor.x = element_blank()) +\n  theme(panel.grid.major.y = element_blank()) +\n  theme(panel.grid.minor.y = element_blank()) +\n  theme(legend.position=\"top\") +\n  facet_grid(~Cluster)\n```\n\n### Variable contributions\n\nVariable contributions to each PC and their relationship to individual observations and clusters can be visualized using a biplot.\n\n```{r}\n#| fig-width: 7\n#| fig-height: 4\n#| fig-cap: \"Biplot showing the variables with the 10 highest contributions to principal components 1 and 2.\"\n\nfviz_pca_biplot(km_pca, repel = TRUE,\n                select.var = list(contrib=10),\n                col.var = \"#2E9FDF\", # Variables color\n                col.ind = \"#696969\"  # Individuals color\n)\n```\n\nTo extract the precise percentage each variable contributes to a PC we can use **fviz_contrib()**, but let's briefly look at how those contributions are calculated.\n\nFirst, we can use **get_pca_var()** from the **factoextra** package to extract the loadings. \n\n```{r}\nkm_pca_var <- get_pca_var(km_pca)\n```\n\nLoadings are the coordinates of the features/variables on the principal components. Loadings are unstandardized eigenvectors’ elements.\n\nContributions are the square of the loading matrix (the cos2 output from **get_pca_var()** function) divided by the column sums of the cos2 matrix, which are the variances of PCs.\n\nThere is a nice explanation here:\n\n<https://littlebitofdata.com/en/2017/12/pca/>\n\nTo verify this, we can check to see if the relevant vectors are equal:\n\n```{r}\nall.equal(km_pca_var$cos2[,1] * 100 / sum(km_pca_var$cos2[,1]), km_pca_var$contrib[,1])\n```\n\nHere they are in tablular form:\n\n```{r}\n#| code-fold: true\n#| tbl-cap: \"Contributions to PC1\"\n# PC1 % Contribution\n\nkm_pca_var$contrib[,1] |>\n  sort(decreasing = T) |>\n  tibble::enframe() |>\n  gt()\n```\n\n___\n\\begin{center}\nSTOP!\\\\\nCOMPLETE TASK 2\n\\end{center} \n___\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"cluster-analysis.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.549","editor":"source"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":[]}