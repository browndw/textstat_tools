{"title":"Introduction to Vector Models and Word Embeddings","markdown":{"headingText":"Introduction to Vector Models and Word Embeddings","containsRefs":false,"markdown":"\n\nThis is just a very quick walk-though tutorial on using vector models. This Lab only uses a pre-trained model. However, you can build your own model (which we will do in another Lab). You can also use vector models for classification tasks. For that, you should consult the document for the R package we're using: **ruimtehol**.\n\nThere is an overview here:\n\n<http://www.bnosac.be/index.php/blog/86-neural-text-modelling-with-r-package-ruimtehol>\n\nAnd here:\n\n<https://github.com/bnosac/ruimtehol>\n\nFor more about Word2vec and embeddings generally, see the overview here:\n\n<https://medium.com/compassred-data-blog/introduction-to-word-embeddings-and-its-applications-8749fd1eb232>\n\n```{r setup}\n#| message: false\n#| error: false\n#| warning: false\n\nlibrary(ruimtehol)\nlibrary(tidyverse)\nlibrary(Rtsne)\nlibrary(gt)\n```\n\n## Basics\n\nA variety of models are available for you on CMU Box:\n\n<https://cmu.box.com/s/o2y5lbaonmguf51h5kipa6i2v4y3nm3n>\n\nYou can simply download a model and place it in the `models` directory. Then load the model.\n\nFor demo purposes, we'll use a small one that contains the 50,000 most frequent tokens in COCA and embeddings in only 100 dimensions.\n\n\n```{r load_model}\n# model <- starspace_load_model(\"../models/en_coca_vec_sm.ruimtehol\")\n# model <- starspace_load_model(\"../models/en_stanford_cc_lg.ruimtehol\")\n\nmodel <- starspace_load_model(\"../models/en_coca_vec_50k.ruimtehol\")\n```\n\nNow we can check how similarities work.\n\n```{r similarity}\nembedding_similarity(\n  starspace_embedding(model, \"dog\"),\n  starspace_embedding(model, \"cat\"), \n  type = \"cosine\") |>\n  tibble::enframe() |>\n  gt()\n```\n\nLook for nearest embeddings.\n\n```{r shakespeare}\nstarspace_knn(model, \"shakespeare\", 10) |>\n  data.frame() |>\n  gt()\n```\n\nWe can also calculate document similarity.\n\n```{r doc_similarity}\nembedding_similarity(\n  starspace_embedding(model, \"what does this bunch of text look like\", type = \"document\"),\n  starspace_embedding(model, \"word abracadabra is even in the dictionary\", type = \"document\"), \n  type = \"cosine\") |>\n  tibble::enframe() |>\n  gt()\n```\n\n\nNote that we can extract embeddings, as well.\n\n```{r as_matrix}\nembedding <- as.matrix(model)\n```\n\n```{r dog_cat}\nembedding[c(\"dog\", \"cat\"), ][,1:10] |>\n  data.frame() |>\n  gt()\n```\n\nThis ability is useful if we want to plot the locations of words or documents.\n\n## Example from the MICUSP data\n\nFirst, we'll read in the text data and load some functions.\n\n```{r}\nload(\"../data/micusp_mini.rda\")\nsource(\"../R/helper_functions.R\")\n```\n\nNext, we'll filter out two disciplines and prepocess the text.\n\n```{r process_micusp}\n#| message: false\n#| error: false\n#| warning: false\n\ndf <- micusp_mini %>%\n  filter(str_detect(doc_id, \"^BIO|^ENG\")) %>%\n  mutate(text = preprocess_text(text, remove_numbers = T))\n```\n\nLet's sample out one example for each score from question \"P1\".\n\n```{r embed_function}\nget_embeddings <- function(x){\n  \n  df <- x\n  idx <- seq(1:nrow(df))\n  doc_embed <- function(i){\n    e <- starspace_embedding(model, x = df[i,2], type = \"document\")\n    rownames(e) <- df[i,1]\n    e <- data.frame(e)\n  }\n  doc_embeds <- lapply(idx, doc_embed)\n  doc_embeds <- bind_rows(doc_embeds)\n  return(doc_embeds)\n}\n```\n\nNow we can extract the embeddings for each our documents:\n\n```{r extract_embeddings}\ndoc_embeddings <- get_embeddings(df)\n\ndoc_embeddings <- doc_embeddings %>% \n  rownames_to_column(\"doc_id\") %>%\n  mutate(group = str_extract(doc_id, \"^[A-Z]+\")) %>%\n  dplyr::select(doc_id, group, everything())\n```\n\nFrom that, we calculated document similarity:\n\n```{r closest_docs}\n#| message: false\n#| error: false\n#| warning: false\n\nclosest_docs <- function(doc_query){\n  pts <- doc_embeddings %>% filter(doc_id == doc_query) %>% select(X1:X100)\n  closest <- RANN::nn2(data = doc_embeddings[,3:102], query = pts, k = 11)\n  doc_ids <- doc_embeddings[closest$nn.idx,1]\n  doc_dist <- closest$nn.dists %>% as.vector()\n  df <- data.frame(doc_ids, doc_dist)\n  return(df)\n}\n```\n\nIn order to exact the embeddings we'll create a very simple function.\n\n```{r closest_bio}\nclosest_docs(\"BIO.G0.02.1\") |>\n  gt()\n```\n\nAnd extract them.\n\n```{r closest_eng}\nclosest_docs(\"ENG.G2.04.1\") |>\n  gt()\n```\n\n## Plotting\n\nFor dimension reduction use a different technique: t-SNE. For more about t-Distributed Stochastic Neighbor Embedding see here:\n\n<https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1>\n\n```{r dist}\n#| message: false\n#| error: false\n#| warning: false\n\nm <- dist(doc_embeddings[,3:102]) %>% as.matrix()\ndimnames(m) <- dimnames(m) <- list(doc_embeddings$doc_id, doc_embeddings$doc_id) \ndf_pairs <- t(combn(doc_embeddings$doc_id, 2))\ndist_df <- data.frame(df_pairs, dist=m[df_pairs])\ndist_df <- dist_df %>% arrange(dist)\n```\n\n```{r tsne}\n\ndoc_tsne <- Rtsne(as.matrix(doc_embeddings[,3:102]), check_duplicates = FALSE, pca = FALSE, perplexity=5, theta=0.5, dims=2)\n\ndoc_tsne <- as.data.frame(doc_tsne$Y) %>% bind_cols(select(doc_embeddings, doc_id, group))\n```\n\n\n```{r tsne-plot}\n#| warning: false\n#| fig-height: 4\n#| fig-width: 7\n#| fig-cap: \"Histogram of the token *the*\"\n\nggplot(doc_tsne, aes(x = V1, y = V2, fill = group)) +\n  geom_point(shape = 21) +\n  ggrepel::geom_text_repel(aes(label = doc_id), size = 3) +\n  viridis::scale_fill_viridis(discrete = T) +\n  theme_linedraw() +\n  theme(panel.grid.minor = element_blank())\n```\n\n## Another Example\n\nNext, let's build a predictive model from the Federalist Papers. Again, we'll convert the text to lower case and remove punctuation.\n\n\n```{r fed-data}\nload(\"../data/federalist_meta.rda\")\nload(\"../data/federalist_papers.rda\")\n```\n\n\n```{r fed-prep}\n#| message: false\n#| error: false\n#| warning: false\n\nfed_meta <- federalist_meta %>%\n  dplyr::select(doc_id, author_id)\n\nfed_txt <- federalist_papers\n\nfed_txt <- fed_txt %>%\n  left_join(fed_meta) %>%\n  filter(author_id == \"Hamilton\" | author_id == \"Madison\" | author_id == \"Disputed\") %>%\n  mutate(text = preprocess_text(text, remove_numbers = T))\n```\n\nNext, we'll subset out our training and testing data. Note that we're going to down-sample in order to balance majority and minority data classes.\n\n```{r train-test}\n#| message: false\n#| error: false\n#| warning: false\n\ntrain <- fed_txt %>%\n  filter(author_id == \"Hamilton\" | author_id == \"Madison\") %>%\n  group_by(author_id) %>%\n  sample_n(14) %>%\n  ungroup()\n\ntest <- fed_txt %>%\n  filter(author_id == \"Disputed\")\n```\n\nNow we can build the model. Here we use bigrams with a minimum count of 2\n\n```{r pred-embed}\n#| message: false\n#| error: false\n#| warning: false\n#| results: hide\n\npred_model <- embed_tagspace(x = train$text, y = train$author_id,\n                        early_stopping = 0.9 ,dim = 300, \n                        lr = 0.01, epoch = 10, loss = \"softmax\", adagrad = TRUE, \n                        similarity = \"cosine\", negSearchLimit = 10,\n                        ngrams = 2, minCount = 2)\n```\n\nWe can predict the text of any selected row....\n\n```{r}\n#| message: false\n#| error: false\n#| warning: false\n\npredict(pred_model, test$text[1], k = 10)\n```\n\nOr create a vector of texts and generate predictions for each... \n\n```{r}\n#| message: false\n#| error: false\n#| warning: false\n\ntext   <- test$text\n\nemb_labels <- as.matrix(pred_model, type =\"labels\", prefix = FALSE)\n\nscores <- embedding_similarity(starspace_embedding(pred_model, text), emb_labels, type = \"cosine\", top_n = 1)\n```\n\nThen construct a dataframe from the results:\n\n```{r}\n#| warning: false\n#| message: false\n\ndata.frame(test$doc_id, scores[, c ( \"term2\" , \"rank\" )]) |>\n  gt()\n```\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"vector-models.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.549","editor":"source"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":[]}