{"title":"Lab 14","markdown":{"yaml":{"title":"Lab 14","author":"My Name","output":{"pdf_document":{"fig_caption":"yes","number_sections":true}},"header-includes":[""]},"headingText":"Lab 14: Random Forest","headingAttr":{"id":"","classes":["unnumbered"],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n```{r, include=FALSE}\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#>\",\n  tidy.opts=list(width.cutoff=70),\n  tidy=TRUE\n)\n```\n\n\nWe're going to repeat the Federalist classification problem. Rather than a loasso regression model, however, this time we'll be constructing a random forest classification model.\n\nBroadly, random forests generate many classification trees. Each tree gives a classification, and we say the tree \"votes\" for that class. The forest chooses the classification having the most votes (over all the trees in the forest).  \n\nIf you're unfamiliar with random forests, you can (and should) read more here:\n\n<https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm>\n\n\n```{r setup, message = FALSE, error=FALSE, warning=FALSE}\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(rsample)\nlibrary(randomForest) # basic implementation\nlibrary(rfUtilities) # utilities for performance evaluation\n```\n\n# Variables\n\nAs with the Lab on lasso regression, we'll start with Mosteller & Wallace's lists of tokens.\n\nTheir first group contains 70 tokens...\n\n```{r group_1}\nmw_group1 <- c(\"a\", \"all\", \"also\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"been\", \"but\", \"by\", \"can\", \"do\", \"down\", \"even\", \"every\", \"for\", \"from\", \"had\", \"has\", \"have\", \"her\", \"his\", \"if\", \"in\", \"into\", \"is\", \"it\",  \"its\", \"may\", \"more\", \"must\", \"my\", \"no\", \"not\", \"now\", \"of\", \"on\", \"one\", \"only\", \"or\", \"our\", \"shall\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"the\", \"their\", \"then\", \"there\", \"things\", \"this\", \"to\", \"up\", \"upon\", \"was\", \"were\", \"what\", \"when\", \"which\", \"who\", \"will\", \"with\", \"would\", \"your\")\n```\n\n\nTheir second an addional 47...\n\n```{r group_2}\nmw_group2 <- c(\"affect\", \"again\", \"although\", \"among\", \"another\", \"because\", \"between\", \"both\", \"city\", \"commonly\", \"consequently\", \"considerable\", \"contribute\", \"defensive\", \"destruction\", \"did\", \"direction\", \"disgracing\", \"either\", \"enough\", \"fortune\", \"function\", \"himself\", \"innovation\", \"join\", \"language\", \"most\", \"nor\", \"offensive\", \"often\", \"pass\", \"perhaps\", \"rapid\", \"same\", \"second\", \"still\", \"those\", \"throughout\", \"under\", \"vigor\", \"violate\", \"violence\", \"voice\", \"where\", \"whether\", \"while\", \"whilst\")\n```\n\nAnd their third another 48 (though they identify some by lemmas and another \"expence\" doesn't appear in our data, possibly because of later editing done in our particular edition)...\n\n```{r gorup_3}\nmw_group3 <- c(\"about\", \"according\", \"adversaries\", \"after\", \"aid\", \"always\", \"apt\", \"asserted\", \"before\", \"being\", \"better\", \"care\", \"choice\", \"common\", \"danger\", \"decide\", \"decides\", \"decided\", \"deciding\", \"degree\", \"during\", \"expense\", \"expenses\", \"extent\", \"follow\", \"follows\", \"followed\", \"following\", \"i\", \"imagine\", \"imagined\", \"intrust\", \"intrusted\", \"intrusting\",\"kind\", \"large\", \"likely\", \"matter\", \"matters\", \"moreover\", \"necessary\", \"necessity\", \"necessities\", \"others\", \"particularly\", \"principle\", \"probability\", \"proper\", \"propriety\", \"provision\", \"provisions\", \"requisite\", \"substance\", \"they\", \"though\", \"truth\", \"truths\", \"us\", \"usage\", \"usages\", \"we\", \"work\", \"works\")\n```\n\nAll together, they list 165 candidate variables, though it works out to be 180 unlemmatized tokens as potential variables for their model.\n\nWe'll concatenate a vector of all their variables into a single vector.\n\n```{r}\nmw_all <- sort(c(mw_group1, mw_group2, mw_group3))\n```\n\n# *The Federalist Papers*\n\nAnd we'll start by setting up our data much like we did for lasso. First, we'll get the metadata...\n\n\n```{r}\nload(\"../data/federalist_meta.rda\")\nload(\"../data/federalist_papers.rda\")\n```\n\n\n```{r}\nfed_meta <- federalist_meta %>%\n  dplyr::select(doc_id, author_id)\n```\n\nWe'll read in the text...\n\n```{r}\nfed_txt <- federalist_papers\n```\n\n# Preparing the Data\n\nNow, we'll tokenize the data.\n\n```{r}\nfed_tokens <- fed_txt %>%\n  corpus() %>%\n  tokens(remove_punct = T, remove_symbols = T, what = \"word\")\n```\n\nAnd create a weighted dfm. The 3rd line preps the column so it can be merged with our metadata. The 4th orders the tokens by their mean frequencies. This isn't necessary here, but can be useful when doing quick subsetting of variables. And the 5th changes the column name for easy joining.\n\n```{r}\nfed_dfm <- fed_tokens %>% dfm() %>% dfm_weight(scheme = \"prop\") %>%\n  convert(to = \"data.frame\") %>%\n  mutate(doc_id = str_remove(doc_id, \".txt$\")) %>%\n  select(doc_id, names(sort(colMeans(.[,-1]), decreasing = TRUE)))\n```\n\nNow let's join the author_id from the metadata.\n\n```{r, message=FALSE}\nfed_dfm <- fed_dfm %>% \n  right_join(fed_meta) %>% \n  dplyr::select(doc_id, author_id, everything())\n```\n\n# Training and testing data\n\nNow we can subset out our training and testing data.\n\n```{r}\ntrain_dfm <- fed_dfm %>% \n  filter(author_id == \"Hamilton\" | author_id == \"Madison\") %>% \n  dplyr::select(doc_id, author_id, all_of(mw_all)) %>%\n  mutate(author_id = factor(author_id)) %>%\n  column_to_rownames(\"doc_id\") %>%\n  as_tibble()\n\n# Note that some R functions have difficulty when column names are same as base function names like 'in' or 'if'.\n# To head off any problems, we'll convert the first letter to upper case.\ncolnames(train_dfm)[-1] <- colnames(train_dfm)[-1] %>% str_to_title()\n\ntest_dfm <- fed_dfm %>% \n  filter(author_id == \"Disputed\") %>% \n  dplyr::select(doc_id, author_id, all_of(mw_all)) %>%\n  column_to_rownames(\"doc_id\") %>%\n  as_tibble()\n\ncolnames(test_dfm)[-1] <- colnames(test_dfm)[-1] %>% str_to_title()\n```\n\n# Create a model\n\nNow let's generate a random forest model. Fist, note that when the training set for the current tree is drawn by sampling with replacement, about one-third of the cases are left out of the sample. This oob (out-of-bag) data is used to get a running unbiased estimate of the classification error as trees are added to the forest. It is also used to get estimates of variable importance.\n\nThus, we don't necessarily need to sample out a validation set of our data, though you certainly can.\n\nWe'll set our seed. And generate a model.\n\n```{r}\nset.seed(123)\nfed_m1 <- randomForest(formula = author_id ~ ., data = train_dfm)\n```\n\n```{r}\nfed_m1\n```\n\nLet's set aside the error rate for a moment, and walk through the prediction... So now we use the model on our test data.\n\n```{r}\npred_fed <- predict(fed_m1, test_dfm)\n```\n\n```{r}\npred_fed %>% knitr::kable(col.names = c(\"Pred. Author\"))\n```\n\nThis is exactly what we predicted using lasso regression! Now let's look quickly at variable importance.\n\nEvery node in the decision trees is a condition on a single feature,  designed to split the dataset into two so that similar response values end up in the same set. The measure based on which (locally) optimal condition is chosen is called impurity. For classification, it is typically either  Gini impurity or information gain/entropy. Thus when training a tree,  it can be computed how much each feature decreases the weighted impurity in a tree. \n\nFor a forest, the impurity decrease from each feature can be averaged and the features are ranked according to this measure. These are the features that are important to our particular model and from our model, we can retrieve using **importance()** these.\n\n```{r}\nimportance(fed_m1) %>% as.data.frame() %>% dplyr::arrange(-MeanDecreaseGini) %>% head() %>% knitr::kable(digits = 2)\n```\n\n# Evalutating and tuning a model\n\nLet's return to our model. We have a 7.69% OOB error rate. There are variety of ways to try to reduce OOB error. One is to tune the random forest model.  And there are only a few parameters that can be adjusted:\n\n* mtry: the number of variables to randomly sample as candidates at each split.\n* ntree: the number of trees.\n* sampsize: the number of samples to train on.\n* nodesize: minimum number of samples within the terminal nodes.\n* maxnodes: maximum number of terminal nodes.\n\nInitial tuning can be done with the **tuneRF()** function. The function will start at a value of mtry that you supply and increase by a certain step  factor until the OOB error stops improving be a specified amount. For example, the below starts with **mtry = 5** and increases by a factor of 2 until the OOB error stops improving by 1%.\n\n```{r}\nset.seed(1234)\ntest <- tuneRF(\n  x          = train_dfm[-1],\n  y          = train_dfm$author_id,\n  ntreeTry   = 500,\n  mtryStart  = 5,\n  stepFactor = 2,\n  improve    = 0.01,\n  trace      = FALSE \n)\n```\n\n\nThe result is close to the default value of features/3 or 180/3. There are other methods turning other parameters. For instructions on how to do those, you can consult tutorials like the one here:\n\n<https://uc-r.github.io/random_forests>\n\n# Dealing with imbalanced data\n\nThere is one particularly imporant issue with our data, which we need to consider. Check the (im)balance in our data.\n\n```{r}\ntrain_dfm %>% group_by(author_id) %>% tally() %>% knitr::kable()\n```\n\nAnd, again, look at the confusion matrix.\n\n```{r}\nfed_m1$confusion %>% knitr::kable( digits = 2)\n```\n\nTo access our default model in more detail, we can use the accuracy function from the **rfUtilities** package. We give the function a vector of the predicted values, and one of the observed values. \n\nToo read more about evaluating the accuracy of classifiers, see here:\n\n<http://gsp.humboldt.edu/olm_2019/courses/GSP_216_Online/lesson6-2/metrics.html>\n\nAnd here:\n\n<https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0222916>\n\n\n```{r}\naccuracy(fed_m1$predicted, train_dfm$author_id) %>% print()\n```\n\nRandom forest is sensitive to unbalanced data. There is an excellent discussion of handling imbalanced data here:\n\n<https://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf>\n\nI will quote the authors here:\n\n> \"For any classifier, there is always a trade off between true positive rate and true negative rate; and the same applies for recall and precision. In the case of learning extremely imbalanced data, quite often the rare class is of great interest.  In many applications, it is desirable to have a classifier that gives high prediction accuracy over the minority class, while maintaining reasonable accuracy for the majority class.\"\n\nThere are a variety of ways to try to achieve this. One is to use the classwt argument. Another is to change the sample sizes.\n\nWe'll create 4 models.\n\n```{r}\nrf1 <- randomForest(author_id~., train_dfm, ntree=500, sampsize=65)\nrf2 <- randomForest(author_id~., train_dfm, ntree=500, sampsize=c(38,14))\nrf3 <- randomForest(author_id~., train_dfm, ntree=500, sampsize=c(25,14))\nrf4 <- randomForest(author_id~., train_dfm, ntree=500, sampsize=c(14,14))\n```\n\nAnd check the OOB error for each.\n\n```{r}\ndata.frame(rf1 = tail(rf1$err.rate[,1], n=1)*100, rf2 = tail(rf2$err.rate[,1], n=1)*100,\n           rf3 = tail(rf3$err.rate[,1], n=1)*100, rf4 = tail(rf4$err.rate[,1], n=1)*100) %>%\n  knitr::kable(digits = 2)\n```\n\nNow, let's compare models over multiple iterations. We'll start by setting up vector for 100 iterations. Next, we'll create an empty vector.\n\n```{r}\nintervals <- c(1:100)\noob_error <- NULL\n```\n\nNow, we'll generate OOB errors for 100 models based on the full training data and populate the vector. This will take a few seconds.\n\n```{r}\nfor(i in intervals){\n  m1 <- randomForest(\n    formula = author_id ~ .,\n    data    = train_dfm\n  )\n  oob <-tail(m1$err.rate[,1], n=1)\n  oob_error[[i]] <- oob*100\n}\n```\n\nLet's look at the maximum, minimum, and mean values.\n\n```{r}\noob_error <- unlist(oob_error)\ndata.frame(oob_max = max(oob_error), oob_min = min(oob_error), mean_oob = mean(oob_error)) %>%\n  knitr::kable(digits = 2)\n```\n\nNow, we'll reset the vector. And populate it with values based on the model with adjusted sampling.\n\n```{r}\noob_error <- NULL\n\nfor(i in intervals){\n  m1 <- randomForest(\n    formula  = author_id ~ .,\n    data     = train_dfm,\n    ntree    = 500, \n    sampsize = c(14,14)\n  )\n  oob <-tail(m1$err.rate[,1], n=1)\n  oob_error[[i]] <- oob*100\n}\n\n```\n\nAgain, we'll check our OOB error.\n \n```{r}\noob_error <- unlist(oob_error)\ndata.frame(oob_max = max(oob_error), oob_min = min(oob_error), mean_oob = mean(oob_error)) %>%\n  knitr::kable(digits = 2)\n```\n\nFinally, let's check similar model ajustments agaist a validcation set. For this, as we did with lasso, we'll use some handy functions from the **rsample** package. First, we make an 80/20 split. From that we create a new and smaller training set.\n\n```{r}\nset.seed(123)\nvalid_split <- initial_split(train_dfm, .85)\ntrain_dfm_v2 <- analysis(valid_split)\n```\n\nAnd check the breakdown of our authors.\n\n```{r}\ntrain_dfm_v2 %>% group_by(author_id) %>% tally() %>% knitr::kable()\n```\n\nAnd a validation set.\n\n```{r}\ntrain_valid <- assessment(valid_split)\n```\n\nNow we can create a new model with balanced sampling.\n\n```{r}\nrf_valid <- randomForest(\n  formula  = author_id ~ .,\n  data     = train_dfm_v2,\n  ntree    = 500, \n  sampsize = c(12,12)\n)\n```\n\nMake our predictions on the validation set.\n\n```{r}\npred_randomForest <- predict(rf_valid, train_valid)\n```\n\nLook at what they actually are...\n\n```{r}\ntable(pred=pred_randomForest, true=train_valid$author_id) %>% knitr::kable()\n```\n\nAnd calculate our model accuracy...\n\n```{r}\nmean(pred_randomForest == train_valid$author_id) %>% knitr::kable(col.names = \"Accuracy\", digits = 2)\n```\n\nFinally we can make predictions with our adjusted model.\n\n```{r}\npred_fed <- predict(rf4, test_dfm)\n```\n\nAnd view those predicitons..\n\n```{r}\npred_fed %>% knitr::kable(col.names = \"Pred. Author\")\n```\n\nOkay, we've repeated this task a couple of times now. Both times, we've taken advantage of Mosteller & Wallace's filtering of variables. This has made life much easier for us. But what if we didn't have their candidate words to feed into our models? What if we were starting from the dfm with 8765 words? How would you tackle variable selection, knowing that random forests don't do well with highly zero-skewed variables. So just feeding the entire dfm into random forest isn't going to be the best solution.\n\nOn the one hand, more common words are certainly good candidates for a model. On the other, you don't want to throw out less frequent words that might be highly discriminatory....\n","srcMarkdownNoYaml":"\n\n```{r, include=FALSE}\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#>\",\n  tidy.opts=list(width.cutoff=70),\n  tidy=TRUE\n)\n```\n\n# Lab 14: Random Forest {-}\n\nWe're going to repeat the Federalist classification problem. Rather than a loasso regression model, however, this time we'll be constructing a random forest classification model.\n\nBroadly, random forests generate many classification trees. Each tree gives a classification, and we say the tree \"votes\" for that class. The forest chooses the classification having the most votes (over all the trees in the forest).  \n\nIf you're unfamiliar with random forests, you can (and should) read more here:\n\n<https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm>\n\n\n```{r setup, message = FALSE, error=FALSE, warning=FALSE}\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(rsample)\nlibrary(randomForest) # basic implementation\nlibrary(rfUtilities) # utilities for performance evaluation\n```\n\n# Variables\n\nAs with the Lab on lasso regression, we'll start with Mosteller & Wallace's lists of tokens.\n\nTheir first group contains 70 tokens...\n\n```{r group_1}\nmw_group1 <- c(\"a\", \"all\", \"also\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"been\", \"but\", \"by\", \"can\", \"do\", \"down\", \"even\", \"every\", \"for\", \"from\", \"had\", \"has\", \"have\", \"her\", \"his\", \"if\", \"in\", \"into\", \"is\", \"it\",  \"its\", \"may\", \"more\", \"must\", \"my\", \"no\", \"not\", \"now\", \"of\", \"on\", \"one\", \"only\", \"or\", \"our\", \"shall\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"the\", \"their\", \"then\", \"there\", \"things\", \"this\", \"to\", \"up\", \"upon\", \"was\", \"were\", \"what\", \"when\", \"which\", \"who\", \"will\", \"with\", \"would\", \"your\")\n```\n\n\nTheir second an addional 47...\n\n```{r group_2}\nmw_group2 <- c(\"affect\", \"again\", \"although\", \"among\", \"another\", \"because\", \"between\", \"both\", \"city\", \"commonly\", \"consequently\", \"considerable\", \"contribute\", \"defensive\", \"destruction\", \"did\", \"direction\", \"disgracing\", \"either\", \"enough\", \"fortune\", \"function\", \"himself\", \"innovation\", \"join\", \"language\", \"most\", \"nor\", \"offensive\", \"often\", \"pass\", \"perhaps\", \"rapid\", \"same\", \"second\", \"still\", \"those\", \"throughout\", \"under\", \"vigor\", \"violate\", \"violence\", \"voice\", \"where\", \"whether\", \"while\", \"whilst\")\n```\n\nAnd their third another 48 (though they identify some by lemmas and another \"expence\" doesn't appear in our data, possibly because of later editing done in our particular edition)...\n\n```{r gorup_3}\nmw_group3 <- c(\"about\", \"according\", \"adversaries\", \"after\", \"aid\", \"always\", \"apt\", \"asserted\", \"before\", \"being\", \"better\", \"care\", \"choice\", \"common\", \"danger\", \"decide\", \"decides\", \"decided\", \"deciding\", \"degree\", \"during\", \"expense\", \"expenses\", \"extent\", \"follow\", \"follows\", \"followed\", \"following\", \"i\", \"imagine\", \"imagined\", \"intrust\", \"intrusted\", \"intrusting\",\"kind\", \"large\", \"likely\", \"matter\", \"matters\", \"moreover\", \"necessary\", \"necessity\", \"necessities\", \"others\", \"particularly\", \"principle\", \"probability\", \"proper\", \"propriety\", \"provision\", \"provisions\", \"requisite\", \"substance\", \"they\", \"though\", \"truth\", \"truths\", \"us\", \"usage\", \"usages\", \"we\", \"work\", \"works\")\n```\n\nAll together, they list 165 candidate variables, though it works out to be 180 unlemmatized tokens as potential variables for their model.\n\nWe'll concatenate a vector of all their variables into a single vector.\n\n```{r}\nmw_all <- sort(c(mw_group1, mw_group2, mw_group3))\n```\n\n# *The Federalist Papers*\n\nAnd we'll start by setting up our data much like we did for lasso. First, we'll get the metadata...\n\n\n```{r}\nload(\"../data/federalist_meta.rda\")\nload(\"../data/federalist_papers.rda\")\n```\n\n\n```{r}\nfed_meta <- federalist_meta %>%\n  dplyr::select(doc_id, author_id)\n```\n\nWe'll read in the text...\n\n```{r}\nfed_txt <- federalist_papers\n```\n\n# Preparing the Data\n\nNow, we'll tokenize the data.\n\n```{r}\nfed_tokens <- fed_txt %>%\n  corpus() %>%\n  tokens(remove_punct = T, remove_symbols = T, what = \"word\")\n```\n\nAnd create a weighted dfm. The 3rd line preps the column so it can be merged with our metadata. The 4th orders the tokens by their mean frequencies. This isn't necessary here, but can be useful when doing quick subsetting of variables. And the 5th changes the column name for easy joining.\n\n```{r}\nfed_dfm <- fed_tokens %>% dfm() %>% dfm_weight(scheme = \"prop\") %>%\n  convert(to = \"data.frame\") %>%\n  mutate(doc_id = str_remove(doc_id, \".txt$\")) %>%\n  select(doc_id, names(sort(colMeans(.[,-1]), decreasing = TRUE)))\n```\n\nNow let's join the author_id from the metadata.\n\n```{r, message=FALSE}\nfed_dfm <- fed_dfm %>% \n  right_join(fed_meta) %>% \n  dplyr::select(doc_id, author_id, everything())\n```\n\n# Training and testing data\n\nNow we can subset out our training and testing data.\n\n```{r}\ntrain_dfm <- fed_dfm %>% \n  filter(author_id == \"Hamilton\" | author_id == \"Madison\") %>% \n  dplyr::select(doc_id, author_id, all_of(mw_all)) %>%\n  mutate(author_id = factor(author_id)) %>%\n  column_to_rownames(\"doc_id\") %>%\n  as_tibble()\n\n# Note that some R functions have difficulty when column names are same as base function names like 'in' or 'if'.\n# To head off any problems, we'll convert the first letter to upper case.\ncolnames(train_dfm)[-1] <- colnames(train_dfm)[-1] %>% str_to_title()\n\ntest_dfm <- fed_dfm %>% \n  filter(author_id == \"Disputed\") %>% \n  dplyr::select(doc_id, author_id, all_of(mw_all)) %>%\n  column_to_rownames(\"doc_id\") %>%\n  as_tibble()\n\ncolnames(test_dfm)[-1] <- colnames(test_dfm)[-1] %>% str_to_title()\n```\n\n# Create a model\n\nNow let's generate a random forest model. Fist, note that when the training set for the current tree is drawn by sampling with replacement, about one-third of the cases are left out of the sample. This oob (out-of-bag) data is used to get a running unbiased estimate of the classification error as trees are added to the forest. It is also used to get estimates of variable importance.\n\nThus, we don't necessarily need to sample out a validation set of our data, though you certainly can.\n\nWe'll set our seed. And generate a model.\n\n```{r}\nset.seed(123)\nfed_m1 <- randomForest(formula = author_id ~ ., data = train_dfm)\n```\n\n```{r}\nfed_m1\n```\n\nLet's set aside the error rate for a moment, and walk through the prediction... So now we use the model on our test data.\n\n```{r}\npred_fed <- predict(fed_m1, test_dfm)\n```\n\n```{r}\npred_fed %>% knitr::kable(col.names = c(\"Pred. Author\"))\n```\n\nThis is exactly what we predicted using lasso regression! Now let's look quickly at variable importance.\n\nEvery node in the decision trees is a condition on a single feature,  designed to split the dataset into two so that similar response values end up in the same set. The measure based on which (locally) optimal condition is chosen is called impurity. For classification, it is typically either  Gini impurity or information gain/entropy. Thus when training a tree,  it can be computed how much each feature decreases the weighted impurity in a tree. \n\nFor a forest, the impurity decrease from each feature can be averaged and the features are ranked according to this measure. These are the features that are important to our particular model and from our model, we can retrieve using **importance()** these.\n\n```{r}\nimportance(fed_m1) %>% as.data.frame() %>% dplyr::arrange(-MeanDecreaseGini) %>% head() %>% knitr::kable(digits = 2)\n```\n\n# Evalutating and tuning a model\n\nLet's return to our model. We have a 7.69% OOB error rate. There are variety of ways to try to reduce OOB error. One is to tune the random forest model.  And there are only a few parameters that can be adjusted:\n\n* mtry: the number of variables to randomly sample as candidates at each split.\n* ntree: the number of trees.\n* sampsize: the number of samples to train on.\n* nodesize: minimum number of samples within the terminal nodes.\n* maxnodes: maximum number of terminal nodes.\n\nInitial tuning can be done with the **tuneRF()** function. The function will start at a value of mtry that you supply and increase by a certain step  factor until the OOB error stops improving be a specified amount. For example, the below starts with **mtry = 5** and increases by a factor of 2 until the OOB error stops improving by 1%.\n\n```{r}\nset.seed(1234)\ntest <- tuneRF(\n  x          = train_dfm[-1],\n  y          = train_dfm$author_id,\n  ntreeTry   = 500,\n  mtryStart  = 5,\n  stepFactor = 2,\n  improve    = 0.01,\n  trace      = FALSE \n)\n```\n\n\nThe result is close to the default value of features/3 or 180/3. There are other methods turning other parameters. For instructions on how to do those, you can consult tutorials like the one here:\n\n<https://uc-r.github.io/random_forests>\n\n# Dealing with imbalanced data\n\nThere is one particularly imporant issue with our data, which we need to consider. Check the (im)balance in our data.\n\n```{r}\ntrain_dfm %>% group_by(author_id) %>% tally() %>% knitr::kable()\n```\n\nAnd, again, look at the confusion matrix.\n\n```{r}\nfed_m1$confusion %>% knitr::kable( digits = 2)\n```\n\nTo access our default model in more detail, we can use the accuracy function from the **rfUtilities** package. We give the function a vector of the predicted values, and one of the observed values. \n\nToo read more about evaluating the accuracy of classifiers, see here:\n\n<http://gsp.humboldt.edu/olm_2019/courses/GSP_216_Online/lesson6-2/metrics.html>\n\nAnd here:\n\n<https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0222916>\n\n\n```{r}\naccuracy(fed_m1$predicted, train_dfm$author_id) %>% print()\n```\n\nRandom forest is sensitive to unbalanced data. There is an excellent discussion of handling imbalanced data here:\n\n<https://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf>\n\nI will quote the authors here:\n\n> \"For any classifier, there is always a trade off between true positive rate and true negative rate; and the same applies for recall and precision. In the case of learning extremely imbalanced data, quite often the rare class is of great interest.  In many applications, it is desirable to have a classifier that gives high prediction accuracy over the minority class, while maintaining reasonable accuracy for the majority class.\"\n\nThere are a variety of ways to try to achieve this. One is to use the classwt argument. Another is to change the sample sizes.\n\nWe'll create 4 models.\n\n```{r}\nrf1 <- randomForest(author_id~., train_dfm, ntree=500, sampsize=65)\nrf2 <- randomForest(author_id~., train_dfm, ntree=500, sampsize=c(38,14))\nrf3 <- randomForest(author_id~., train_dfm, ntree=500, sampsize=c(25,14))\nrf4 <- randomForest(author_id~., train_dfm, ntree=500, sampsize=c(14,14))\n```\n\nAnd check the OOB error for each.\n\n```{r}\ndata.frame(rf1 = tail(rf1$err.rate[,1], n=1)*100, rf2 = tail(rf2$err.rate[,1], n=1)*100,\n           rf3 = tail(rf3$err.rate[,1], n=1)*100, rf4 = tail(rf4$err.rate[,1], n=1)*100) %>%\n  knitr::kable(digits = 2)\n```\n\nNow, let's compare models over multiple iterations. We'll start by setting up vector for 100 iterations. Next, we'll create an empty vector.\n\n```{r}\nintervals <- c(1:100)\noob_error <- NULL\n```\n\nNow, we'll generate OOB errors for 100 models based on the full training data and populate the vector. This will take a few seconds.\n\n```{r}\nfor(i in intervals){\n  m1 <- randomForest(\n    formula = author_id ~ .,\n    data    = train_dfm\n  )\n  oob <-tail(m1$err.rate[,1], n=1)\n  oob_error[[i]] <- oob*100\n}\n```\n\nLet's look at the maximum, minimum, and mean values.\n\n```{r}\noob_error <- unlist(oob_error)\ndata.frame(oob_max = max(oob_error), oob_min = min(oob_error), mean_oob = mean(oob_error)) %>%\n  knitr::kable(digits = 2)\n```\n\nNow, we'll reset the vector. And populate it with values based on the model with adjusted sampling.\n\n```{r}\noob_error <- NULL\n\nfor(i in intervals){\n  m1 <- randomForest(\n    formula  = author_id ~ .,\n    data     = train_dfm,\n    ntree    = 500, \n    sampsize = c(14,14)\n  )\n  oob <-tail(m1$err.rate[,1], n=1)\n  oob_error[[i]] <- oob*100\n}\n\n```\n\nAgain, we'll check our OOB error.\n \n```{r}\noob_error <- unlist(oob_error)\ndata.frame(oob_max = max(oob_error), oob_min = min(oob_error), mean_oob = mean(oob_error)) %>%\n  knitr::kable(digits = 2)\n```\n\nFinally, let's check similar model ajustments agaist a validcation set. For this, as we did with lasso, we'll use some handy functions from the **rsample** package. First, we make an 80/20 split. From that we create a new and smaller training set.\n\n```{r}\nset.seed(123)\nvalid_split <- initial_split(train_dfm, .85)\ntrain_dfm_v2 <- analysis(valid_split)\n```\n\nAnd check the breakdown of our authors.\n\n```{r}\ntrain_dfm_v2 %>% group_by(author_id) %>% tally() %>% knitr::kable()\n```\n\nAnd a validation set.\n\n```{r}\ntrain_valid <- assessment(valid_split)\n```\n\nNow we can create a new model with balanced sampling.\n\n```{r}\nrf_valid <- randomForest(\n  formula  = author_id ~ .,\n  data     = train_dfm_v2,\n  ntree    = 500, \n  sampsize = c(12,12)\n)\n```\n\nMake our predictions on the validation set.\n\n```{r}\npred_randomForest <- predict(rf_valid, train_valid)\n```\n\nLook at what they actually are...\n\n```{r}\ntable(pred=pred_randomForest, true=train_valid$author_id) %>% knitr::kable()\n```\n\nAnd calculate our model accuracy...\n\n```{r}\nmean(pred_randomForest == train_valid$author_id) %>% knitr::kable(col.names = \"Accuracy\", digits = 2)\n```\n\nFinally we can make predictions with our adjusted model.\n\n```{r}\npred_fed <- predict(rf4, test_dfm)\n```\n\nAnd view those predicitons..\n\n```{r}\npred_fed %>% knitr::kable(col.names = \"Pred. Author\")\n```\n\nOkay, we've repeated this task a couple of times now. Both times, we've taken advantage of Mosteller & Wallace's filtering of variables. This has made life much easier for us. But what if we didn't have their candidate words to feed into our models? What if we were starting from the dfm with 8765 words? How would you tackle variable selection, knowing that random forests don't do well with highly zero-skewed variables. So just feeding the entire dfm into random forest isn't going to be the best solution.\n\nOn the one hand, more common words are certainly good candidates for a model. On the other, you don't want to throw out less frequent words that might be highly discriminatory....\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":{"pdf_document":{"fig_caption":"yes","number_sections":true}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"Lab_14.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.549","editor":"visual","title":"Lab 14","author":"My Name","header-includes":[""]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":[]}