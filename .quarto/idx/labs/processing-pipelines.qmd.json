{"title":"Tokenizing with quanteda","markdown":{"headingText":"Tokenizing with quanteda","containsRefs":false,"markdown":"\nProcessing pipelines\n\n\nIn the previous lab, we did some back-of-the-napkin text processing. In that lab, you were encouraged to think about what exactly is happening when you split a text into tokens and convert those into counts.\n\nWe'll build on that foundational work, but let an R package [**quanteda**](https://quanteda.io/) do some of the heavy lifting for us. So let's load our packages:\n\n::: {.callout-note}\nWhy use **quanteda** rather than something like **tidytext**? Quite simply processing speed. While most packages are built around R functions and structures, **quanteda** does its processing in C+++ under the hood. Compiled languages like C+++ and Rust are much more efficient for string processing. This is also why [**polars**](https://pola.rs/) is useful for processing large dataframes.\n:::\n\n```{r}\n#| message: false\n#| error: false\n#| warning: false\n\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(tidyverse)\nlibrary(gt)\n```\n\nLoad in some useful functions:\n\n```{r}\nsource(\"../R/helper_functions.R\")\n```\n\nAnd again, we'll start with the first sentence from *A Tale of Two Cities*.\n\n```{r}\n\ntotc_txt <- \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair.\"\n```\n\n## Create a corpus\n\nThe first step is to create a corpus object:\n\n```{r}\n\ntotc_corpus <- corpus(totc_txt)\n```\n\nAnd see what we have:\n\n```{r}\n#| code-fold: true\n#| tbl-cap: \"Summary of a corpus.\"\n\ntotc_corpus |>\n  summary() |>\n  gt()\n```\n\nNote that if we had more than 1 document, we would get a count of how many documents in which the token appear, and that we can assign documents to grouping variable. This will become useful later.\n\n## Tokenize the corpus\n\n```{r}\n\ntotc_tkns <- tokens(totc_corpus, what = \"word\", remove_punct = TRUE)\n```\n\n## Create a document-feature matrix (dfm)\n\n```{r create_dfm}\n\ntotc_dfm <- dfm(totc_tkns)\n```\n\nA **dfm** is an important data structure to understand, as it often serves as the foundation for all kinds of downstream statistical processing. It is a table with rows for documents (or observations) and columns for tokens (or variables)\n\n```{r}\n#| code-fold: true\n#| tbl-cap: \"Part of a document-feature matrix.\"\n\ntotc_dfm |>\n  convert(to = \"data.frame\") |>\n  dplyr::select(1:12) |>\n  gt()\n```\n\n## And count our tokens\n\n```{r}\n#| code-fold: true\n#| tbl-cap: \"Token counts of sample sentence.\"\n\ntotc_dfm |>\n  textstat_frequency() |>\n  gt()\n```\n\n## Using pipes to expidite the process\n\nThis time, we will change **remove_punct** to **FALSE**.\n\n```{r}\ntotc_freq <- totc_corpus %>%\n  tokens(what = \"word\", remove_punct = FALSE) %>%\n  dfm() %>%\n  textstat_frequency()\n```\n \n```{r}\n#| code-fold: true\n#| tbl-cap: \"Token counts of sample sentence.\"\n\ntotc_freq |>\n  gt()\n```\n\n## Tokenizing options\n\nIn the previous lab, you were asked to consider the questions: What counts as a token/word? And how do you tell the computer to count what you want?\n\nAs the above code block suggest, the `tokens()` function in [**quanteda**](http://quanteda.io/reference/tokens.html) gives you some measure on control.\n\nWe'll read in a more complex string:\n\n```{r}\ntext_2 <- \"Jane Austen was not credited as the author of 'Pride and Prejudice.' In 1813, the title page simply read \\\"by the author of Sense and Sensibility.\\\" It wasn't until after Austen's death that her identity was revealed. #MentalFlossBookClub with @HowLifeUnfolds #15Pages https://pbs.twimg.com/media/EBOUqbfWwAABEoj.jpg\"\n```\n\nAnd process it as we did earlier.\n\n```{r}\ntext_2_freq <- text_2 %>%\n  corpus() %>%\n  tokens(what = \"word\", remove_punct = TRUE) %>%\n  dfm() %>%\n  textstat_frequency()\n```\n \n```{r}\n#| code-fold: true\n#| tbl-cap: \"Token counts of sample Tweet\"\n\ntext_2_freq |>\n  gt()\n```\n\nNote that in addition to various logical \"remove\" arguments (**remove_punct**, **remove_symbols**, etc.), the `tokens()` function has a **what** argument. The default, \"word\", is \"smarter\", but also slower. Another option is \"fastestword\", which splits at spaces.\n\n```{r}\n\ntext_2_freq <- text_2 %>%\n  corpus() %>%\n  tokens(what = \"fastestword\", remove_punct = TRUE, remove_url = TRUE) %>%\n  dfm() %>%\n  textstat_frequency()  %>%\n  as_tibble() %>%\n  dplyr::select(feature, frequency)\n```\n \n```{r}\n#| code-fold: true\n#| tbl-cap: \"Token counts of sample Tweet\"\n\ntext_2_freq |>\n  gt()\n```\n\nThis, of course, makes no difference with just a few tokens, but does if you're trying to process millions.\n\nAlso note that we've used the `select()` function to choose specific columns.\n\n## Pre-processing\n\nAn alternative to making tokenizing decisions inside the tokenizing process, you can process the text before tokenizing using functions for manipulating strings in **stringr**, **stringi**, **textclean**, or base R (like **grep( )**). Some common and convenient transformations are wrapped in a function called `preprocess_text( )`\n\n```{r pre_process}\ntext_2_freq <- text_2 %>%\n  preprocess_text() %>%\n  corpus() %>%\n  tokens(what = \"fastestword\") %>%\n  dfm() %>%\n  textstat_frequency() %>%\n  as_tibble() %>%\n  dplyr::select(feature, frequency) %>%\n  rename(Token = feature, AF = frequency) %>%\n  mutate(New = NA)\n```\n \n```{r}\n#| code-fold: true\n#| tbl-cap: \"Token counts of sample Tweet\"\n\ntext_2_freq |>\n  gt()\n```\n\nNote how the default arguments treat negation and possessive markers. As with the `tokens ()` function, many of these (options)[http://htmlpreview.github.io/?https://raw.githubusercontent.com/browndw/quanteda.extras/main/vignettes/preprocess_introduction.html] are logical.\n\nNote, too, that we've renamed the columns and added a new one using `mutate()`. \n\n::: callout-important\n## Pause for Lab Set Question\n\nComplete [Task 1 in Lab Set 1](../lab_sets/LabSet_01.qmd#tokenizing-with-quanteda).\n:::\n\n## Creating a corpus composition table\n\nWhenever you report the results of a corpus-based analysis, it is best practice to include a table that summarizes the composition of your corpus (or corpora) and any relevant variables. Most often this would include token counts aggregated by relevant categorical variables and a row of totals.\n\n## Adding a grouping variable\n\nWe have 2 short texts (one from fiction and one from Twitter). Let's first combine them into a single corpus. First, a data frame is created that has 2 columns (**doc_id** and **text**). Then, the **text** column is passed to the `preprocess_text()` function before creating the corpus.\n\n```{r}\ncomb_corpus <-   data.frame(doc_id = c(\"text_1\", \"text_2\"), text = c(totc_txt, text_2)) %>%\n  mutate(text = preprocess_text(text)) %>%\n  corpus()\n```\n\nNext well assign a grouping variable using `docvars()`. In later labs, we'll use a similar process to assign variables from tables of metadata.\n\n```{r}\ndocvars(comb_corpus) <- data.frame(text_type = c(\"Fiction\", \"Twitter\"))\n```\n\nNow we can tokenize.\n\n```{r}\ncomb_tkns <- comb_corpus %>%\n  tokens(what = \"fastestword\")\n```\n\nOnce we have done this, we can use that grouping variable to manipulate the data in a variety of ways. We could use `dfm_group()` to aggregate by group instead of individual text. (Though because we have only 2 texts here, it amounts to the same thing.) \n\n```{r}\ncomb_dfm <- dfm(comb_tkns) %>% \n  dfm_group(groups = text_type)\n\ncorpus_comp <- ntoken(comb_dfm) %>%\n  data.frame(frequency = .) %>%\n  rownames_to_column(\"group\") %>%\n  group_by(group) %>%\n  summarize(Texts = n(),\n            Tokens = sum(frequency))\n```\n\n\n```{r}\n#| code-fold: true\n#| label: tbl-corpus\n#| tbl-cap: \"Composition of corpus.\"\n\ncorpus_comp |> \n  gt() |>\n  fmt_integer() |>\n  cols_label(\n    group = md(\"**Text Type**\"),\n    Texts = md(\"**Texts**\"),\n    Tokens = md(\"**Tokens**\")\n  ) |>\n  grand_summary_rows(\n    columns = c(Texts, Tokens),\n    fns = list(\n      Total ~ sum(.)\n    ) ,\n    fmt = ~ fmt_integer(.)\n    )\n```\n\n::: callout-important\n## Pause for Lab Set Question\n\nComplete [Task 2 in Lab Set 1](../lab_sets/LabSet_01.qmd#tokenizing-with-quanteda).\n:::\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"processing-pipelines.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.549","editor":"source"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":[]}