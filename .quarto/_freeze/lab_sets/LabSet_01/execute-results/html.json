{
  "hash": "502a6c3feabef278299ebdbdab04a8c0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab Set 1\"\nauthor: \"My Name\"\noutput:\n  pdf_document:\n    fig_caption: yes\n    number_sections: true\nheader-includes:\n  - |\n    ```{=latex}\n    \\usepackage{fvextra}\n    \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{\n      breaksymbolleft={}, \n      showspaces = false,\n      showtabs = false,\n      breaklines,\n      commandchars=\\\\\\{\\}\n    }\n    ```\n---\n\n\n# Lab Set 1\n\n::: callout-note\nThe preview of this lab set is rendered in HTML. However, all assignments must be rendered in PDF for submission on Canvas. The the **textstat_tools** repository is already set up to do this for you. Be sure to [follow the directions](https://quarto.org/docs/output-formats/pdf-basics.html) including the installation of **tinytex**.\n:::\n\n## Questions related to the first week discussions\n\n### Task 1\n\nConsider the results of the preliminary study described in slides 65-70:\n\nIf you were working on this project, what would you suggest the team do next? In other words, what limitations do you see in the results of this initial study? What might be done to increase its reliability? Or its generalizability? And what potential challenges do you foresee in applying your suggestions?\n\nDiscuss with a couple of your neighbors and write your response in a short paragraph.\n\n> Your response:\n\n### Task 2\n\nAnswer the the following question:\n\nWhat interests you about the quantitative analysis of text?\n\n> Your response:\n\n## Mosteller & Wallace\n\n### Task 1\n\nMosteller and Wallace talk about a \"little book of decisions\" -- a record of the choices that they made in carrying out their project. Most data-driven projects require similarly complex choices. In defending them, sometimes in defending them, we check them. In making choice *x*, have we unduly influenced the result?\n\nPick one of Mosteller and Wallace's decisions and describe how you might check whether it affecting their findings.\n\n> Your response\n\n### Task 2\n\nOur model predicts all but 55 were written by Madison. Our model is not particularly confident about that result. This hews pretty closely to Mosteller & Wallace's findings, through they come down (sort of) on the side of Madison for 55. However, they also acknowledge that the evidence is weak and not very convincing.\n\nGive at least 3 possible factors that might explain that low probability?\n\n> Your response\n\n## NLP Basics\n\n### Task 1\n\n#### What counts as a token?\n\nThese choices are important. To carry our any statistical analysis on texts, we radically reorganize texts into counts. Precisely how we choose to do that--the decisions we make in exactly **what** to count--affects everything else downstream.\n\nSo let's look at a chunk of text that is a little more complicated than the example from *A Tale of Two Cities*. Consider the following text:\n\n> In spite of some problems, we saw a 35% uptick in our user-base in the U.S. But that's still a lot fewer than we had last year. We'd like to get that number closer to what we've experienced in the U.K.--something close to 3 million users.\n\nYou are a member of team tasked with tokenizing 20,000 texts that are similar to this one. A member of the suggests using a regular expression that splits on word boundaries: `\\\\b` using the `str_split()` function, try splitting the example string above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# your code goes here\n```\n:::\n\n\nBased on the result, do you think the suggested strategy is a good one? Why or why not?\n\n> Your response\n\n### Task 2\n\nBriefly describe the tokens that you want to output\n\n> Your response:\n\nWhat are some kinds of tokens that you think are particularly challenging to deal with (e.g., hyphenated words, contractions, abbreviations, etc.)?\n\n> Your response:\n\n## Tokenizing with quanteda\n\n### Task 1\n\nUse the following text:\n\n> \"The more I dove in, though, the less I cared. I watched BTS perform their 2018 anthem \"Idol\" on The Tonight Show and wondered how their lungs didn't explode from exertion. I watched the sumptuous short film for their 2016 hit \"Blood, Sweat, and Tears\" and couldn't tell whether I was more impressed by the choreography or the high-concept storytelling. And I was entranced by the video for \"Spring Day,\" with its dreamlike cinematography and references to Ursula K. Le Guin and Bong Joon-ho's film Snowpiercer. When I learned that the video is often interpreted as a tribute to the school-age victims of 2014's Sewol ferry disaster, I replayed it and cried.\"\n\nIn the code chunk below, construct a pipeline that:\n\n-   tokenizes the text\n-   creates a corpus object\n-   creates a dfm\n-   generates a frequency count of tokens\n-   uses the **mutate()** function to add a **RF** column (for \"relative frequency\") to the data frame.\n\nHint: Relative frequency (or normalized frequency) just takes the frequency, divides it by total number of tokens/words, and multiplies by a normalizing factor (e.g., by 100 for percent of tokens).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# your code goes here\n```\n:::\n\n\nAnd report the results in a gt table:\n\n\n::: {.cell}\n\n:::\n\n\n### Task 2\n\nData from Lab 02:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(quanteda)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"../R/helper_functions.R\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntotc_txt <- \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair.\"\n\ntext_2 <- \"Jane Austen was not credited as the author of 'Pride and Prejudice.' In 1813, the title page simply read \\\"by the author of Sense and Sensibility.\\\" It wasn't until after Austen's death that her identity was revealed. #MentalFlossBookClub with @HowLifeUnfolds #15Pages https://pbs.twimg.com/media/EBOUqbfWwAABEoj.jpg\"\n\ncomb_corpus <-   data.frame(doc_id = c(\"text_1\", \"text_2\"), text = c(totc_txt, text_2)) %>%\n  mutate(text = preprocess_text(text)) %>%\n  corpus()\n\ndocvars(comb_corpus) <- data.frame(text_type = c(\"Fiction\", \"Twitter\"))\n\ncomb_tkns <- comb_corpus %>%\n  tokens(what = \"fastestword\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# structure 1:\ncomb_dfm <- dfm(comb_tkns) %>% dfm_group(groups = text_type)\ncomb_freq <- dfm(comb_tkns) %>% quanteda.textstats::textstat_frequency(groups = text_type)\n\n# structure 2:\ncomb_ntoken <- data.frame(\"Tokens\" = ntoken(comb_tkns), docvars(comb_tkns))\n```\n:::\n\n\nUse one of these 2 data structures (**comb_freq** or **comb_ntoken**) to make a corpus composition table. It should have 2 columns (one for \"Text Type\" and the other for \"Tokens\") and 3 rows (\"Fiction\", \"Twitter\" and \"Total\"). And report the results in a gt table.\n\n::: callout-tip\n## Aggregating across columns\n\nUse the gt function **grand_summary_rows()** [to create a count of totals or other measures](https://gt.rstudio.com/reference/grand_summary_rows.html).\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# your code for a gt table goes here\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}