{
  "hash": "39340c2a8b4166170ba343269c405b47",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab Set 2\"\nauthor: \"My Name\"\ndate: last-modified\nformat:\n  pdf:\n    number-sections: true\ninclude-in-header: \n  text: |\n    \\usepackage{fvextra}\n    \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n\n---\n\n\n# Lab Set 2\n\n## Distributions\n\n### Task 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"../data/sample_corpus.rda\")\nsource(\"../R/dispersion_functions.R\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsc_tokens <- sample_corpus %>%\n  corpus() %>%\n  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, what = \"word\") %>%\n  tokens_tolower()\n\nsc_dfm <- sc_tokens %>%\n  dfm()\n\nsc_freq <- sc_dfm %>%\n  textstat_frequency() %>%\n  mutate(RF = (frequency/sum(frequency))*1000000)\n```\n:::\n\n\nPlot a histogram (or histograms) for the the 1st, 10th, and 100th most frequent tokens in the sample corpus.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# your code goes here\n```\n:::\n\n\nWhat do you notice (or what conclusions can you draw) from the plots you've generated about the distributions of tokens as their frequency decreases?\n\n> Your response\n\n### Task 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthe <- dispersions_token(sc_dfm, \"the\") %>% unlist()\ndata <- dispersions_token(sc_dfm, \"data\") %>% unlist()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nthe['Deviation of proportions DP']\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDeviation of proportions DP \n                  0.1388907 \n```\n\n\n:::\n\n```{.r .cell-code}\ndata['Deviation of proportions DP']\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDeviation of proportions DP \n                   0.845857 \n```\n\n\n:::\n:::\n\n\n\nWhat do you note about the difference in the Deviation of Proportions for *the* vs. *data*?\n\n> Your response\n\n### Task 3\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsc_ft <- frequency_table(sc_tokens)\n```\n:::\n\n\nWhich token is the most frequent? The most dispersed?\n\n> Your response\n\nWrite a sentence or two reporting the frequencies and dispersions of *the* and *data* fowling the examples on page 53 of Brezina:\n\n> Your response\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sc_freq %>% filter(rank < 101), aes(x = rank, y = frequency)) +\n  geom_point(shape = 1, alpha = .5) +\n  theme_classic() +\n  ylab(\"Absolute frequency\") +\n  xlab(\"Rank\")\n```\n\n::: {.cell-output-display}\n![Token rank vs. frequency.](LabSet_02_files/figure-pdf/unnamed-chunk-8-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nThe relationship you're seeing between the rank of a token and it's frequency holds true for almost any corpus and is referred to as **Zipf's Law** (see Brezina pg. 44).\n\n### Task 4\n\nDescribe at least one statistical and one methodological implication of what the plot is illustrating.\n\n> Your response\n\n## Collocations and association measures\n\n### Task 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"../R/helper_functions.R\")\nsource(\"../R/utility_functions.R\")\nsource(\"../R/collocation_functions.R\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsc_tokens <- sample_corpus %>%\n  mutate(text = preprocess_text(text)) %>%\n  corpus() %>%\n  tokens(what=\"fastestword\", remove_numbers=TRUE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmoney_collocations <- collocates_by_MI(sc_tokens, \"money\")\ntime_collocations <- collocates_by_MI(sc_tokens, \"time\")\n```\n:::\n\n\nReport the collocations of *time* and *money* in 2 or 3 sentences following the conventions described in Brezina (pg. 75).\n\n> Your response\n\n### Task 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntc <- time_collocations %>% filter(col_freq >= 5 & MI_1 >= 5)\nmc <- money_collocations %>% filter(col_freq >= 5 & MI_1 >= 5)\nnet <- col_network(tc, mc)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggraph)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggraph(net, weight = link_weight, layout = \"stress\") + \n  geom_edge_link(color = \"gray80\", alpha = .75) + \n  geom_node_point(aes(alpha = node_weight, size = 3, color = n_intersects)) +\n  geom_node_text(aes(label = label), repel = T, size = 3) +\n  scale_alpha(range = c(0.2, 0.9)) +\n  theme_graph() +\n  theme(legend.position=\"none\")\n```\n\n::: {.cell-output-display}\n![](LabSet_02_files/figure-pdf/unnamed-chunk-15-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nWrite a 2-4 sentence interpretation of the *time* vs. *money* collocational network.\n\n> Your response\n\n### Task 3\n\nLoad the down-sampled screenplays, extract the dialogue, and tokenize the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"../data/screenplays.rda\")\n\nsp <- from_play(sp, extract = \"dialogue\")\n\nsp <-   sp %>%\n  mutate(text = preprocess_text(text)) %>%\n  corpus() %>%\n  tokens(what=\"fastestword\", remove_numbers=TRUE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nb <- collocates_by_MI(sp, \"boy\", left = 3, right = 0)\nb <- b %>% filter(col_freq >= 3 & MI_1 >= 3)\n\ng  <- collocates_by_MI(sp, \"girl\", left = 3, right = 0)\ng <- g %>% filter(col_freq >= 3 & MI_1 >= 3)\n```\n:::\n\n\n#### Plot the network\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnet <- col_network(b, g)\n\nggraph(net, weight = link_weight, layout = \"stress\") + \n  geom_edge_link(color = \"gray80\", alpha = .75) + \n  geom_node_point(aes(alpha = node_weight, size = 3, color = n_intersects)) +\n  geom_node_text(aes(label = label), repel = T, size = 3) +\n  scale_alpha(range = c(0.2, 0.9)) +\n  theme_graph() +\n  theme(legend.position=\"none\")\n```\n\n::: {.cell-output-display}\n![](LabSet_02_files/figure-pdf/unnamed-chunk-18-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nWrite a 3-5 sentence interpretation of the *boy* vs. *girl* collocational network, which includes reporting relevant association measures following the example in Brezina (pg. 75).\n\n> Your response\n\n## Keyness\n\n### Task 1\n\n#### Create a keyness table\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"../R/keyness_functions.R\")\nsource(\"../R/helper_functions.R\")\nload(\"../data/sample_corpus.rda\")\n```\n:::\n\n\n\n1. In the code block below, create a document-feature matrix of the blog text-type.\n2. In the same code-block create a keyness table with the blog text-type as the target corpus and the news text-type as the reference.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# your code goes here\n```\n:::\n\n\n3. Use the code block below to output the head of the keyness table with an accompanying caption.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# your table goes here\n```\n:::\n\n\n#### Answer the following questions\n\n1. What are the 2 tokens with the highest keyness values?\n\n> Your response\n\n2. Posit an explanation for their greater frequency in blog corpus, being as **descriptive** as possible. Think about the **communicative purposes** of these text-types, as opposed to value judgments about the writers or the genres.\n\n> Your response\n\n3. What are the 2 tokens with the greatest effect sizes?\n\n> Your response\n\n4. Posit a reason for that result.\n\n> Your response\n\n\n",
    "supporting": [
      "LabSet_02_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}