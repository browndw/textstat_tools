{
  "hash": "d1bd56d22af06ad2fb37880f9e446001",
  "result": {
    "engine": "knitr",
    "markdown": "\n# Time Series\n\nFor this exercise, we going to make use two packages that can help us plot and manipulate dendrograms.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(dendextend)\nlibrary(ggdendro)\n```\n:::\n\n\nLoad the functions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"../R/google_ngrams.R\")\nsource(\"../R/vnc_functions.R\")\n```\n:::\n\n\n## Importing Google Ngrams Data\n\nWe won't do much with the google_ngram() function because most of Google data table are HUGE. Though they are formatted as simple tab-delimited text files, they often run in the multiple gigabytes in size. You can access them here:\n\n<http://storage.googleapis.com/books/ngrams/books/datasetsv2.html>\n\nWith that in mind, we'll do a simple demo of the function with one of the smaller 1-gram tables: Q. First, we'll make a vector of the word forms we want to count. In this case, we'll count 3 common forms of *quiz*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwfs <- c(\"quiz\", \"quizzes\", \"quizzed\")\nq <- google_ngram(wfs, variety = \"eng\", by = \"year\")\n```\n:::\n\n\nNote that before we plot, we should check for gaps -- in other words, years when counts are not in the data and, thus, zero. We can use the `is.sequence()` function.\n\nHere we'll check the full **Year** column and only years after 1799.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq$Year %>% is.sequence()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n\n```{.r .cell-code}\nq$Year[q$Year > 1799] %>% is.sequence()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n## Peaks and Troughs\n\nIf we wanted to plot from the first instance (1515) onward, we would need to fill in the all missing years (rows) with zero values. We would also want to check the underlying data. A quick search of Google Books seems to suggest that sixteenth century examples come from passages that are not in English and often in the form *qu'iz*.\n\nInstead, we will plot only data from 1800 and after. And we can plot the data with a confidence interval to identify \"peaks and troughs\" (Brezina pgs. 241-247).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(q %>% filter(Year > 1799), aes(x=Year, y=Per_10.6)) +\n    geom_point(size = .5) +\n    geom_smooth(method = \"gam\", formula = y ~ s(x, bs = \"cs\"), size=.25) +\n    labs(x=\"Year\", y = \"Frequency (per million words)\")+ \n    theme(panel.grid.minor.x=element_blank(),\n          panel.grid.major.x=element_blank()) +\n    theme(panel.grid.minor.y =   element_blank(),\n          panel.grid.major.y =   element_line(colour = \"gray\",size=0.25)) +\n    theme(rect = element_blank()) +\n    theme(legend.title=element_blank())\n```\n\n::: {.cell-output-display}\n![Frequencies (per million words) of lemmatized *quiz* from the 18th through the 20th centuries.](time-series_files/figure-html/quiz_year-1.png){width=672}\n:::\n:::\n\n\nWe can also aggregate the counts by decade.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq <- google_ngram(wfs, variety = \"eng\", by = \"decade\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(q %>% filter(Decade > 1799), aes(x=Decade, y=Per_10.6)) +\n    geom_bar(stat = \"identity\") +\n    labs(x=\"Decade\", y = \"Frequency (per million words)\")+ \n    theme(panel.grid.minor.x=element_blank(),\n          panel.grid.major.x=element_blank()) +\n    theme(panel.grid.minor.y =   element_blank(),\n          panel.grid.major.y =   element_line(colour = \"gray\",size=0.25)) +\n    theme(rect = element_blank()) +\n    theme(legend.title=element_blank())\n```\n\n::: {.cell-output-display}\n![Frequencies by decade (per million words) of lemmatized *quiz* from the 18th through the 20th centuries.](time-series_files/figure-html/quiz_decade-1.png){width=672}\n:::\n:::\n\n\n### Confidence Intervals\n\nTo add confidence intervals to a bar plot, we need to calculate the upper and lower bounds. For this we'll use the `prop.cint()` from the **corpora** package:\n\n<https://www.rdocumentation.org/packages/corpora/versions/0.5/topics/prop.cint>\n\nWe only need to pass the function a vector of frequencies and a vector of total counts (corpus sizes). We'll bind those to a new data frame and normalize per million tokens.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nword_freq <- q %>% \n  bind_cols(corpora::prop.cint(k = q$AF, n = q$Total, conf.level = 0.95)) %>%\n  mutate(lower = lower*1000000) %>%\n  mutate(upper = upper*1000000)\n```\n:::\n\n\nAnd plot adding **geom_errorbar()**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(word_freq %>% filter(Decade > 1799), aes(x=Decade, y=Per_10.6)) +\n    geom_bar(stat = \"identity\", fill=\"steelblue\") +\n    geom_errorbar(aes(ymin=lower, ymax=upper), width=.1) +\n    labs(x=\"Decade\", y = \"Frequency (per million words)\") + \n    theme(panel.grid.minor.x=element_blank(),\n          panel.grid.major.x=element_blank()) +\n    theme(panel.grid.minor.y =   element_blank(),\n          panel.grid.major.y =   element_line(colour = \"gray\",size=0.25)) +\n    theme(rect = element_blank()) +\n    theme(legend.title=element_blank())\n```\n\n::: {.cell-output-display}\n![Frequencies by decade (per million words and with 95% confidence intervals) of lemmatized *quiz* from the 18th through the 20th centuries.](time-series_files/figure-html/quiz_ci-1.png){width=672}\n:::\n:::\n\n\n## Periodization\n\nJust eyeballing the data, it looks like there might be some interesting changes in frequency in the middle of the 20th century, and late in the 20th century.\n\nTo better understand how these changes group together (what is called \"periodization\") we can turn turn Variability-Based Neighbor clustering.\n\nWe're going take data for the 20th century onward and begin with a scree plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq <- q %>% filter(Decade > 1899)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvnc_scree(q$Decade, q$Per_10.6)\n```\n\n::: {.cell-output-display}\n![Scree plot of standard deviations for lemmatized *quiz*.](time-series_files/figure-html/quiz_scree-1.png){width=672}\n:::\n:::\n\n\nFrom the scree plot, it looks like we have 2-3 well-formed clusters. Now, we'll generate the data for our dendrogram. Keep in mind this is a very specific implementation of hierarchical clustering as we need to maintain the order of our time series.\n\nThe distance is based on standard deviations of sequential pairs of time intervals. Alternatively, you can set the distance.measure to \"cv\" for to use the coefficient of variation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhc <- vnc_clust(q$Decade, q$Per_10.6, distance.measure = \"sd\")\nplot(hc, hang = -1, main = \"\", sub = \"\", xlab = \"\")\n```\n\n::: {.cell-output-display}\n![VNC dendrogram showing frequencies of lemmatized *quiz* in 20th century English.](time-series_files/figure-html/quiz_vnc-1.png){width=672}\n:::\n:::\n\n\nThe purpose of Variability-Based Neighbor Clustering is to divide the use of a word or phrase into historical periods based on changes in frequency. Rather than assuming that a year, decade, or other division is statistically meaningful, the algorithm clusters segments of time into periods.\n\nNow let's look at some other data: frequencies of the bigram *witch hunt* and the plural *witch hunts*. These also comes from Google Books. You can gather the data yourself at a later time using `google_ngram()`, if you want, but for the purposes of this exercise we'll skip that step to save time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"../data/witch_hunt.rda\")\n\nwh <- witch_hunt %>%\n  filter(decade > 1899) %>% \n  dplyr::select(decade, counts_permil)\n\nvnc_scree(wh$decade, wh$counts_permil, distance.measure = \"sd\")\n```\n\n::: {.cell-output-display}\n![Scree plot of standard deviations for lemmatized *witch hunt*.](time-series_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhc <- vnc_clust(wh$decade, wh$counts_permil, distance.measure = \"sd\")\n```\n:::\n\n\nFor the next step, we’ll cut the dendrogram into 3 clusters based on the output of the scree plot we that generated. Note that we’re storing the output into a list **cut_hc**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(hc, hang = -1)\ncut_hc <- rect.hclust(hc, k=3)\n```\n\n::: {.cell-output-display}\n![VNC dendrogram showing frequencies of lemmatized *witch hunt* in 20th century English.y cut into 3 clusters.](time-series_files/figure-html/wh_cut-1.png){width=672}\n:::\n:::\n\n\n___\n\\begin{center}\nSTOP!\\\\\nCOMPLETE TASK 1\n\\end{center} \n___\n\n### Advanced plotting\n\nWe’ve already plotted our data with base R. However, if we want more control, we probably want to use ggplot2. To do that, we need to go through a couple of intermediate steps. First, convert the cut_hc object that we just generated into a data frame and join that with our original **witch hunt** data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclust_df <- data.frame(decade=as.numeric(names(unlist(cut_hc))),\n  clust=rep(c(paste0(\"clust_\", seq(1:length(cut_hc)))),\n  times=sapply(cut_hc,length)))\n\nclust_df <- clust_df %>% right_join(wh, by = \"decade\")\n```\n:::\n\n\nNext, we’ll convert our cluster data into dendrogram data using `as.dendrogram()` from ggdendro. We also MUST maintain the order of our time series. There are a variety of ways of doing this, but dendextend has an easy function called `sort()`. We’ll take the easy way!\n\nTo get ggplot-friendly data, we have to transform it yet again… This time using the **ggdendro** package’s function `dendro_data()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndend <- as.dendrogram(hc) %>% sort\ndend_data <- dendro_data(dend, type = \"rectangle\")\n```\n:::\n\n\nNow let’s do some fancy plotting! We’re going to combine the dendrogram and a time series line plot like Gries and Hilpert [@gries2012variability] do on pg. 140 of their chapter on VNC.\n\nThe first three lines pull data from clust_df for the line plot using the clusters to color each point according to group. The geom_segment pulls data from dend_data to build the dendrogram. For the tick marks we again pull from **dend_data** using the x column for the breaks and and the label column to label the breaks.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(clust_df, aes(x = as.numeric(rownames(clust_df)), y = counts_permil)) +\n  geom_line(linetype = \"dotted\") +\n  geom_point(aes(color = clust), size = 2) +\n  geom_segment(data = dend_data$segments, aes(x = x, y = y, xend = xend, yend = yend))+\n  scale_x_continuous(breaks = dend_data$labels$x,\n    labels=as.character(dend_data$labels$label)) +\n  xlab(\"\") + ylab(\"Frequency (per million words)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![VNC dendrogram showing frequencies of lemmatized *witch hunt* in 20th century English cut into 3 clusters.](time-series_files/figure-html/wh_overlay\"-1.png){width=672}\n:::\n:::\n\n\n## Works cited\n\n",
    "supporting": [
      "time-series_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}