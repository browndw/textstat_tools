{
  "hash": "1f97ae64bbd13ff27b916f0e2b5bf9e0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 2\"\nauthor: \"My Name\"\noutput:\n  pdf_document:\n    fig_caption: yes\n    number_sections: true\nheader-includes:\n  - |\n    ```{=latex}\n    \\usepackage{fvextra}\n    \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{\n      breaksymbolleft={}, \n      showspaces = false,\n      showtabs = false,\n      breaklines,\n      commandchars=\\\\\\{\\}\n    }\n    ```\n---\n\n\n\n\n# Lab 2 (Set 1): The basics {-}\n\n# A simple processing pipeline\n\nLet's begin by creating an object consisting of a character string. In this case, the first sentence from *A Tale of Two Cities*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntotc_txt <- \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair.\"\n```\n:::\n\n\nAnd we'll load the tidyverse libraries.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\nWe could then split the vector, say at each space.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntotc_tkns <- totc_txt %>%\n    str_split(\" \")\n```\n:::\n\n\nThen, we can create a table of counts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntotc_df <- table(totc_tkns) %>% # make a table of counts\n  as_tibble() %>%\n  rename(Token = totc_tkns, AF = n) %>% # rename columns\n  arrange(-AF) # sort the data by frequency\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: Token counts of sample sentence.\n\n|Token | AF|\n|:-----|--:|\n|of    | 10|\n|the   | 10|\n|was   | 10|\n|it    |  9|\n|age   |  2|\n|epoch |  2|\n\n\n:::\n:::\n\n\nThe process of splitting the string vector into constituent parts is called **tokenizing**. Think of this as telling the computer how to define a word (or a \"token\", which is a more precise, technical term). In this case, we've done it in an extremely simple way--by defining a token as any string that is bounded by spaces.\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: Case sensitive counts of the token *it*.\n\n|Token | AF|\n|:-----|--:|\n|it    |  9|\n|It    |  1|\n\n\n:::\n:::\n\n\nNote that in doing so, we are counting capitalized and non-capitalized words as distinct tokens.\n\nThere may be specific instances when we want to do this. But normally, we'd want *it* and *It* to be the same token. To do that, we can add a step in the processing pipeline that converts our vector to lower case before tokenizing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntotc_df <- tolower(totc_txt) %>%\n  str_split(\" \") %>%\n  table() %>% # make a table of counts\n  as_tibble() %>%\n  rename(Token = \".\", AF = n) %>% # rename columns\n  arrange(-AF) # sort the data by frequency\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: Token counts of sample sentence.\n\n|Token | AF|\n|:-----|--:|\n|it    | 10|\n|of    | 10|\n|the   | 10|\n|was   | 10|\n|age   |  2|\n|epoch |  2|\n\n\n:::\n:::\n\n\n___\n\\begin{center}\nSTOP!\\\\\nCOMPLETE TASKS 1 \\& 2\n\\end{center} \n___\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}