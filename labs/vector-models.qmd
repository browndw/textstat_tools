
# Introduction to Vector Models and Word Embeddings

This is just a very quick walk-though tutorial on using vector models. This Lab only uses a pre-trained model. However, you can build your own model (which we will do in another Lab). You can also use vector models for classification tasks. For that, you should consult the document for the R package we're using: **ruimtehol**.

There is an overview here:

<http://www.bnosac.be/index.php/blog/86-neural-text-modelling-with-r-package-ruimtehol>

And here:

<https://github.com/bnosac/ruimtehol>

For more about Word2vec and embeddings generally, see the overview here:

<https://medium.com/compassred-data-blog/introduction-to-word-embeddings-and-its-applications-8749fd1eb232>

```{r setup}
#| message: false
#| error: false
#| warning: false

library(ruimtehol)
library(tidyverse)
library(Rtsne)
library(gt)
```

## Basics

A variety of models are available for you on CMU Box:

<https://cmu.box.com/s/o2y5lbaonmguf51h5kipa6i2v4y3nm3n>

You can simply download a model and place it in the `models` directory. Then load the model.

For demo purposes, we'll use a small one that contains the 50,000 most frequent tokens in COCA and embeddings in only 100 dimensions.


```{r load_model}
# model <- starspace_load_model("../models/en_coca_vec_sm.ruimtehol")
# model <- starspace_load_model("../models/en_stanford_cc_lg.ruimtehol")

model <- starspace_load_model("../models/en_coca_vec_50k.ruimtehol")
```

Now we can check how similarities work.

```{r similarity}
embedding_similarity(
  starspace_embedding(model, "dog"),
  starspace_embedding(model, "cat"), 
  type = "cosine") |>
  tibble::enframe() |>
  gt()
```

Look for nearest embeddings.

```{r shakespeare}
starspace_knn(model, "shakespeare", 10) |>
  data.frame() |>
  gt()
```

We can also calculate document similarity.

```{r doc_similarity}
embedding_similarity(
  starspace_embedding(model, "what does this bunch of text look like", type = "document"),
  starspace_embedding(model, "word abracadabra is even in the dictionary", type = "document"), 
  type = "cosine") |>
  tibble::enframe() |>
  gt()
```


Note that we can extract embeddings, as well.

```{r as_matrix}
embedding <- as.matrix(model)
```

```{r dog_cat}
embedding[c("dog", "cat"), ][,1:10] |>
  data.frame() |>
  gt()
```

This ability is useful if we want to plot the locations of words or documents.

## Example from the MICUSP data

First, we'll read in the text data and load some functions.

```{r}
load("../data/micusp_mini.rda")
source("../R/helper_functions.R")
```

Next, we'll filter out two disciplines and prepocess the text.

```{r process_micusp}
#| message: false
#| error: false
#| warning: false

df <- micusp_mini %>%
  filter(str_detect(doc_id, "^BIO|^ENG")) %>%
  mutate(text = preprocess_text(text, remove_numbers = T))
```

Let's sample out one example for each score from question "P1".

```{r embed_function}
get_embeddings <- function(x){
  
  df <- x
  idx <- seq(1:nrow(df))
  doc_embed <- function(i){
    e <- starspace_embedding(model, x = df[i,2], type = "document")
    rownames(e) <- df[i,1]
    e <- data.frame(e)
  }
  doc_embeds <- lapply(idx, doc_embed)
  doc_embeds <- bind_rows(doc_embeds)
  return(doc_embeds)
}
```

Now we can extract the embeddings for each our documents:

```{r extract_embeddings}
doc_embeddings <- get_embeddings(df)

doc_embeddings <- doc_embeddings %>% 
  rownames_to_column("doc_id") %>%
  mutate(group = str_extract(doc_id, "^[A-Z]+")) %>%
  dplyr::select(doc_id, group, everything())
```

From that, we calculated document similarity:

```{r closest_docs}
#| message: false
#| error: false
#| warning: false

closest_docs <- function(doc_query){
  pts <- doc_embeddings %>% filter(doc_id == doc_query) %>% select(X1:X100)
  closest <- RANN::nn2(data = doc_embeddings[,3:102], query = pts, k = 11)
  doc_ids <- doc_embeddings[closest$nn.idx,1]
  doc_dist <- closest$nn.dists %>% as.vector()
  df <- data.frame(doc_ids, doc_dist)
  return(df)
}
```

In order to exact the embeddings we'll create a very simple function.

```{r closest_bio}
closest_docs("BIO.G0.02.1") |>
  gt()
```

And extract them.

```{r closest_eng}
closest_docs("ENG.G2.04.1") |>
  gt()
```

## Plotting

For dimension reduction use a different technique: t-SNE. For more about t-Distributed Stochastic Neighbor Embedding see here:

<https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1>

```{r dist}
#| message: false
#| error: false
#| warning: false

m <- dist(doc_embeddings[,3:102]) %>% as.matrix()
dimnames(m) <- dimnames(m) <- list(doc_embeddings$doc_id, doc_embeddings$doc_id) 
df_pairs <- t(combn(doc_embeddings$doc_id, 2))
dist_df <- data.frame(df_pairs, dist=m[df_pairs])
dist_df <- dist_df %>% arrange(dist)
```

```{r tsne}

doc_tsne <- Rtsne(as.matrix(doc_embeddings[,3:102]), check_duplicates = FALSE, pca = FALSE, perplexity=5, theta=0.5, dims=2)

doc_tsne <- as.data.frame(doc_tsne$Y) %>% bind_cols(select(doc_embeddings, doc_id, group))
```


```{r tsne-plot}
#| warning: false
#| fig-height: 4
#| fig-width: 7
#| fig-cap: "Histogram of the token *the*"

ggplot(doc_tsne, aes(x = V1, y = V2, fill = group)) +
  geom_point(shape = 21) +
  ggrepel::geom_text_repel(aes(label = doc_id), size = 3) +
  viridis::scale_fill_viridis(discrete = T) +
  theme_linedraw() +
  theme(panel.grid.minor = element_blank())
```

## Another Example

Next, let's build a predictive model from the Federalist Papers. Again, we'll convert the text to lower case and remove punctuation.


```{r fed-data}
load("../data/federalist_meta.rda")
load("../data/federalist_papers.rda")
```


```{r fed-prep}
#| message: false
#| error: false
#| warning: false

fed_meta <- federalist_meta %>%
  dplyr::select(doc_id, author_id)

fed_txt <- federalist_papers

fed_txt <- fed_txt %>%
  left_join(fed_meta) %>%
  filter(author_id == "Hamilton" | author_id == "Madison" | author_id == "Disputed") %>%
  mutate(text = preprocess_text(text, remove_numbers = T))
```

Next, we'll subset out our training and testing data. Note that we're going to down-sample in order to balance majority and minority data classes.

```{r train-test}
#| message: false
#| error: false
#| warning: false

train <- fed_txt %>%
  filter(author_id == "Hamilton" | author_id == "Madison") %>%
  group_by(author_id) %>%
  sample_n(14) %>%
  ungroup()

test <- fed_txt %>%
  filter(author_id == "Disputed")
```

Now we can build the model. Here we use bigrams with a minimum count of 2

```{r pred-embed}
#| message: false
#| error: false
#| warning: false
#| results: hide

pred_model <- embed_tagspace(x = train$text, y = train$author_id,
                        early_stopping = 0.9 ,dim = 300, 
                        lr = 0.01, epoch = 10, loss = "softmax", adagrad = TRUE, 
                        similarity = "cosine", negSearchLimit = 10,
                        ngrams = 2, minCount = 2)
```

We can predict the text of any selected row....

```{r}
#| message: false
#| error: false
#| warning: false

predict(pred_model, test$text[1], k = 10)
```

Or create a vector of texts and generate predictions for each... 

```{r}
#| message: false
#| error: false
#| warning: false

text   <- test$text

emb_labels <- as.matrix(pred_model, type ="labels", prefix = FALSE)

scores <- embedding_similarity(starspace_embedding(pred_model, text), emb_labels, type = "cosine", top_n = 1)
```

Then construct a dataframe from the results:

```{r}
#| warning: false
#| message: false

data.frame(test$doc_id, scores[, c ( "term2" , "rank" )]) |>
  gt()
```

