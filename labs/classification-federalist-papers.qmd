
# Mosteller & Wallace

We going to (mostly) replicate a famous classification task that was first done by Frederick Mosteller and David L. Wallace in the early 1960s [-@mosteller1963inference]. They undertook this task without the benefit of modern text processing. It was truly a pioneering work of computational linguistics and information theory. Keep that in mind and we do in minutes what took them months.


```{r}
#| message: false
#| error: false
#| warning: false

library(quanteda)
library(rsample)
library(tidyverse)
library(gt)
```

## Variables

Because of computational limits, they needed to identify potentially productive variables ahead of building their regression model. This is not how we would go about it now, but it was a constraint at the time. They ended up creating 6 bins of likely words, and those are reported in 3 groupings in their study.

Their first group contains 70 tokens...

```{r}

mw_group1 <- c("a", "all", "also", "an", "and", "any", "are", "as", "at", "be", "been", "but", "by", "can", "do", "down", "even", "every", "for", "from", "had", "has", "have", "her", "his", "if", "in", "into", "is", "it",  "its", "may", "more", "must", "my", "no", "not", "now", "of", "on", "one", "only", "or", "our", "shall", "should", "so", "some", "such", "than", "that", "the", "their", "then", "there", "things", "this", "to", "up", "upon", "was", "were", "what", "when", "which", "who", "will", "with", "would", "your")
```


Their second an additional 47...

```{r}

mw_group2 <- c("affect", "again", "although", "among", "another", "because", "between", "both", "city", "commonly", "consequently", "considerable", "contribute", "defensive", "destruction", "did", "direction", "disgracing", "either", "enough", "fortune", "function", "himself", "innovation", "join", "language", "most", "nor", "offensive", "often", "pass", "perhaps", "rapid", "same", "second", "still", "those", "throughout", "under", "vigor", "violate", "violence", "voice", "where", "whether", "while", "whilst")
```

And their third another 48 (though they identify some by lemmas and another "expence" doesn't appear in our data, possibly because of later editing done in our particular edition)...

```{r}

mw_group3 <- c("about", "according", "adversaries", "after", "aid", "always", "apt", "asserted", "before", "being", "better", "care", "choice", "common", "danger", "decide", "decides", "decided", "deciding", "degree", "during", "expense", "expenses", "extent", "follow", "follows", "followed", "following", "i", "imagine", "imagined", "intrust", "intrusted", "intrusting","kind", "large", "likely", "matter", "matters", "moreover", "necessary", "necessity", "necessities", "others", "particularly", "principle", "probability", "proper", "propriety", "provision", "provisions", "requisite", "substance", "they", "though", "truth", "truths", "us", "usage", "usages", "we", "work", "works")
```

All together, they list 165 candidate variables, though it works out to be 180 unlemmatized tokens as potential variables for their model.

We'll concatenate a vector of all their variables into a single vector.

```{r}

mw_all <- sort(c(mw_group1, mw_group2, mw_group3))
```

## *The Federalist Papers*

For their task, Mosteller & Wallace were interested in solving a long-standing historical debate about the disputed authorship of 12 of the Federalist Papers.

The Federalist Papers are made up of 85 articles and essays written by Alexander Hamilton, James Madison, and John Jay under the pseudonym "Publius" to promote the ratification of the United States Constitution.

Authorship of the articles has been disputed since their publication, with Hamilton providing a list to his lawyer before his death, and Madison another disputed some of Hamilton's claims.

We're going to work from the generally accepted authorship designations, which assign authorship of 51 articles to Hamilton, 14 to Madison, 5 to Jay, and 3 to joint authorship. The other 12 are in doubt as to whether they were written by Hamilton or Madison.

So let's begin. First, we'll get the metadata.

```{r}

load("../data/federalist_meta.rda")
load("../data/federalist_papers.rda")
```


```{r}
fed_meta <- federalist_meta %>%
  dplyr::select(doc_id, author_id)
```

And we're going to read in ALL of the data. Why do it this way? We could build out separate data sets for training, validating, and predicting. HOWEVER, we need our data to be identical in structure at every step. This can become tedious if you're forever wrangling data.frames to get them as the need to be. It's much easier to begin with one dfm and subset as necessary for the classification process.

So let's read in the text.

```{r}
fed_txt <- federalist_papers
```

## Preparing the Data

Now, we'll tokenize the data.

```{r}
fed_tokens <- fed_txt %>%
  corpus() %>%
  tokens(remove_punct = T, remove_symbols = T, what = "word")
```

And create a weighted dfm. The 3rd line preps the column so it can be merged with our metadata. The 4th orders the tokens by their mean frequencies. This isn't necessary here, but can be useful when doing quick sub-setting of variables. And the 5th changes the column name for easy joining.

```{r}
fed_dfm <- fed_tokens %>% dfm() %>% dfm_weight(scheme = "prop") %>%
  convert(to = "data.frame") %>%
  select(doc_id, names(sort(colMeans(.[,-1]), decreasing = TRUE)))
```

Now let's join the author_id from the metadata.

```{r}
#| message: false

fed_dfm <- fed_dfm %>% 
  right_join(fed_meta) %>% 
  dplyr::select(doc_id, author_id, everything()) %>% 
  as_tibble()
```

### Training and testing data

Now we can subset out our training and testing data.

```{r}
train_dfm <- fed_dfm %>% filter(author_id == "Hamilton" | author_id == "Madison")
test_dfm <- fed_dfm %>% filter(author_id == "Disputed")
```

For the next step we're going to again separate our training data. We want a subset of known data against which we can validate our model.

For this, we'll use some handy functions from the **rsample** package. First, we make an 80/20 split. From that we create a new, smaller training set, and a validation set.

```{r}
set.seed(123)
valid_split <- initial_split(train_dfm, .8)
train_dfm_v2 <- analysis(valid_split)
train_valid <- assessment(valid_split)
```

Next, we'll select only those 70 tokens from Mosteller & Wallace's first group. We also need to convert the author_id into a 2-level factor, and to move the text_id to row names. The same for the validation data, but we don't need to worry about the factor conversion.

```{r}
train_dfm_v2_1 <- train_dfm_v2 %>% 
  dplyr::select(doc_id, author_id, all_of(mw_group1)) %>%
  mutate(doc_id = factor(doc_id)) %>%
  column_to_rownames("doc_id")

train_valid_1 <- train_valid %>% 
  dplyr::select(doc_id, author_id, all_of(mw_group1)) %>%
  column_to_rownames("doc_id")
```

## Lasso

For our regression, we're going to take advantage of lasso regression. This is a form of penalized logistic regression, which imposes a penalty to the logistic model for having too many variables. This results in shrinking the coefficients of the less contributive  variables toward zero. This is also known as regularization.

For this, we'll use the **glmnet** package.

```{r}
#| message: false
#| error: false
#| warning: false

library(glmnet)
```

### Ridge & lasso regression

Least squares fits a model by minimizing the sum of squared residuals.

$$RSS = \sum_{i=1}^{n}(y_{i}-\beta_0-\sum_{j=1}^{p}\beta_jx_ij)^2$$

Ridge Regression is similar, but it includes another term.

$$\sum_{i=1}^{n}(y_{i}-\beta_0-\sum_{j=1}^{p}\beta_jx_ij)^2 + \lambda \sum_{j=1}^{p}\beta_{j}^{2} = RSS + \lambda \sum_{j=1}^{p}\beta_{j}^{2}$$

In order to minimize this equation $\beta_1,...\beta_p$ should be close to zero and so it shrinks the coefficients. The tuning parameter, $\lambda$, controls the impact.

Ridge regression does have some disadvantages.

* Unlike subset selection, ridge regression includes all *p* predictors.
* The penalty term will shrink all of the coefficients towards zero, but none of them will be exactly zero.
* Such a large model often makes interpretation difficult.

The lasso helps overcome these problems. It is similar to ridge regression, but the penalty term is slightly different.

$$\sum_{i=1}^{n}(y_{i}-\beta_0-\sum_{j=1}^{p}\beta_jx_ij)^2 + \lambda \sum_{j=1}^{p}|\beta_{j}| = RSS + \lambda \sum_{j=1}^{p}|\beta_{j}|$$

Like ridge regression it shrinks the coefficients towards zero. However, the lasso allows some of the coefficients to be exactly zero.

For more detail on lasso regression you can look here:

<https://eight2late.wordpress.com/2017/07/11/a-gentle-introduction-to-logistic-regression-and-lasso-regularisation-using-r/>

This is a very useful technique for variable selection and can reduce the likelihood of overfitting. This is particularly helpful in linguistic analysis where we're often working with many variables making the implementation of functions like **step()** sometimes tedious.

### Using **glmnet**

To help you decide which lambda to use, the `cv.glmnet()` function does cross-validation. The default sets **alpha=1** for lasso. If we wanted ridge, we would set **alpha=0**.

```{r}

cv_fit <- cv.glmnet(as.matrix(train_dfm_v2_1[, -1]), train_dfm_v2_1[, 1], family = "binomial")
```

We can plot the log of the resulting lambdas.

```{r}
#| fig-height: 4
#| fig-width: 7

plot(cv_fit)
```

The plot displays the cross-validation error according to the log of lambda. The left dashed vertical line indicates that the log of the optimal value of lambda is approximately -6, which is the one that minimizes the prediction error. This lambda value will give the most accurate model. 

The exact value of lambda can also be viewed. We'll save our regression coefficients.

```{r}

lambda_min <- cv_fit$lambda.min
lambda_lse <- cv_fit$lambda.1se
```

By filtering those variables with coefficients of zero, we see only the variables have been included in the model. Ours has 13.

```{r}

coef(cv_fit, s = "lambda.min") |>
  as.matrix() |>
  data.frame() |>
  rownames_to_column("Variable") |>
  filter(s1 !=0) |>
  dplyr::rename(Coeff = s1)  |>
  gt() |>
  fmt_number(columns = "Coeff",
             decimals = 2)
```

## Validate the model

To validate the model, let's create a model matrix from the texts we've split off for that purpose.

```{r}

x_test <- model.matrix(author_id ~., train_valid_1)[,-1]
```

From our model, we'll predict the author_id of the validation set.

```{r}

lasso_prob <- predict(cv_fit, newx = x_test, s = lambda_lse, type = "response")
```

From the probabilities, we can return the predicted authors.

```{r}

lasso_predict <- ifelse(lasso_prob > 0.5, "Madison", "Hamilton")
```

```{r}
#| code-fold: true

lasso_predict |> 
  data.frame() |>
  dplyr::rename(Predict = s1) |>
  tibble::rownames_to_column("Test") |>
  gt()
```

Retrieve what they actually are and calculate our model accuracy.

```{r}

table(pred=lasso_predict, true=train_valid_1$author_id)
```

```{r}
paste0(mean(lasso_predict == train_valid_1$author_id)*100, "%")
```

Ours is 100% accurate. Not bad. Note that if you wanted to really test the model, we could create a function to run through this process starting with the sampling.That way, we could generate a range of accuracy over repeated sampling of training and validation data.

## Mosteller & Wallace's Experiment

Let's repeat the process, but this time we'll start with all of Mosteller & Wallace's candidate variables.

### Create a new training and validation set

```{r}
train_dfm_v2_2 <- train_dfm_v2 %>% 
  dplyr::select(doc_id, author_id, all_of(mw_all)) %>%
  mutate(author_id = factor(author_id)) %>%
  column_to_rownames("doc_id")

train_valid_2 <- train_valid %>% 
  dplyr::select(doc_id, author_id, all_of(mw_all)) %>%
  column_to_rownames("doc_id")
```
 
### Carry out cross-validation
 
```{r}
cv_fit <- cv.glmnet(as.matrix(train_dfm_v2_2[, -1]), train_dfm_v2_2[, 1], family = "binomial")
```
 
Look at our coefficients... 17 this time...
 
```{r}
coef(cv_fit, s = "lambda.min") |>
  as.matrix() |>
  data.frame() |>
  rownames_to_column("Variable") |>
  filter(s1 !=0) |>
  dplyr::rename(Coeff = s1) |>
  gt() |>
  fmt_number(columns = "Coeff",
             decimals = 2)
```

Save our minimum lambda and our regression coefficients.

```{r}
lambda_min <- cv_fit$lambda.min
lambda_lse <- cv_fit$lambda.1se
```

### Create a matrix from the validation set

```{r}
x_test <- model.matrix(author_id ~., train_valid_2)[,-1]
```

### Predict the **author_id** of the validation set.

```{r}
lasso_prob <- predict(cv_fit, newx = x_test, s = lambda_lse, type = "response")
```

### Return the predicted authors.

```{r}
lasso_predict <- ifelse(lasso_prob > 0.5, "Madison", "Hamilton")
```

### Check confusion matrix

```{r}
 table(pred=lasso_predict, true=train_valid_1$author_id)
```

The model looks good... So let's proceed with the data in question.

### Prepare full training set

```{r}
train_dfm <- train_dfm %>% 
  dplyr::select(doc_id, author_id, all_of(mw_all)) %>%
  mutate(author_id = factor(author_id)) %>%
  column_to_rownames("doc_id")
```

### Prepare test data

```{r}
test_dfm <- test_dfm %>% 
  dplyr::select(doc_id, author_id, all_of(mw_all)) %>%
  column_to_rownames("doc_id")
```

### Carry out cross-validation

```{r}
cv_fit <- cv.glmnet(as.matrix(train_dfm[, -1]), train_dfm[, 1], family = "binomial")
```

As we would expect, this is close to what we saw previously.

```{r}
coef(cv_fit, s = "lambda.min") |>
  as.matrix() |>
  data.frame() |>
  rownames_to_column("Variable") |>
  filter(s1 !=0) |>
  dplyr::rename(Coeff = s1) |>
  gt() |>
  fmt_number(columns = "Coeff",
             decimals = 2)
```

### Run lasso

```{r}

lasso_fit <- glmnet(as.matrix(train_dfm[, -1]), train_dfm[, 1], alpha = 1, family = "binomial", lambda = cv_fit$lambda.min)
```

### Create a matrix from the test set and predict author

```{r}

x_test <- model.matrix(author_id ~., test_dfm)[,-1]
lasso_prob <- predict(cv_fit, newx = x_test, s = lambda_lse, type = "response")
lasso_predict <- ifelse(lasso_prob > 0.5, "Madison", "Hamilton")
```

### Check results

```{r}

data.frame(lasso_predict, lasso_prob) |>
  dplyr::rename(Author = s1, Prob = s1.1) |>
  gt() |>
  fmt_number(columns = "Prob",
             decimals = 2)
```

Our model predicts all but 55 were written by Madison. Our model is not particularly confident about that result. This hews pretty closely to Mosteller & Wallace's findings, through they come down (sort of) on the side of Madison for 55. However, they also acknowledge that the evidence is weak and not very convincing.

It's worth noting, too, that other studies using other techniques have suggested that 55 was authored by Hamilton. See, for example, here:

<https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0054998>

::: callout-important
## Pause for Lab Set Question

Complete [Tasks 1 and 2 in Lab Set 1](../lab_sets/LabSet_01.qmd#mosteller-wallace).
:::

## Works cited
