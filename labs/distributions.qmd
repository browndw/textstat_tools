
# Distributions

## Prepare a corpus

### Load the needed packages

```{r}
#| message: false
#| error: false
#| warning: false

library(gt)
library(quanteda)
library(quanteda.textstats)
library(tidyverse)
```


### Load a corpus

The  repository comes with some data sets. The conventional way to format text data prior to processing is as a table with a column of document ids (which correspond to the the file names) and a column of texts. Such a table is easy to create from text data on your own local drive using the package [**readtext**](https://readtext.quanteda.io/articles/readtext_vignette.html).


```{r}

load("../data/sample_corpus.rda")
load("../data/multiword_expressions.rda")
```


To peek at the data, we'll look at the first 100 characters in the "text" column of the first row:

```{r}
#| code-fold: true

sample_corpus |>
  mutate(text = substr(text, start = 1, stop = 100)) |>
  head(10) |>
  gt()
```


### Load functions

The repository also contains a number of useful functions. Here, will load some that will calculate a number of common dispersion measures.

```{r}

source("../R/dispersion_functions.R")
source("../R/helper_functions.R")
```

### Create and corpus

Make a corpus object.

```{r}
sc <- corpus(sample_corpus)
```

And check the result:

```{r}
#| code-fold: true
#| label: tbl-summary
#| tbl-cap: "Partial summary of sample corpus."

sc |>
  summary() |>
  head(10) |>
  gt()
```

## Document variables (Name your files systematically!)

::: callout-important
File names can encode important meta-data. In this case, the names include text-types, much like the [Corpus of Contemporary American English](https://www.english-corpora.org/coca/help/texts.asp).

This is **extremely important**. When you build your own corpora, you want to purposefully and systematically name your files and organize your directories. This will save you time and effort later in your analysis.
:::

We are now going to extract the meta-data from the file names and pass them as a variable.

```{r}

doc_categories <- str_extract(sample_corpus$doc_id, "^[a-z]+")
```

Check the result:

```{r echo=FALSE}
#| code-fold: true
#| tbl-cap: "Document categories."

doc_categories |>
  unique() |>
  data.frame(stringsAsFactors = F) |>
  structure(names = c("doc_cats")) |>
  gt() |> 
  tab_options(table.width = pct(50))
```

We will now assign the variable to the corpus. The following command might look backwards, with the function on the left hand side of the **<-** operator. That is because it's an accessor function, which lets us add or modify data in an object. You can tell when a function is an accessor function like this because its help file will show that you can use it with **<-**, for example in **?docvars**.

```{r}
docvars(sc, field = "text_type") <- doc_categories
```

And check the summary again:

```{r}
#| code-fold: true
#| tbl-cap: "Partial summary of sample corpus."

sc |>
  summary() |>
  head(10) |>
  gt()
```

::: callout-warning
Assigning **docvars** is based entirely on ordering. In other words, you are simply attaching a vector of categories to the corpus object. There is no merging by shared keys. Thus, you always need to be sure that your **docvars** are in the same order as your **doc_ids**. This is the reason why we extracted them directly from the **doc_ids**.
:::

## Tokenize the corpus

We'll use **quanteda** to tokenize. And after tokenization, we'll convert them to lower case. *Why do that here?* As a next step, we'll being combining tokens like *a* and *lot* into single units. And we'll be using a list of expressions that isn't case sensitive.

```{r}

sc_tokens <- tokens(sc, include_docvars=TRUE, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, what = "word")

sc_tokens <- tokens_tolower(sc_tokens)
```

## Multi-word Expressions

An issue that we run into frequently with corpus analysis is what to do with multi-word expressions. For example, consider a common English quantifier: "a lot". Typical tokenization rules will split this into two tokens: *a* and *lot*. But counting *a lot* as a single unit might be important depending on our task. We have a way of telling **quanteda** to account for these tokens.

All that we need is a list of multi-word expressions.

The **cmu.textstat** comes with an example of an mwe list called **multiword_expressions**:

```{r echo=FALSE}
knitr::kable(tail(multiword_expressions), col.names = "", caption = "Examples of multi-word expressions.")
```

The **tokens_compound()** function looks for token sequences that match our list
and combines them using an underscore.

```{r compound_tokens}
sc_tokens <- tokens_compound(sc_tokens, pattern = phrase(multiword_expressions))
```

## Create a document-feature matrix

With our tokens object we can now create a document-feature-matrix using the
**dfm()** function. As a reminder, a **dfm** is table with one row per document in the corpus, and one column per unique token in the corpus. Each cell contains a count of how many times a token shows up in that document.

```{r dfm}
sc_dfm <- dfm(sc_tokens)
```

Next we'll create a **dfm** with proportionally weighted counts.

```{r}
prop_dfm <- dfm_weight(sc_dfm, scheme = "prop")
```

## Token distributions

### Distributions of *the*

Let's start by selecting frequencies of the most common token in the corpus:

```{r}
freq_df <- textstat_frequency(sc_dfm) %>%
  data.frame(stringsAsFactors = F)
```

```{r}
#| code-fold: true
#| tbl-cap: "The 10 most frequent tokens in the sample corpus."

freq_df |>
  head(10) |>
  gt()
```

From the weighted **dfm**, we can select any token that we'd like to look at more closely. In this case, we'll select the most frequent token: *the*.

After selecting the variable, we will convert the data into a more friendly data structure.

There are easier ways of doing this, but the first bit of the code-chunk allows us to filter by rank and return a character vector that we can pass. This way, we can find a word of any arbitrary rank.

Also note how the **rename()** function is set up. Let's say our token is *the*. The **dfm_select()** function would result with a column named **the** that we'd want to rename **RF**. So our typical syntax would be: **rename(RF = the)**. In the chunk below, however, our column name is the variable **word**. To pass that variable to **rename**, we use **!!name(word)**.

```{r select}
#| warning: false
#| message: false

word <- freq_df %>% 
  filter(rank == 1) %>% 
  dplyr::select(feature) %>%
  as.character()

word_df <- dfm_select(prop_dfm, word, valuetype = "fixed") # select the token

word_df <- word_df %>% 
  convert(to = "data.frame") %>% 
  cbind(docvars(word_df)) %>% 
  rename(RF = !!as.name(word)) %>% 
  mutate(RF = RF*1000000)
```

With that data it is a simple matter to generate basic summary statistics using the **group_by()** function:

```{r}
summary_table <- word_df %>% 
  group_by(text_type) %>%
  summarize(MEAN = mean(RF),
              SD = sd(RF),
              N = n())
```

```{r}
#| code-fold: true
#| tbl-cap: "Means and standard deviations by text-type."

summary_table |>
  gt()
```

And we can inspect a histogram of the frequencies. To set the width of our bins we'll use the Freedman-Diaconis rule. The bin-width is set to:
$$h = 2 x \frac{IQR(x)}{n^{1/3}}$$

So the number of bins is (max-min)/h, where n is the number of observations, max is the maximum value and min is the minimum value.

```{r bin_width}
bin_width <- function(x){
  2 * IQR(x) / length(x)^(1/3)
  }
```

Now we can plot a histogram. We're also adding a dashed line showing the mean. Note we're also going to use the **scales** package to remove scientific notation from our tick labels.

```{r}
#| fig-height: 2.5
#| fig-width: 7
#| fig-cap: "Histogram of the token *the*"

ggplot(word_df,aes(RF)) + 
  geom_histogram(binwidth = bin_width(word_df$RF), colour="black", fill="white", linewidth=.25) +
  geom_vline(aes(xintercept=mean(RF)), color="red", linetype="dashed", linewidth=.5) +
  theme_classic() +
  scale_x_continuous(labels = scales::comma) +
  xlab("RF (per mil. words)")
```

### Distributions of *the* and *of*

Now let's try plotting histograms of two tokens on the same plot. First we're going to use regular expressions to select the columns. The carat or hat **^** looks for the start of line. Without it, we would also get words like "blather". The dollar symbol **$** looks for the end of a line. The straight line | means OR. Think about how useful this flexibility can be. You could, for example, extract all words that end in *-ion*.

```{r}
#| warning: false

# Note "regex" rather than "fixed"
word_df <- dfm_select(prop_dfm, "^the$|^of$", valuetype = "regex")

# Now we'll convert our selection and normalize to 10000 words.
word_df <- word_df %>% 
  convert(to = "data.frame") %>%
  mutate(the = the*10000) %>%
  mutate(of = of*10000)

# Use "pivot_longer" to go from a wide format to a long one
word_df <- word_df %>% 
  pivot_longer(!doc_id, names_to = "token", values_to = "RF") %>% 
  mutate(token = factor(token))
```

Now let's make a new histogram. Here we assign the values of color and fill to the "token" column. We also make the columns a little transparent using the "alpha" setting.

```{r}
#| fig-width: 7
#| fig-height: 2.5
#| fig-cap: "Histogram of the tokens *the* and *of*."

ggplot(word_df,aes(x = RF, color = token, fill = token)) + 
  geom_histogram(binwidth = bin_width(word_df$RF), alpha=.5, position = "identity") +
  theme_classic() +
  xlab("RF (per mil. words)") +
  theme(axis.text = element_text(size=5))
```

If we don't want overlapping histograms, we can use **facet_wrap()** to split the plots.

```{r}
#| fig-width: 7
#| fig-height: 2.5
#| fig-cap: "Histogram of the tokens *the* and *of*."

ggplot(word_df,aes(x = RF, color = token, fill = token)) + 
  geom_histogram(binwidth = bin_width(word_df$RF), alpha=.5, position = "identity") +
  theme_classic() +
  theme(axis.text = element_text(size=5)) +
  theme(legend.position = "none") +
  xlab("RF (per mil. words)") +
  facet_wrap(~ token)
```

::: callout-important
## Pause for Lab Set Question

Complete [Task 1 in Lab Set 2](../lab_sets/LabSet_02.qmd#distributions).
:::

## Dispersion

We can also calculate dispersion, and there are a variety of measures at our disposal. Our toolkit has several functions for producing these calculations.

For example, we can find the dispersion of any specific token:

```{r token_dispersions}
the <- dispersions_token(sc_dfm, "the") %>% unlist()
```

```{r echo=FALSE}
knitr::kable(the, digits = 3, caption = "Dispersion measures for the token \\textit{the}.", "simple", col.names = "")
```

And let's try another token to compare:

```{r}
data <- dispersions_token(sc_dfm, "data") %>% unlist()
```

```{r echo=FALSE}
knitr::kable(data.frame(the = the["Deviation of proportions DP"], data = c(data["Deviation of proportions DP"])), digits = 3, caption = "Deviation of Proportions for the tokens \\textit{the} and \\textit{data}.", "simple")
```

::: callout-important
## Pause for Lab Set Question

Complete [Task 2 in Lab Set 2](../lab_sets/LabSet_02.qmd#distributions).
:::

### Dispersions for all tokens

We can also calculate selected dispersion measures for all tokens using **dispersions_all()**:

```{r all_dispersions, warning=FALSE, message=FALSE}
d <- dispersions_all(sc_dfm)
```

```{r echo=FALSE}
knitr::kable(head(d), digits = 3, caption = "Dispersion measures for all tokens.", "simple")
```

## Generating a frequency table

Alternatively, **frequency_table()** returns only Deviation of Proportions and Average Reduced Frequency.

Note that ARF requires a tokens object and takes a couple of minutes to calculate.

```{r frequency_table}
ft <- frequency_table(sc_tokens)
```

```{r echo=FALSE}
knitr::kable(head(ft), digits = 3, caption = "Frequency and dispersion measures for all tokens.", "simple")
```

::: callout-important
## Pause for Lab Set Question

Complete [Task 3 in Lab Set 2](../lab_sets/LabSet_02.qmd#distributions).
:::


## Zipf's Law

Let's plot rank against frequency for the 100 most frequent tokens in the sample corpus.

```{r}
#| fig-width: 7
#| fig-height: 4
#| fig-cap: "Token rank vs. frequency."

ggplot(freq_df %>% filter(rank < 101), aes(x = rank, y = frequency)) +
  geom_point(shape = 1, alpha = .5) +
  theme_classic() +
  ylab("Absolute frequency") +
  xlab("Rank")
```

The relationship you're seeing between the rank of a token and it's frequency holds true for almost any corpus and is referred to as **Zipf's Law** (see Brezina pg. 44).

::: callout-important
## Pause for Lab Set Question

Complete [Task 4 in Lab Set 2](../lab_sets/LabSet_02.qmd#distributions).
:::

